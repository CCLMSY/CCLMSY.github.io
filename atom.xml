<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CCLMSY💫</title>
  
  
  <link href="https://www.cclmsy.cc/atom.xml" rel="self"/>
  
  <link href="https://www.cclmsy.cc/"/>
  <updated>2025-03-15T12:36:41.973Z</updated>
  <id>https://www.cclmsy.cc/</id>
  
  <author>
    <name>深翼💫</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Bag of Features 图像检索算法</title>
    <link href="https://www.cclmsy.cc/posts/k_means.html"/>
    <id>https://www.cclmsy.cc/posts/k_means.html</id>
    <published>2025-03-14T16:00:00.000Z</published>
    <updated>2025-03-15T12:36:41.973Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><h3 id="引入：词袋模型（Bag-of-Words）"><a href="#引入：词袋模型（Bag-of-Words）" class="headerlink" title="引入：词袋模型（Bag of Words）"></a>引入：词袋模型（Bag of Words）</h3><p>Bag of Words是文本分类中一种通俗易懂的策略。<br>一般来讲，如果我们要了解一段文本的主要内容，最行之有效的策略是抓取文本中的关键词，根据关键词出现的频率确定这段文本的中心思想。</p><p>Bag of Words中的Words是区分度较高的单词。<br>根据这些Words ，我们就可以快速识别出文章内容，并对文章进行分类。</p><p>Bag of Features是对图像的一种类似的处理方法，抽出图像中的关键特征，根据这些特征来识别图像。</p><h3 id="Bag-of-Features"><a href="#Bag-of-Features" class="headerlink" title="Bag of Features"></a>Bag of Features</h3>]]></content>
    
    
    <summary type="html">一种是用于图像和视频检索的算法</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="计算机视觉" scheme="https://www.cclmsy.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像检索" scheme="https://www.cclmsy.cc/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>K-means/K-means++</title>
    <link href="https://www.cclmsy.cc/posts/k_means.html"/>
    <id>https://www.cclmsy.cc/posts/k_means.html</id>
    <published>2025-03-14T16:00:00.000Z</published>
    <updated>2025-03-15T12:37:12.005Z</updated>
    
    <content type="html"><![CDATA[<h2 id="K-Means聚类算法"><a href="#K-Means聚类算法" class="headerlink" title="K-Means聚类算法"></a>K-Means聚类算法</h2><h3 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h3><p>K-means算法是一种迭代求解的聚类算法，将数据集中的n个样本划分为K个簇（聚类）。<br>每个对象到簇中心的距离之和最小。<br>簇内的对象相似度较高，簇间的对象相似度较低。</p><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><h4 id="1-选择K个初始聚类中心"><a href="#1-选择K个初始聚类中心" class="headerlink" title="1. 选择K个初始聚类中心"></a>1. 选择K个初始聚类中心</h4><p>随机选择K个样本作为初始聚类中心。</p><p>选择对最终的聚类结果有一定影响，因此在实际应用中，通常会采用一些启发式的方法来选择较好的初始聚类中心，如K-means++算法</p><h4 id="2-计算每个样本到聚类中心的距离"><a href="#2-计算每个样本到聚类中心的距离" class="headerlink" title="2. 计算每个样本到聚类中心的距离"></a>2. 计算每个样本到聚类中心的距离</h4><p>对于每个样本，计算其到K个聚类中心的距离，将其划分到距离最近的聚类中心所在的簇中。</p><p>通常使用欧式距离：$d(x<em>i, c_j) = \sqrt{\sum</em>{k=1}^{n}(x<em>{ik}-c</em>{jk})^2}$</p><h4 id="3-更新聚类中心"><a href="#3-更新聚类中心" class="headerlink" title="3. 更新聚类中心"></a>3. 更新聚类中心</h4><p>对于每个聚类，重新计算其聚类中心，新的聚类中心是该聚类内所有数据点的均值：$c<em>j = \dfrac{1}{|S_j|}\sum</em>{x_i\in S_j}x_i$</p><h4 id="4-迭代"><a href="#4-迭代" class="headerlink" title="4. 迭代"></a>4. 迭代</h4><p>重复步骤2和3，直到聚类中心不再发生显著变化或者达到最大迭代次数。</p><h3 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h3><p>优点：逻辑简单、易于实现、收敛速度快</p><p>缺点：需要事先确定聚类数目K，对初始聚类中心敏感，可能收敛到局部最优解</p><h3 id="K-Means-实现"><a href="#K-Means-实现" class="headerlink" title="K-Means 实现"></a>K-Means 实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">k_means</span>(<span class="params">data, k=<span class="number">3</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;K-means 聚类算法</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: 数据集，list[element]，element是一个list[float]</span></span><br><span class="line"><span class="string">        k: 聚类数</span></span><br><span class="line"><span class="string">        max_iter: 最大迭代次数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化聚类中心</span></span><br><span class="line">    centers = random.sample(data, k)</span><br><span class="line">    <span class="comment"># 初始化聚类结果</span></span><br><span class="line">    clusters = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k)]</span><br><span class="line">    <span class="comment"># 迭代聚类</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        <span class="comment"># 分配数据到最近的聚类中心</span></span><br><span class="line">        <span class="keyword">for</span> element <span class="keyword">in</span> data: <span class="comment"># 对于每个数据</span></span><br><span class="line">            min_dist = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="comment"># 最小距离</span></span><br><span class="line">            min_idx = -<span class="number">1</span> <span class="comment"># 最近聚类</span></span><br><span class="line">            <span class="keyword">for</span> i, center <span class="keyword">in</span> <span class="built_in">enumerate</span>(centers):</span><br><span class="line">                dist = <span class="built_in">sum</span>((x-y)**<span class="number">2</span> <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(element, center)) <span class="comment"># 计算距离</span></span><br><span class="line">                <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">                    min_dist = dist</span><br><span class="line">                    min_idx = i</span><br><span class="line">            clusters[min_idx].append(element)</span><br><span class="line">        <span class="comment"># 更新聚类中心</span></span><br><span class="line">        new_centers = [<span class="literal">None</span>] * k</span><br><span class="line">        <span class="keyword">for</span> i, cluster <span class="keyword">in</span> <span class="built_in">enumerate</span>(clusters):</span><br><span class="line">            new_centers[i] = [<span class="built_in">sum</span>(x)/<span class="built_in">len</span>(cluster) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">zip</span>(*cluster)]</span><br><span class="line">        <span class="comment"># 判断是否收敛：中心点是否变化小于eps</span></span><br><span class="line">        eps = <span class="number">1e-6</span></span><br><span class="line">        fl = <span class="built_in">all</span>((<span class="built_in">sum</span>((x-y)**<span class="number">2</span> <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(a, b)) &lt; eps) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(centers, new_centers))</span><br><span class="line">        <span class="keyword">if</span> fl <span class="keyword">or</span> <span class="built_in">iter</span> == max_iter-<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        centers = new_centers</span><br><span class="line">        clusters = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k)]</span><br><span class="line">    <span class="keyword">return</span> clusters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">data = [[random.random() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)]</span><br><span class="line">clusters = k_means(data, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">colors = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;m&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, cluster <span class="keyword">in</span> <span class="built_in">enumerate</span>(clusters):</span><br><span class="line">    <span class="keyword">for</span> element <span class="keyword">in</span> cluster:</span><br><span class="line">        plt.scatter(element[<span class="number">0</span>], element[<span class="number">1</span>], c=colors[i])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://source.cclmsy.cc/Posts/DL/Note/k_means_1.png" alt="k_means"></p><h2 id="K-Means-算法"><a href="#K-Means-算法" class="headerlink" title="K-Means++算法"></a>K-Means++算法</h2><p>K-Means++算法是K-means算法的改进，优化了初始聚类中心的选择，使得初始聚类中心更有代表性，收敛速度更快。</p><p>逐个选取k个簇中心，且离其它簇中心越远的样本点越有可能被选为下一个簇中心</p><h3 id="算法步骤（选择K个初始聚类中心的过程）"><a href="#算法步骤（选择K个初始聚类中心的过程）" class="headerlink" title="算法步骤（选择K个初始聚类中心的过程）"></a>算法步骤（选择K个初始聚类中心的过程）</h3><h4 id="1-选择第一个簇中心"><a href="#1-选择第一个簇中心" class="headerlink" title="1. 选择第一个簇中心"></a>1. 选择第一个簇中心</h4><p>随机选择一个样本作为第一个簇中心</p><h4 id="2-选择下一个簇中心"><a href="#2-选择下一个簇中心" class="headerlink" title="2. 选择下一个簇中心"></a>2. 选择下一个簇中心</h4><p>对于样本x，计算其到已有簇中心的最短距离，记为$D(x)$，选取x作为下一个簇中心的概率为$\dfrac{D(x)^2}{\sum_{x\in X}D(x)^2}$</p><h4 id="3-迭代"><a href="#3-迭代" class="headerlink" title="3. 迭代"></a>3. 迭代</h4><p>重复步骤2，直到选取k个簇中心</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择第一个聚类中心</span></span><br><span class="line">centers = [random.choice(data)]</span><br><span class="line"><span class="comment"># 选择其他聚类中心</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k-<span class="number">1</span>):</span><br><span class="line">    dists = [<span class="built_in">min</span>(<span class="built_in">sum</span>((x-y)**<span class="number">2</span> <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(element, center)) <span class="keyword">for</span> center <span class="keyword">in</span> centers) <span class="keyword">for</span> element <span class="keyword">in</span> data]</span><br><span class="line">    probs = [dist**<span class="number">2</span>/<span class="built_in">sum</span>(dists) <span class="keyword">for</span> dist <span class="keyword">in</span> dists]</span><br><span class="line">    centers.append(random.choices(data, probs)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">最常用的聚类算法</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="聚类算法" scheme="https://www.cclmsy.cc/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>SIFT 图像特征匹配算法</title>
    <link href="https://www.cclmsy.cc/posts/k_means.html"/>
    <id>https://www.cclmsy.cc/posts/k_means.html</id>
    <published>2025-03-14T16:00:00.000Z</published>
    <updated>2025-03-16T07:47:30.465Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SIFT简介"><a href="#SIFT简介" class="headerlink" title="SIFT简介"></a>SIFT简介</h2><p>SIFT（Scale-Invariant Feature Transform，尺度不变特征变换匹配算法）是一种高效区域检测算法。</p><p>SIFT算法可以解决的问题：</p><ol><li>RST（Rotation Scale Translation）：图像的旋转、缩放、平移等变换</li><li>图像的仿射变换（Affine Transformation）：图像的拉伸、压缩、扭曲等变换</li><li>图像的光照变化</li><li>目标部分遮挡</li><li>杂物和噪声干扰</li></ol><h2 id="SIFT算法原理"><a href="#SIFT算法原理" class="headerlink" title="SIFT算法原理"></a>SIFT算法原理</h2><h3 id="1-高斯滤波"><a href="#1-高斯滤波" class="headerlink" title="1. 高斯滤波"></a>1. 高斯滤波</h3><p>一个图像的尺度空间$L$是高斯函数$G(x,y,\sigma)$与图像$I(x,y)$的卷积：</p><script type="math/tex; mode=display">L(x,y,\sigma) = G(x,y,\sigma) * I(x,y) \\\\G(x,y,\sigma) = \dfrac{1}{2\pi\sigma^2}e^{-\frac{(x-x_0)^2+(y-y_0)^2}{2\sigma^2}}</script><p>其中，$G(x,y,\sigma)$是高斯函数，$\sigma$是尺度因子，$*$是卷积运算，$x_0,y_0$是高斯函数的中心（卷积核的中心）。</p><p>$\sigma$越小，图像被平滑的越少，图像的细节越多。<br>小尺度对应于图像的细节，大尺度对应于图像的整体特征。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/Gaussian_Blur.png" alt="Gaussian Blur"></p><h3 id="2-高斯金字塔"><a href="#2-高斯金字塔" class="headerlink" title="2. 高斯金字塔"></a>2. 高斯金字塔</h3><p><img src="https://source.cclmsy.cc/Posts/DL/Note/高斯金字塔.png" alt="高斯金字塔"></p><p>高斯金字塔包含多组（Octave）图像，每组图像由多层（Interval）不同尺度的高斯模糊图像组成。</p><p>构建过程：</p><ol><li>将原图像扩大到两倍，作为第0组的第0层</li><li>对第0组的第0层进行$\sigma_0$的高斯模糊，得到第0组的第1层</li><li>选定一个比例系数$k$，对第0组的第1层进行$k\sigma_0$的高斯模糊，得到第0组的第2层</li><li>对第0组的第2层进行$k^2\sigma_0$的高斯模糊，得到第0组的第3层，以此类推…</li><li>将<strong>上一组倒数第3层图像</strong>做比例因子为2的降采样，得到下一组的第0层，重复步骤2-4</li></ol><p>组数计算公式：$O=\lfloor\log_2(\min(M,N))\rfloor-3$</p><ul><li>$M,N$：原图像的长宽</li></ul><p>层数公式：$S=n+3$</p><ul><li>$S$是每组的层数（自设）</li><li>$n$是最终想在差分金字塔中提取极值点的层数（见差分金字塔）</li></ul><p>平滑因子公式：$\sigma(o,r)=\sigma_0 2^{o+\frac{r}{n}}$，其中$o$是组数，$r$是层数</p><ul><li>SIFT算法中，$\sigma_0=1.6$。但由于相机具有初始模糊$\sigma_0=0.5$，实际$\sigma_0=\sqrt{1.6^2-0.5^2}=1.52$</li><li>第o层的初始平滑因子$\sigma(o,0)=2^o\sigma_0$</li><li>$k=2^{1/n}$</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/高斯金字塔2.png" alt="高斯金字塔2"></p><p>金字塔内各图片的关系：</p><ul><li>在同一组内，不同层图像的尺寸是一样的，后一层图像的高斯平滑因子σ是前一层图像平滑因子的k倍</li><li>在不同组内，后一组第一个图像是前一组倒数第三个图像的二分之一采样，图像大小是前一组的一半</li></ul><h3 id="3-高斯差分金字塔与极值点检测"><a href="#3-高斯差分金字塔与极值点检测" class="headerlink" title="3. 高斯差分金字塔与极值点检测"></a>3. 高斯差分金字塔与极值点检测</h3><h4 id="3-1-高斯差分金字塔"><a href="#3-1-高斯差分金字塔" class="headerlink" title="3.1 高斯差分金字塔"></a>3.1 高斯差分金字塔</h4><p>对高斯金字塔的每一组图像，计算相邻两层图像的差分，得到高斯差分金字塔(DoG, Difference of Gaussian)。</p><script type="math/tex; mode=display">D(x,y,\sigma) = (G(x,y,k\sigma)-G(x,y,\sigma)) * I(x,y)</script><h4 id="3-2-估计极值点"><a href="#3-2-估计极值点" class="headerlink" title="3.2 估计极值点"></a>3.2 估计极值点</h4><p>将每个像素点和它在3×3×3的邻域内的26个像素点进行比较。<br>如果这个点是这27个点中的最大值或最小值，则认为这个点是一个极值点。</p><p>DoG金字塔是离散的(因为尺度空间和像素点都是离散的)，所以找到的极值点不太准确的，很大可能在真正极值点附近，因此需要对每个极值点进行插值。</p><h4 id="3-3-泰勒展开求精确极值点"><a href="#3-3-泰勒展开求精确极值点" class="headerlink" title="3.3 泰勒展开求精确极值点"></a>3.3 泰勒展开求精确极值点</h4><p>对每个极值点$X_0(x_0,y_0,\sigma_0)^T$，进行三元二次泰勒展开：</p><script type="math/tex; mode=display">f(X) = f(x_0)+\dfrac{\partial f}{\partial X}^T\hat{X}+\dfrac{1}{2}\hat{X}^T\dfrac{\partial^2 f}{\partial X^2}\hat{X} \\\\\hat{X} = X-X_0 \\\\X = (x,y,\sigma)^T</script><p>将$f(X)$对$X$求导：</p><script type="math/tex; mode=display">\dfrac{\partial f(X)}{\partial X} = \dfrac{\partial f}{\partial X}^T+\dfrac{1}{2}\left( \dfrac{\partial^2 f}{\partial X^2} + \dfrac{\partial^2 f}{\partial X^2}^T \right)\hat{X} = \dfrac{\partial f}{\partial X}^T+\dfrac{\partial^2 f}{\partial X^2}\hat{X}</script><p>导数为0时，解得极值点：</p><script type="math/tex; mode=display">\hat{X} = -\dfrac{\partial^2 f}{\partial X^2}^{-1}\dfrac{\partial f}{\partial X}</script><p>代入$f(X)$得到极值：</p><script type="math/tex; mode=display">f(X) = f(x_0)+\dfrac{1}{2}\dfrac{\partial f}{\partial X}^T\hat{X}</script><h4 id="3-4-去除低对比度点"><a href="#3-4-去除低对比度点" class="headerlink" title="3.4 去除低对比度点"></a>3.4 去除低对比度点</h4><p>舍去$|f(X_0)|&lt;\dfrac{T}{n}, T=0.04$的极值点，因为这些点对应的差分值太小，对比度低，不具有代表性。</p><h4 id="3-5-去除边缘响应"><a href="#3-5-去除边缘响应" class="headerlink" title="3.5 去除边缘响应"></a>3.5 去除边缘响应</h4><p>DoG在边缘会有比较大的值（边缘响应强），但边缘不一定能够提供稳定的特征，因此需要去除边缘响应强的点。</p><p>对于每个极值点，计算Hessian矩阵的迹和行列式：</p><script type="math/tex; mode=display">H = \begin{bmatrix} D_{xx} & D_{xy} \\ D_{xy} & D_{yy} \end{bmatrix} \\\\Tr(H) = D_{xx}+D_{yy}=\alpha+\beta \\\\Det(H) = D_{xx}D_{yy}-(D_{xy})^2=\alpha\beta</script><p>其中：</p><ul><li>$\alpha$为较大的特征值，$\beta$为较小的特征值</li><li>$\alpha = r \beta$，$r$是一个大于1的实数</li></ul><p>过滤：</p><ol><li>$Det(H)&lt;0$</li><li>$\dfrac{(Tr(H))^2}{Det(H)}\ge (r+1)^2/r$，论文推荐值$r=10$</li></ol><h3 id="4-关键点方向分配"><a href="#4-关键点方向分配" class="headerlink" title="4. 关键点方向分配"></a>4. 关键点方向分配</h3><ol><li>0~360度划分为36 bins，每个bin为10度</li><li>在<strong>高斯金字塔</strong>找到关键点对应位置，以它为圆心，半径为$1.5\sigma$画圆</li><li>统计圆中所有像素的梯度方向、梯度幅值（模），直方图平滑处理，用$1.5\sigma$进行高斯加权<ul><li>平滑处理：防止某个梯度方向角度受噪声干扰等因素突变 </li><li>高斯加权：使特征点附件的梯度幅值具有更大的权重，弥补因没有仿射不变性而导致的特征不稳定</li></ul></li><li>统计出数值最高的梯度方向，作为主方向；保留大于主方向80%的方向作为辅方向</li></ol><p>梯度方向和梯度幅值的计算：</p><script type="math/tex; mode=display">m(x,y) = \sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2} \\\\\theta(x,y) = \arctan\left(\dfrac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right)</script><p>高斯加权公式：</p><script type="math/tex; mode=display">W_{i,j} = e^{\dfrac{-(i^2+j^2)}{2\times(1.5\sigma)^2}}</script><ul><li>$i,j$是像素点到关键点的距离</li></ul><h3 id="5-关键点描述"><a href="#5-关键点描述" class="headerlink" title="5. 关键点描述"></a>5. 关键点描述</h3><p>描述符是一组向量，用于描述特征点及其领域点的特征，以便更好地与其他图片匹配</p><ol><li>将坐轴标移到关键点方向</li><li>将特征点的领域划分为$d\times d$个子区域，每个子区域大小为$m\sigma\times m\sigma$，并划分为8个方向。论文推荐值：$m=3$，$d=4$</li><li>每一个子块进行8个方向的直方图统计操作，共$d\times d\times 8=128$个bin，得到一个128维的特征向量（描述符）</li></ol><h3 id="6-特征点匹配"><a href="#6-特征点匹配" class="headerlink" title="6. 特征点匹配"></a>6. 特征点匹配</h3><p>分别对模板图（参考图，reference image）和实时图（观测图，observation image）建立关键点描述子集合。</p><p>目标的识别是通过两点集内关键点描述子的比对来完成，一般采用欧氏距离来衡量两个描述子之间的相似度。</p><p>可以使用<a href="https://oi-wiki.org/ds/kdt/">KD树</a>优化搜索过程。</p><h2 id="SIFT实现"><a href="#SIFT实现" class="headerlink" title="SIFT实现"></a>SIFT实现</h2><p>cv2中已经给出了SIFT算法的实现，可以直接调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图像</span></span><br><span class="line">img=cv2.imread(<span class="string">&#x27;nz.jpg&#x27;</span>)</span><br><span class="line">cat=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SIFT实例化对象</span></span><br><span class="line">sift=cv2.SIFT_create()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关键点检测：kp关键点信息包括方向，尺度，位置信息，des是关键点的描述符</span></span><br><span class="line">kp,des=sift.detectAndCompute(cat,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在图像上绘制关键点的检测结果</span></span><br><span class="line">cv2.drawKeypoints(img,kp,img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像显示</span></span><br><span class="line">plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))</span><br><span class="line">plt.title(<span class="string">&#x27;SIFT Result&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://source.cclmsy.cc/Posts/DL/Note/SIFT_Result.png" alt="SIFT Result"></p>]]></content>
    
    
    <summary type="html">尺度不变特征变换匹配算法 Scale-Invariant Feature Transform</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="计算机视觉" scheme="https://www.cclmsy.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像检索" scheme="https://www.cclmsy.cc/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>复现|ChatIR：基于对话的图像检索系统</title>
    <link href="https://www.cclmsy.cc/posts/ChatIR.html"/>
    <id>https://www.cclmsy.cc/posts/ChatIR.html</id>
    <published>2025-03-07T04:00:00.000Z</published>
    <updated>2025-03-11T12:51:49.062Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/2305.20062">Chatting Makes Perfect: Chat-based Image Retrival</a></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>ChatIR系统包含2个部分：对话构建（Dialog Building）和图像搜索（Image Search）</p><ul><li>对话构建：使用问题生成器G，考虑当前对话历史，生成下一个问题</li><li>图像搜索：使用模型F，将不同长度的对话序列映射到视觉嵌入空间</li><li>两个组成部分建立在Instructional LLMs和fundation Vision and Language Models上</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/ChatIR.png" alt="Figure1"></p><p>考虑三个问题：</p><ol><li>使用什么数据集训练？是否需要新创建和标注数据集？<ul><li>使用VisDial数据集</li><li>问题：VisDial是一个用于“创建关于图像的聊天”的数据集，没有检索目标</li><li>解决：输入输出反置，对话作为输入、图像作为输出</li></ul></li><li>如何独立评估ChatIR系统的不同组件？ <ul><li>测试使用不同的<code>F训练策略</code>和<code>提问模型G</code>对检索性能的影响</li><li>使用了BLIP替代用户回答问题</li></ul></li><li>如何定义评估指标？<ul><li>每一轮对话的成功检索概率Hit@10</li></ul></li></ol><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><ol><li>视觉对话（Visual Conversation）领域<ul><li>当前视觉领域工作的重点在于图像理解和生成模型，而不是检索</li><li>生成式视觉对话中，近期的基础模型V&amp;L性能优越，因此ChatIR系统在此基础上构建</li></ul></li><li>视觉搜索（Visual Search）领域<ul><li>CoIR：使用多模态询问查找目标图像</li><li>一些研究基于CoIR，利用用户反馈细化查询结果</li><li>但是，没有考虑用户交互（只有用户反馈，没有机器提问）、没有明确利用对话历史</li></ul></li></ol><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><h3 id="3-1-Dialog-Builder-Model-对话生成模型"><a href="#3-1-Dialog-Builder-Model-对话生成模型" class="headerlink" title="3.1 Dialog Builder Model 对话生成模型"></a>3.1 Dialog Builder Model 对话生成模型</h3><p>对话记录表示：$D_i = (C, Q_1, A_1, …, Q_i)$</p><ul><li>$C$：目标图像的初始文本描述（标题）</li><li>$Q_i$：第i个问题</li><li>$A_i$：第i个回答</li></ul><p>对话生成模型包含两个部分：</p><ul><li>问题生成器G：一个LLM，根据对话记录生成下一个问题<ul><li>$G: D<em>i \rightarrow Q</em>{i+1}$</li><li>G不知道目标图像T是什么，只知道对话历史</li></ul></li><li>答案提供者A：在实践中，通常是一个脑海中有大致目标图像的人类<ul><li>由于需要大规模实验，不能依赖用户提供答案</li><li>因此，使用了一个现成的模型BLIP2来回答</li></ul></li></ul><h3 id="3-2-Image-Retrieval-Model-图像检索模型"><a href="#3-2-Image-Retrieval-Model-图像检索模型" class="headerlink" title="3.2 Image Retrieval Model 图像检索模型"></a>3.2 Image Retrieval Model 图像检索模型</h3><p>图像搜索过程：在<code>查询</code>和<code>目标图像</code>共享的视觉嵌入空间中，搜索匹配项。</p><ul><li>所有的目标图像先经过图像嵌入模块进行编码，由一个$d$维的特征向量$f\in \mathbb R^d$表示。</li><li>图像检索模块F将对话历史$D_i$映射到视觉嵌入空间，$F: D_i \rightarrow \mathbb R^d$，</li><li>候选对象根据相似度进行排序。</li></ul><p>引入分隔符[SEP]和添加符[CLS]，表示整个对话序列，投射到视觉嵌入空间。</p><p>F采用（使用BLIP）预训练的图像/文本编码器，并通过对比学习，对基于对话的检索进行微调。</p><p>通过提取VisDial数据集中的图像和相应对话，手动标注，训练F。</p><h2 id="4-Evaluation"><a href="#4-Evaluation" class="headerlink" title="4. Evaluation"></a>4. Evaluation</h2><p>在评估环节，原文使用了Hit@10指标，即目标图像在前10个检索结果中的试验占比。</p><p>原文从三个方面进行了对比：</p><ol><li>与现有文本到图像（Text to Image,TTI）检索方法的比较<ol><li>ChatIR使用ChatGPT作为提问者G，BLIP2作为回答者A</li><li>与Zero-shot的BLIP、CLIP以及fine-tuned SoTA TTI BLIP进行比较</li><li>结论：ChatIR在多轮对话环境中，相比传统单跳 TTI 方法表现更优</li></ol></li><li>不同提问者G的比较<ol><li>使用ChatGPT、FLAN-ALPACA-XXL、人类等8中不同提问者</li><li>ChatGPT表现最好</li></ol></li><li>人类参与对话的影响<ol><li>ChatGPT提问，人类回答；ChatGPT提问，BLIP2回答；人类提问，人类回答</li><li>由于人类生成的答案质量明显优于BLIP2，因此人类参与对话时，检索性能会比测试的数据更好</li></ol></li></ol><h2 id="总结和复现"><a href="#总结和复现" class="headerlink" title="总结和复现"></a>总结和复现</h2><ul><li>ChatIR系统是一个基于对话的图像检索系统，包含对话构建和图像搜索两个部分</li><li>对话构建：使用问题生成器G，考虑当前对话历史，生成下一个问题<ul><li>原文测试了不同的问题生成器，其中ChatGPT表现最好</li></ul></li><li>图像搜索：使用模型F，将不同长度的对话序列映射到视觉嵌入空间<ul><li>我使用了论文仓库提供的预训练BLIP_ITM模型</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms.functional <span class="keyword">import</span> InterpolationMode</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="comment"># 多进程处理</span></span><br><span class="line">multiprocessing.set_start_method(<span class="string">&#x27;spawn&#x27;</span>, force=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> OpenAI <span class="keyword">import</span> request_chat <span class="comment"># 自己实现的调用API函数 request_chat(dialog:list) -&gt; new_question:str</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys <span class="comment"># 添加import路径</span></span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">&#x27;./BLIP&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> BLIP.models.blip_itm <span class="keyword">import</span> blip_itm <span class="comment"># BLIP用于图像文本匹配的预训练模型</span></span><br><span class="line"></span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&quot;corpus_path&quot;</span>: <span class="string">&quot;VisualDial/search_space.json&quot;</span>, <span class="comment"># 图像库路径</span></span><br><span class="line">    <span class="string">&quot;queries_path&quot;</span>: <span class="string">&quot;dialogues/ChatGPT4oMini_BLIP2.json&quot;</span>, <span class="comment"># 测试对话数据路径</span></span><br><span class="line">    <span class="string">&quot;corpus_cache&quot;</span>: <span class="string">&quot;VisualDial/corpus_cache.pth&quot;</span>, <span class="comment"># 处理好的图像库缓存的路径</span></span><br><span class="line">    <span class="string">&quot;device&quot;</span>: <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>,</span><br><span class="line">    <span class="string">&quot;sep&quot;</span>: <span class="string">&quot;, &quot;</span>, <span class="comment"># 对话分隔符</span></span><br><span class="line">    <span class="string">&quot;batch_size&quot;</span>: <span class="number">100</span>, <span class="comment"># 批处理大小</span></span><br><span class="line">    <span class="string">&quot;num_workers&quot;</span>: <span class="number">8</span>, <span class="comment"># 多进程处理数</span></span><br><span class="line">    <span class="string">&quot;image_size&quot;</span> : <span class="number">224</span> <span class="comment"># 图像大小</span></span><br><span class="line">&#125;</span><br><span class="line">corpus = <span class="literal">None</span> <span class="comment"># 处理好的图像库 </span></span><br><span class="line">dialog = [] <span class="comment"># 对话</span></span><br><span class="line">images = [] <span class="comment"># 图像库路径 list[图像路径]</span></span><br></pre></td></tr></table></figure><h3 id="图像数据集类"><a href="#图像数据集类" class="headerlink" title="图像数据集类"></a>图像数据集类</h3><p>继承自Dataset，用于加载图像数据集</p><p>corpus_path：图像数据集.json，包含一个list[str]，每个元素是一个图像的路径</p><p>建立路径字符串到索引的映射，便于赋值传递和查询</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Corpus</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;图像数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus_path, preprocessor</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;加载图像数据集</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            corpus_path: 图片路径列表</span></span><br><span class="line"><span class="string">            preprocessor: 图像预处理函数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(corpus_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            self.corpus = json.load(f)</span><br><span class="line">            f.close()</span><br><span class="line">        self.preprocessor = preprocessor</span><br><span class="line">        <span class="comment"># 图片路径到索引的映射，用于快速查找</span></span><br><span class="line">        self.path2idx = &#123;self.corpus[i]:i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.corpus))&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.corpus)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        image = self.preprocessor(self.corpus[idx])</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;idx&quot;</span>: idx, <span class="string">&quot;image&quot;</span>: image&#125;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">path_to_index</span>(<span class="params">self, path</span>):</span><br><span class="line">        <span class="keyword">return</span> self.path2idx[path]</span><br></pre></td></tr></table></figure><h3 id="图像预处理函数"><a href="#图像预处理函数" class="headerlink" title="图像预处理函数"></a>图像预处理函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">image_preprocessor</span>(<span class="params">image_path</span>):</span><br><span class="line">    transform_prep = transforms.Compose([ </span><br><span class="line">        transforms.Resize((config[<span class="string">&quot;image_size&quot;</span>], config[<span class="string">&quot;image_size&quot;</span>]), </span><br><span class="line">                          interpolation=InterpolationMode.BICUBIC),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.48145466</span>, <span class="number">0.4578275</span>, <span class="number">0.40821073</span>), </span><br><span class="line">                             (<span class="number">0.26862954</span>, <span class="number">0.26130258</span>, <span class="number">0.27577711</span>)) <span class="comment"># 参数参考BLIP的demo</span></span><br><span class="line">    ])</span><br><span class="line">    raw = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">    img = transform_prep(raw)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><h3 id="BLIP-ITM模型的图像编码器和对话编码器"><a href="#BLIP-ITM模型的图像编码器和对话编码器" class="headerlink" title="BLIP_ITM模型的图像编码器和对话编码器"></a>BLIP_ITM模型的图像编码器和对话编码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_funcs</span>():</span><br><span class="line">    <span class="comment"># model_url = &#x27;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth&#x27;</span></span><br><span class="line">    model = blip_itm(pretrained=<span class="string">&#x27;chatir_weights.ckpt&#x27;</span>, <span class="comment"># 论文仓库中的预训练模型</span></span><br><span class="line">                    med_config=<span class="string">&quot;BLIP/configs/med_config.json&quot;</span>, </span><br><span class="line">                    image_size=config[<span class="string">&quot;image_size&quot;</span>],</span><br><span class="line">                    vit=<span class="string">&quot;base&quot;</span>)</span><br><span class="line">    device = config[<span class="string">&quot;device&quot;</span>]</span><br><span class="line">    model = model.to(device).<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">image_encoder</span>(<span class="params">img</span>):</span><br><span class="line">        embeddings = model.visual_encoder(img) <span class="comment"># embedding</span></span><br><span class="line">        <span class="comment"># print(embeddings.shape) # (批次大小, patch个数+1, 隐层维度)</span></span><br><span class="line">        vision_proj = model.vision_proj(embeddings[:, <span class="number">0</span>, :]) <span class="comment"># 取[CLS] token，提取全局特征</span></span><br><span class="line">        <span class="keyword">return</span> F.normalize(vision_proj, dim=-<span class="number">1</span>) <span class="comment"># 正则化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dialog_encoder</span>(<span class="params">dialog</span>):</span><br><span class="line">        text = model.tokenizer(dialog, padding=<span class="string">&#x27;longest&#x27;</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">200</span>, <span class="comment"># 填充到最长，截断到200</span></span><br><span class="line">                            return_tensors=<span class="string">&quot;pt&quot;</span>).to(device)  <span class="comment"># 返回PyTorch张量</span></span><br><span class="line">        text_out = model.text_encoder(text.input_ids, attention_mask=text.attention_mask, </span><br><span class="line">                                    return_dict=<span class="literal">True</span>, mode=<span class="string">&#x27;text&#x27;</span>) <span class="comment"># embedding</span></span><br><span class="line">        shift = model.text_proj(text_out.last_hidden_state[:, <span class="number">0</span>, :]) <span class="comment"># 同</span></span><br><span class="line">        <span class="keyword">return</span> F.normalize(shift, dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image_encoder, dialog_encoder</span><br></pre></td></tr></table></figure><h3 id="处理图像库"><a href="#处理图像库" class="headerlink" title="处理图像库"></a>处理图像库</h3><p>由于加载时间长，一次加载后将数据存储在本地，便于二次调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">corpus_dataset = Corpus(config[<span class="string">&quot;corpus_path&quot;</span>], image_preprocessor)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_corpus</span>(<span class="params">image_encoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;处理图像库</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        corpus: tuple[torch.Tensor, torch.Tensor] 图像库索引和对应的图像特征向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">global</span> corpus</span><br><span class="line">    corpus_cache = config[<span class="string">&quot;corpus_cache&quot;</span>]</span><br><span class="line">    <span class="keyword">if</span> corpus_cache <span class="keyword">and</span> os.path.exists(corpus_cache): <span class="comment"># 读取缓存</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;-----Loading corpus from <span class="subst">&#123;corpus_cache&#125;</span>-----&quot;</span>)</span><br><span class="line">        corpus = torch.load(corpus_cache)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----Preparing corpus-----&quot;</span>)</span><br><span class="line">    corpus_dataloader = DataLoader( <span class="comment"># 图像库的DataLoader</span></span><br><span class="line">        corpus_dataset,</span><br><span class="line">        batch_size=config[<span class="string">&quot;batch_size&quot;</span>],</span><br><span class="line">        shuffle=<span class="literal">False</span>,</span><br><span class="line">        num_workers=config[<span class="string">&quot;num_workers&quot;</span>],</span><br><span class="line">        pin_memory=<span class="literal">True</span>,</span><br><span class="line">        drop_last=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">    corpus_vectors = []</span><br><span class="line">    corpus_ids = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(corpus_dataloader): <span class="comment"># 预处理图像库</span></span><br><span class="line">        batch_vectors = F.normalize(image_encoder(batch[<span class="string">&quot;image&quot;</span>].to(config[<span class="string">&quot;device&quot;</span>])), dim=-<span class="number">1</span>) <span class="comment"># 正则化</span></span><br><span class="line">        corpus_vectors.append(batch_vectors)</span><br><span class="line">        corpus_ids.append(batch[<span class="string">&quot;idx&quot;</span>].to(config[<span class="string">&quot;device&quot;</span>]))</span><br><span class="line"></span><br><span class="line">    corpus_vectors = torch.cat(corpus_vectors)</span><br><span class="line">    corpus_ids = torch.cat(corpus_ids)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照索引排序</span></span><br><span class="line">    arg_ids = torch.argsort(corpus_ids)</span><br><span class="line">    corpus_vectors = corpus_vectors[arg_ids]</span><br><span class="line">    corpus_ids = corpus_ids[arg_ids]</span><br><span class="line"></span><br><span class="line">    corpus = corpus_ids, corpus_vectors</span><br><span class="line">    <span class="keyword">if</span> config[<span class="string">&quot;corpus_cache&quot;</span>]:</span><br><span class="line">        torch.save(corpus, config[<span class="string">&quot;corpus_cache&quot;</span>]) </span><br></pre></td></tr></table></figure><h3 id="提问与匹配函数"><a href="#提问与匹配函数" class="headerlink" title="提问与匹配函数"></a>提问与匹配函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ask_for_caption</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;询问用户描述&quot;&quot;&quot;</span></span><br><span class="line">    caption = <span class="built_in">input</span>(<span class="string">&quot;Describe the image: &quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> caption</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_question</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;询问用户问题，获取回答&quot;&quot;&quot;</span></span><br><span class="line">    question = request_chat(dialog)</span><br><span class="line">    answer = <span class="built_in">input</span>(<span class="string">f&quot;Q: <span class="subst">&#123;question&#125;</span>\nA: &quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> question+<span class="string">&#x27; &#x27;</span>+answer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_top_results</span>(<span class="params">dialog, dialog_encoder, n=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取前n最佳匹配结果</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dialog: str 对话</span></span><br><span class="line"><span class="string">        dialog_encoder: 对话编码器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tops: list[int] 前n个匹配结果的索引</span></span><br><span class="line"><span class="string">        topscores: list[float] 前n个匹配结果的得分</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dialog = config[<span class="string">&quot;sep&quot;</span>].join(dialog)</span><br><span class="line">    dialog_vector = dialog_encoder(dialog) <span class="comment"># 提取特征&amp;正则化</span></span><br><span class="line">    scores = dialog_vector @ corpus[<span class="number">1</span>].T <span class="comment"># 计算点积相似度</span></span><br><span class="line">    top_id = torch.argsort(scores, descending=<span class="literal">True</span>) <span class="comment"># 排序</span></span><br><span class="line">    tops = top_id.tolist()[<span class="number">0</span>][:n]</span><br><span class="line">    topscores = scores[<span class="number">0</span>][tops].tolist()</span><br><span class="line">    <span class="keyword">return</span> tops, topscores</span><br></pre></td></tr></table></figure><h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    image_encoder, dialog_encoder = get_funcs()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(config[<span class="string">&quot;corpus_path&quot;</span>], <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = json.load(f)</span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        prepare_corpus(image_encoder)</span><br><span class="line">    </span><br><span class="line">        dialog.append(ask_for_caption())</span><br><span class="line">        tops,topscores = get_top_results(dialog, dialog_encoder)</span><br><span class="line">        best_image = images[tops[<span class="number">0</span>]]</span><br><span class="line">        best_score = topscores[<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Best image: <span class="subst">&#123;best_image&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Best score: <span class="subst">&#123;best_score&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="comment"># display(Image.open(best_image))</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            dialog.append(ask_question())</span><br><span class="line">            tops,topscores = get_top_results(dialog, dialog_encoder)</span><br><span class="line">            best_image = images[tops[<span class="number">0</span>]]</span><br><span class="line">            best_score = topscores[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Best image: <span class="subst">&#123;best_image&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Best score: <span class="subst">&#123;best_score&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">1</span>:</span><br><span class="line">                display(Image.<span class="built_in">open</span>(best_image))</span><br></pre></td></tr></table></figure><h2 id="测试和评估"><a href="#测试和评估" class="headerlink" title="测试和评估"></a>测试和评估</h2><p>原文评估ChatIR性能的指标是Hit@10，即目标图像出现在最匹配的10个候选图像中的概率，我采用相同的评估方式</p><h3 id="对话数据集类"><a href="#对话数据集类" class="headerlink" title="对话数据集类"></a>对话数据集类</h3><p>单个测试对话数据结构：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;image&quot;</span><span class="punctuation">:</span> <span class="string">&quot;image_path&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;dialog&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;caption&quot;</span><span class="punctuation">,</span> <span class="string">&quot;question1? answer1&quot;</span><span class="punctuation">,</span> <span class="string">&quot;question2? answer2&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Queries</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对话-图像数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, queries_path, sep</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;加载对话-图像数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            queries_path: str 查询数据集路径</span></span><br><span class="line"><span class="string">            sep: str 分隔符</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(queries_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            self.queries = json.load(f)</span><br><span class="line">            f.close()</span><br><span class="line">        self.dialog_length = <span class="literal">None</span></span><br><span class="line">        self.sep = sep</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.queries)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">assert</span> self.dialog_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        target_path = self.queries[idx][<span class="string">&quot;img&quot;</span>]</span><br><span class="line">        <span class="comment"># 保留对话的前dialog_length轮</span></span><br><span class="line">        text = self.sep.join(self.queries[idx][<span class="string">&quot;dialog&quot;</span>][:self.dialog_length + <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;text&quot;</span>: text, <span class="string">&quot;target_path&quot;</span>: target_path&#125;</span><br></pre></td></tr></table></figure><h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><p>使用了论文仓库提供的测试代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">corpus_dataset = Corpus(config[<span class="string">&quot;corpus_path&quot;</span>], image_preprocessor)</span><br><span class="line">query_dataset = Queries(config[<span class="string">&quot;queries_path&quot;</span>], config[<span class="string">&quot;sep&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_recalls</span>(<span class="params">dataloader, dialog_length, dialog_encoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 计算dataloader中长度为dialog_length的对话的召回结果</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataloader: 数据加载器</span></span><br><span class="line"><span class="string">        dialog_length: 对话长度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dataloader.dataset.dialog_length = dialog_length  <span class="comment"># 设置对话长度</span></span><br><span class="line">    recalls = []  <span class="comment"># 每个对话的召回结果</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        target_ids = torch.tensor(</span><br><span class="line">            [corpus_dataset.path_to_index(p) <span class="keyword">for</span> p <span class="keyword">in</span> batch[<span class="string">&#x27;target_path&#x27;</span>]]</span><br><span class="line">            ).unsqueeze(<span class="number">1</span>).to(config[<span class="string">&#x27;device&#x27;</span>]) <span class="comment"># 图片路径转换为索引</span></span><br><span class="line">        pred_vec = F.normalize(dialog_encoder(batch[<span class="string">&#x27;text&#x27;</span>]), dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        scores = pred_vec @ corpus[<span class="number">1</span>].T <span class="comment"># 计算点积，得到相似度分数</span></span><br><span class="line">        arg_ranks = torch.argsort(scores, descending=<span class="literal">True</span>, dim=<span class="number">1</span>).long() <span class="comment"># 对分数进行排序</span></span><br><span class="line">        </span><br><span class="line">        target_recall = ((arg_ranks - target_ids) == <span class="number">0</span>).nonzero()[:, <span class="number">1</span>] <span class="comment"># 目标图像在检索排名中出现的位置</span></span><br><span class="line">        recalls.append(target_recall)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.cat(recalls)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_first_hitting_time</span>(<span class="params">target_recall, hitting_recall=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 返回(11, n)张量，其中包含每轮（0, 11）的命中时间。inf表示未命中（10轮后没有命中） &quot;&quot;&quot;</span></span><br><span class="line">    target_recalls = target_recall.view(<span class="number">11</span>, -<span class="number">1</span>).T <span class="comment"># 转置</span></span><br><span class="line">    hits = (target_recalls &lt; hitting_recall) <span class="comment"># 目标图像是否在前 hitting_recall 轮内出现</span></span><br><span class="line"></span><br><span class="line">    final_hits = torch.inf * torch.ones(target_recalls.shape[<span class="number">0</span>]) <span class="comment"># 初始化为inf</span></span><br><span class="line"></span><br><span class="line">    hitting_times = [] <span class="comment"># 每轮的命中时间</span></span><br><span class="line">    <span class="keyword">for</span> ro_i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">        rh = hits[:, ro_i]</span><br><span class="line">        final_hits[rh] = torch.<span class="built_in">min</span>(final_hits[rh], torch.ones(final_hits[rh].shape) * ro_i)</span><br><span class="line">        hitting_times.append(final_hits.clone())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.stack(hitting_times)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumulative_hits_per_round</span>(<span class="params">target_recall, hitting_recall=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 返回直到第x轮的平均命中次数 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(hitting_recall) <span class="keyword">is</span> <span class="built_in">tuple</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(hitting_recall) == <span class="number">1</span></span><br><span class="line">        hitting_recall = hitting_recall[<span class="number">0</span>]</span><br><span class="line">    ht_times = get_first_hitting_time(target_recall, hitting_recall)</span><br><span class="line">    <span class="keyword">return</span> ((ht_times &lt; torch.inf).<span class="built_in">sum</span>(dim=-<span class="number">1</span>) * <span class="number">100</span> / ht_times[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eval</span>(<span class="params">image_encoder, dialog_encoder, hits_at=<span class="number">10</span></span>):</span><br><span class="line">    prepare_corpus(image_encoder)</span><br><span class="line">    query_dataloader = torch.utils.data.DataLoader(query_dataset, <span class="comment"># 询问数据集</span></span><br><span class="line">                                             batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], <span class="comment"># 批大小</span></span><br><span class="line">                                             shuffle=<span class="literal">False</span>, <span class="comment"># 不打乱</span></span><br><span class="line">                                             num_workers=config[<span class="string">&#x27;num_workers&#x27;</span>], <span class="comment"># 多线程</span></span><br><span class="line">                                             pin_memory=<span class="literal">True</span>, <span class="comment"># 锁页内存</span></span><br><span class="line">                                             drop_last=<span class="literal">False</span></span><br><span class="line">                                             )</span><br><span class="line">    hits_results = []</span><br><span class="line">    <span class="keyword">for</span> dl <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>): <span class="comment"># 对话长度从0到10</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Calculate recalls for each dialogues of length <span class="subst">&#123;dl&#125;</span>...&quot;</span>)</span><br><span class="line">        dialog_recalls = _get_recalls(query_dataloader, dialog_length=dl, dialog_encoder=dialog_encoder)</span><br><span class="line">        hits_results.append(dialog_recalls)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用cumulative_hits_per_round计算最终的Hits@10结果</span></span><br><span class="line">    <span class="comment"># Hits@10：`在预测的前 10 个候选项中，包含了正确答案`的比例</span></span><br><span class="line">    hits_results = cumulative_hits_per_round(torch.cat(hits_results).cpu(), hitting_recall=<span class="number">10</span>).tolist()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;====== Results for Hits@10 ====== &quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> dl <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\t Dialog Length: <span class="subst">&#123;dl&#125;</span>: <span class="subst">&#123;<span class="built_in">round</span>(hits_results[dl], <span class="number">2</span>)&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    image_encoder, dialog_encoder = get_funcs()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(config[<span class="string">&quot;corpus_path&quot;</span>], <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = json.load(f)</span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="built_in">eval</span>(image_encoder, dialog_encoder)</span><br></pre></td></tr></table></figure><h3 id="对话数据生成"><a href="#对话数据生成" class="headerlink" title="对话数据生成"></a>对话数据生成</h3><p>原文中，为了自动获取测试数据，免除人工回答，使用了BLIP2模型回答问题</p><p>我参考原文的方法，使用BLIP2生成对话数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lavis.models <span class="keyword">import</span> load_model_and_preprocess <span class="comment"># BLIP2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> OpenAI <span class="keyword">import</span> request_chat</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">st,ed = <span class="built_in">int</span>(sys.argv[<span class="number">1</span>]), <span class="built_in">int</span>(sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;../dialogues/ChatGPT_BLIP2.json&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = json.load(f)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">images = [d[<span class="string">&quot;img&quot;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> data][st:ed]</span><br><span class="line">captions = [d[<span class="string">&quot;dialog&quot;</span>][<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> data][st:ed]</span><br><span class="line"></span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&quot;device&quot;</span>: <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>,</span><br><span class="line">    <span class="string">&quot;image_size&quot;</span> : <span class="number">224</span>,</span><br><span class="line">    <span class="string">&quot;sep&quot;</span>: <span class="string">&quot;, &quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> config[<span class="string">&quot;device&quot;</span>] == <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Using GPU&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_image</span>(<span class="params">image_path</span>):</span><br><span class="line">    raw_image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">    image = vis_processors[<span class="string">&quot;eval&quot;</span>](raw_image).unsqueeze(<span class="number">0</span>).to(config[<span class="string">&quot;device&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visual_qa</span>(<span class="params">image, question, model</span>):</span><br><span class="line">    answer = model.generate(&#123;<span class="string">&quot;image&quot;</span>: image, <span class="string">&quot;prompt&quot;</span>: <span class="string">f&quot;Question: <span class="subst">&#123;question&#125;</span> Answer:&quot;</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> answer[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_question</span>(<span class="params">dialog</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;询问用户问题&quot;&quot;&quot;</span></span><br><span class="line">    question = request_chat(dialog)</span><br><span class="line">    <span class="keyword">return</span> question</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_dialog</span>(<span class="params">idx, model</span>):</span><br><span class="line">    image = load_image(images[idx])</span><br><span class="line">    caption = captions[idx]</span><br><span class="line">    dialog = [caption]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        question = ask_question(dialog)</span><br><span class="line">        answer = visual_qa(image, question, model)</span><br><span class="line">        dialog.append(question+answer)</span><br><span class="line">    ret = &#123;</span><br><span class="line">        <span class="string">&quot;img&quot;</span>: images[idx],</span><br><span class="line">        <span class="string">&quot;dialog&quot;</span>: dialog</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    model, vis_processors, _ = load_model_and_preprocess(</span><br><span class="line">        name=<span class="string">&quot;blip2_opt&quot;</span>, model_type=<span class="string">&quot;caption_coco_opt6.7b&quot;</span>, is_eval=<span class="literal">True</span>, device=config[<span class="string">&quot;device&quot;</span>]</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;ChatGPT4oMini_BLIP2.txt&quot;</span>, <span class="string">&quot;a&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="built_in">len</span>(images))):</span><br><span class="line">                dialog = generate_dialog(idx, model)</span><br><span class="line">                json.dump(dialog, f)</span><br><span class="line">                f.write(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">            f.close()</span><br></pre></td></tr></table></figure><h3 id="测试日志"><a href="#测试日志" class="headerlink" title="测试日志"></a>测试日志</h3><ol><li>由于OpenAI的token限制，我使用了讯飞星火API作为提问模型G</li><li>论文没读仔细，一开始以为生成对话数据时，代替人类回答的模型和编码用的模型一样，是BLIP</li><li>在以上基础上，测试的Hit@10结果（40%~60%）与原文（63%~80%）有较大差距</li><li>原本认为是提问模型G没有用ChatGPT的原因，微氪token，调整为ChatGPT4o-mini，但结果依然不理想（40%~67%）</li><li>注意到，40%是$D_0$，也就是只有用BLIP生成的第一句描述时的准确率，和提问模型G无关</li><li>对比原文，发现回答模型应该是另一篇paper中的BLIP2，而不是BLIP</li><li>修改数据生成代码，使用BLIP2生成对话数据，测试结果与原文接近（60%~80%）</li></ol><p>客观问题：由于硬件条件有限，跑出一条数据需要3~5分钟，因此只测试了1021条数据，对于整体性能评估可能不够准确</p><h3 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h3><p>在使用ChatGPT作为提问模型G，BLIP2作为回答模型A，使用同一测试代码的情况下，测试结果和原文对比如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">length</th><th style="text-align:center">原文(2064 testcases)</th><th style="text-align:center">复现（1021 testcases）</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">63.42%</td><td style="text-align:center">62.98%</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">69.43%</td><td style="text-align:center">70.62%</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">72.38%</td><td style="text-align:center">72.67%</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">74.47%</td><td style="text-align:center">74.53%</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">76.02%</td><td style="text-align:center">75.32%</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">77.47%</td><td style="text-align:center">75.42%</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">78.49%</td><td style="text-align:center">76.00%</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">79.65%</td><td style="text-align:center">76.20%</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">80.09%</td><td style="text-align:center">76.69%</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">80.43%</td><td style="text-align:center">76.98%</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">80.77%</td><td style="text-align:center">77.18%</td></tr></tbody></table></div><p>在第五轮对话后，对话长度增加对检索性能的提升不再明显</p>]]></content>
    
    
    <summary type="html">复现论文《Chatting Makes Perfect：Chat-based Image Retrival》</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="论文复现" scheme="https://www.cclmsy.cc/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>Pretrained BLIP 模型调用</title>
    <link href="https://www.cclmsy.cc/posts/pretrained_BLIP.html"/>
    <id>https://www.cclmsy.cc/posts/pretrained_BLIP.html</id>
    <published>2025-03-03T04:00:00.000Z</published>
    <updated>2025-03-02T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/salesforce/BLIP">Github Repo</a></p><p>BLIP全称Bootstrapping Language-Image Pre-training，是一个全新的 统一视觉语言理解与生成的预训练模型</p><p>它统一了视觉语言任务的理解与生成功能，并且通过嵌入 Captioner 和 Filter 去除网络资源中的文本噪声，提高了模型在下游视觉语言任务上的性能</p><h2 id="Colab-Notebook-运行"><a href="#Colab-Notebook-运行" class="headerlink" title="Colab Notebook 运行"></a>Colab Notebook 运行</h2><p>Salesforce在Colab NoteBook中提供了Demo，可以在云端直接运行：<a href="https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb">Colab notebook</a></p><p>第一步配置环境时出现<code>ERROR: Failed building wheel for tokenizers</code>问题，在<a href="https://github.com/salesforce/BLIP/issues/151#issuecomment-1537125671">Issue#151的评论</a>中提供了解决方案：修改transformers版本<code>4.25.1</code></p><h2 id="本地调用"><a href="#本地调用" class="headerlink" title="本地调用"></a>本地调用</h2><p>学习在本地使用Pretrained BLIP模型，测试代码置于BLIP目录下</p><p>实验环境：</p><ul><li>Python 3.11.11 on conda</li><li>transformers 4.25.1</li><li>timm 0.4.12</li><li>fairscale 0.4.4</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms.functional <span class="keyword">import</span> InterpolationMode</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_demo_image</span>(<span class="params">image_size,device</span>):</span><br><span class="line">    img_url = <span class="string">&#x27;https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg&#x27;</span> </span><br><span class="line">    raw_image = Image.<span class="built_in">open</span>(requests.get(img_url, stream=<span class="literal">True</span>).raw).convert(<span class="string">&#x27;RGB&#x27;</span>)   </span><br><span class="line"></span><br><span class="line">    w,h = raw_image.size</span><br><span class="line">    <span class="built_in">print</span>(raw_image.resize((w//<span class="number">5</span>,h//<span class="number">5</span>)))</span><br><span class="line">    </span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.48145466</span>, <span class="number">0.4578275</span>, <span class="number">0.40821073</span>), (<span class="number">0.26862954</span>, <span class="number">0.26130258</span>, <span class="number">0.27577711</span>))</span><br><span class="line">        ]) </span><br><span class="line">    image = transform(raw_image).unsqueeze(<span class="number">0</span>).to(device)   </span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h3 id="Image-Captioning-图像描述"><a href="#Image-Captioning-图像描述" class="headerlink" title="Image Captioning 图像描述"></a>Image Captioning 图像描述</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> models.blip <span class="keyword">import</span> blip_decoder</span><br><span class="line"></span><br><span class="line">image_size = <span class="number">384</span></span><br><span class="line">image = load_demo_image(image_size=image_size, device=device)</span><br><span class="line"></span><br><span class="line">model_url = <span class="string">&#x27;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&#x27;</span></span><br><span class="line">    </span><br><span class="line">model = blip_decoder(pretrained=model_url, image_size=image_size, vit=<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># beam search </span></span><br><span class="line">    caption = model.generate(image, sample=<span class="literal">False</span>, num_beams=<span class="number">3</span>, max_length=<span class="number">20</span>, min_length=<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># nucleus sampling</span></span><br><span class="line">    <span class="comment"># caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)  </span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;caption: &#x27;</span>+caption[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.Moving 0 files to the new cache system0it [00:00, ?it/s]&lt;PIL.Image.Image image mode=RGB size=409x273 at 0x11139327650&gt;reshape position embedding from 196 to 576load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pthcaption: a woman and her dog on the beach</code></pre><h3 id="Visual-Question-Answering-视觉问答"><a href="#Visual-Question-Answering-视觉问答" class="headerlink" title="Visual Question Answering 视觉问答"></a>Visual Question Answering 视觉问答</h3><p>由于存在和ImageCaptioning中的beam_search相同的版本问题，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> models.blip_vqa <span class="keyword">import</span> blip_vqa</span><br><span class="line"></span><br><span class="line">image_size = <span class="number">480</span></span><br><span class="line">image = load_demo_image(image_size=image_size, device=device)     </span><br><span class="line"></span><br><span class="line">model_url = <span class="string">&#x27;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth&#x27;</span></span><br><span class="line">    </span><br><span class="line">model = blip_vqa(pretrained=model_url, image_size=image_size, vit=<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line">question = <span class="string">&#x27;where is the woman sitting?&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    answer = model(image, question, train=<span class="literal">False</span>, inference=<span class="string">&#x27;generate&#x27;</span>) </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;answer: &#x27;</span>+answer[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>&lt;PIL.Image.Image image mode=RGB size=409x273 at 0x11139109090&gt;load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pthanswer: on beach</code></pre><h3 id="Image-Text-Matching-图文匹配"><a href="#Image-Text-Matching-图文匹配" class="headerlink" title="Image-Text Matching 图文匹配"></a>Image-Text Matching 图文匹配</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> models.blip_itm <span class="keyword">import</span> blip_itm</span><br><span class="line"></span><br><span class="line">image_size = <span class="number">384</span></span><br><span class="line">image = load_demo_image(image_size=image_size,device=device)</span><br><span class="line"></span><br><span class="line">model_url = <span class="string">&#x27;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth&#x27;</span></span><br><span class="line">    </span><br><span class="line">model = blip_itm(pretrained=model_url, image_size=image_size, vit=<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">model = model.to(device=device)</span><br><span class="line"></span><br><span class="line">caption = <span class="string">&#x27;a woman sitting on the beach with a dog&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;text: %s&#x27;</span> %caption)</span><br><span class="line"></span><br><span class="line">itm_output = model(image,caption,match_head=<span class="string">&#x27;itm&#x27;</span>)</span><br><span class="line">itm_score = torch.nn.functional.softmax(itm_output,dim=<span class="number">1</span>)[:,<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The image and text is matched with a probability of %.4f&#x27;</span>%itm_score)</span><br><span class="line"></span><br><span class="line">itc_score = model(image,caption,match_head=<span class="string">&#x27;itc&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The image feature and text feature has a cosine similarity of %.4f&#x27;</span>%itc_score)</span><br></pre></td></tr></table></figure><pre><code>&lt;PIL.Image.Image image mode=RGB size=409x273 at 0x11130227FD0&gt;load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pthtext: a woman sitting on the beach with a dogThe image and text is matched with a probability of 0.9960The image feature and text feature has a cosine similarity of 0.5262</code></pre>]]></content>
    
    
    <summary type="html">Salesforce提供的统一视觉语言理解与生成的预训练模型</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="模型调用" scheme="https://www.cclmsy.cc/tags/%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>复现|手搓Transformer模型</title>
    <link href="https://www.cclmsy.cc/posts/Transformer.html"/>
    <id>https://www.cclmsy.cc/posts/Transformer.html</id>
    <published>2025-02-27T16:00:00.000Z</published>
    <updated>2025-02-27T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p><blockquote><p>2017年Google在论文《Attention is All You Need》中提出了Transformer模型，并成功应用到NLP领域。<br>该模型完全基于自注意力机制Attention mechanism实现，弥补了传统的RNN模型的不足。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局变量</span></span><br><span class="line">d_model = <span class="number">512</span> <span class="comment"># 词向量维度</span></span><br><span class="line">d_ff = <span class="number">2048</span> <span class="comment"># 前馈神经网络隐层维度</span></span><br><span class="line">src_vocab_size = <span class="number">10</span> <span class="comment"># 源语言词表大小</span></span><br><span class="line">tgt_vocab_size = <span class="number">10</span> <span class="comment"># 目标语言词表大小</span></span><br><span class="line">n_layers = <span class="number">6</span> <span class="comment"># 编码器和解码器堆叠基础块的数量</span></span><br></pre></td></tr></table></figure><h2 id="0-Transformer原理"><a href="#0-Transformer原理" class="headerlink" title="0.Transformer原理"></a>0.Transformer原理</h2><p>宏观上，Transformer可以看作一个黑箱操作的Seq2Seq模型。<br>拆开黑箱，可以看到模型的本质是一个Encoder-Decoder结构。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Transformer/Transformer整体架构.webp" alt="Transformer整体架构"></p><ul><li>Embedding：词嵌入层，将输入的词转换为词向量</li><li>Positional Encoding：位置编码，为了让模型学习到序列的位置信息</li><li>Multi-Head Attention：多头注意力机制，用于捕捉输入序列的全局依赖关系</li><li>Add &amp; Norm：残差连接和层归一化，用于加速训练、缓解梯度消失问题<ul><li>Add残差连接：把网络的输入和输出相加，有效解决梯度消失问题</li><li>Norm层归一化：对网络的输出进行归一化处理，加速训练</li></ul></li><li>Feed Forward：前馈神经网络，用于对特征进行非线性变换</li><li>Linear：线性变换层，用于将特征映射到输出空间</li><li>Softmax：Softmax层，用于输出概率分布</li></ul><h2 id="1-Encoder"><a href="#1-Encoder" class="headerlink" title="1.Encoder"></a>1.Encoder</h2><p><img src="https://source.cclmsy.cc/Posts/DL/Transformer/Transformer_Encoder.webp" alt="Transformer_Encoder"></p><h3 id="1-1-Embedding-词嵌入"><a href="#1-1-Embedding-词嵌入" class="headerlink" title="1.1 Embedding 词嵌入"></a>1.1 Embedding 词嵌入</h3><p>模型无法直接处理文本数据，需要把文本数据转换成计算机能够识别的向量形式。<br>将文本转化为向量通常有两种方式：</p><ul><li>One-hot编码：词典大小为N，每个词用一个N维向量表示，当前词对应的维度为1，其余维度为0<ul><li>优点：简单直观</li><li>缺点：向量维度高、稀疏、缺乏语义之间的联系</li></ul></li><li>Embedding词嵌入：通过训练学习到的词向量，将词映射到一个低维空间<ul><li>优点：低维、稠密、能够表达词之间的关系</li></ul></li></ul><p>输入Imput的维度是[batch_size, seq_len]，Embedding层的输出维度是[batch_size, seq_len, d_model]。</p><ul><li>batch_size：批次大小（句子个数）</li><li>seq_len：最长句子长度</li><li>d_model：词向量维度</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, d_model</span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment"># 调用nn.Embedding，获得实例化的词嵌入对象lut(look up table)</span></span><br><span class="line">        self.lut = nn.Embedding(vocab_size, d_model)</span><br><span class="line">        self.d_model = d_model <span class="comment"># 词嵌入维度</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Embedding层的前向传播</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: 输入的词索引张量，形状为(batch_size, seq_len)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            词嵌入张量，形状为(batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h3 id="1-2-Positional-Encoding-位置编码"><a href="#1-2-Positional-Encoding-位置编码" class="headerlink" title="1.2 Positional Encoding 位置编码"></a>1.2 Positional Encoding 位置编码</h3><p>Embedding层只能表示词的语义信息，无法表示词的位置信息。<br>Transformer使用的是自注意力机制来提取信息，一个句子中的每个字/词是并行计算，虽然处理每个字的时候考虑到了所有字对其的影响，但是并没有考虑到各个字相互之间的位置信息，也就是上下文，所以需要引入位置信息</p><p>为了让模型学习到序列的位置信息，需要在Embedding层的输出上加上位置编码，得到含有位置信息的词向量$\alpha$。</p><p>Transformer中使用Positional Encoding表示每个字、词的位置信息，公式如下：</p><script type="math/tex; mode=display">PE_{(pos, i)} = \begin{cases}sin(w_k \cdot pos), & i = 2k \\\\cos(w_k \cdot pos), & i = 2k+1\end{cases}</script><script type="math/tex; mode=display">w_k = \dfrac{1}{10000^{2k/d_{model}}} \quad k = 0,1,2,...,d_{model}-1</script><p>其中：</p><ul><li>$PE_{(pos, i)}$：表示第$pos$个字/词的Encoding向量第$i$维的编码值</li><li>$pos$：位置信息，表示句子中的第几个字/词，从0开始</li><li>$i$：位置编码的维度，从0开始</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="comment"># 初始化一个形状为(max_len, d_model)的位置编码矩阵</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># position[i] = i</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">        <span class="comment"># w_k=e^(2k*-log10000/d_model)=1/(10000^(2k/d_model))</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        <span class="comment"># 偶数列使用sin函数编码，奇数列使用cos函数编码</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="comment"># 在第0维增加一个维度，形状变为(batch_size=1, max_len, d_model)</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;位置编码层的前向传播</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: 输入的词嵌入张量，形状为(batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            添加了位置编码的词嵌入张量，形状为(batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = x + self.pe[:, :x.size(<span class="number">1</span>)].clone().detach()</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h3 id="1-3-Self-Attention-自注意力机制"><a href="#1-3-Self-Attention-自注意力机制" class="headerlink" title="1.3 Self Attention 自注意力机制"></a>1.3 Self Attention 自注意力机制</h3><p>一句话中，与语义紧密相关的关键词需要予以更多的关注，而无关的连接词和辅助词则可以忽略。<br>在机器翻译时，更多的注意表现为更大的权重，越重要的词权重越大。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Transformer/Self_Attention.webp" alt="Self Attention"></p><p>包含位置信息的词向量$\alpha^i$（句子的第i+1个词的词向量）作为Self Attention的输入，分别乘以三个权重矩阵$W^Q$、$W^K$、$W^V$，得到：</p><ul><li>$q^i$：Query，查询向量</li><li>$k^i$：Key，键向量，“被查”时的向量</li><li>$v^i$：Value，值向量，“内容”的向量</li></ul><p>以4个字的句子“我是学生”为例，计算第0个字“我”的Self Attention：</p><ol><li>计算 $q^0$ 和 $k^0,k^1,k^2,k^3$ 的点积，得到4个注意力值 $\alpha<em>{00},\alpha</em>{01},\alpha<em>{02},\alpha</em>{03}$</li></ol><ul><li>经过Softmax归一化，得到4个注意力分数 $\hat{\alpha<em>{00}},\hat{\alpha</em>{01}},\hat{\alpha<em>{02}},\hat{\alpha</em>{03}}$ ，它们的和为1</li><li>将这些分数作为权重，对 $v^0,v^1,v^2,v^3$ 进行加权求和，得到Self Attention的输出</li><li>$b^0=\hat{\alpha<em>{00}}v^0+\hat{\alpha</em>{01}}v^1+\hat{\alpha<em>{02}}v^2+\hat{\alpha</em>{03}}v^3$</li></ul><p>为了加速计算，可以使用矩阵运算：</p><p><img src="https://source.cclmsy.cc/Posts/DL/Transformer/Self_Attention_Matrix.png" alt="Self Attention矩阵运算"></p><ul><li>其中$\alpha_{ij}=\dfrac{q_i * k_j^T}{\sqrt{d_k}}$，$d_k$是词向量的维度</li><li>最后计算$b^i=\sum<em>{j=0}^{n-1}\hat{\alpha</em>{ij}}v_j$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, scale_factor, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.scale_factor = scale_factor <span class="comment"># 缩放因子</span></span><br><span class="line">        <span class="comment"># Dropout用于防止过拟合</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        batch_size: 批大小</span></span><br><span class="line"><span class="string">        num_heads: 多头注意力的头数，论文默认为8</span></span><br><span class="line"><span class="string">        seq_len: 序列长度</span></span><br><span class="line"><span class="string">        d_k, d_v: 键和值的维度，默认都是64</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            Q: 查询张量，形状为(batch_size, num_heads, seq_len, d_k)</span></span><br><span class="line"><span class="string">            K: 键张量，形状为(batch_size, num_heads, seq_len, d_k)</span></span><br><span class="line"><span class="string">            V: 值张量，形状为(batch_size, num_heads, seq_len, d_v)</span></span><br><span class="line"><span class="string">            mask: 掩码张量，形状为(batch_size, seq_len, seq_len)，默认为None</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            上下文张量和注意力张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        attn = torch.matmul(Q / self.scale_factor, K.transpose(<span class="number">2</span>, <span class="number">3</span>)) <span class="comment"># K的第2和第3维转置</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if mask is not None:</span></span><br><span class="line">        <span class="comment">#     scores = scores.masked_fill(mask == 0, -1e9)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Softmax计算注意力权重，Dropout减少过拟合</span></span><br><span class="line">        attn = self.dropout(torch.softmax(attn, dim=-<span class="number">1</span>))</span><br><span class="line">        output = torch.matmul(attn, V)</span><br><span class="line">        <span class="keyword">return</span> output, attn <span class="comment"># 返回上下文张量和注意力张量</span></span><br></pre></td></tr></table></figure><h3 id="1-4-Multi-Head-Attention-多头注意力机制"><a href="#1-4-Multi-Head-Attention-多头注意力机制" class="headerlink" title="1.4 Multi-Head Attention 多头注意力机制"></a>1.4 Multi-Head Attention 多头注意力机制</h3><p>多头注意力机制就是把$q^i, k^i, v^i$三个矩阵从特征维度（词向量长度）方向上拆分成为形状相同的小矩阵。<br>再将每个Head Attention的输出拼接起来，得到最终的Multi-Head Attention输出。</p><p>理解：多个头分别关注不同的特征子空间，最后再将这些子空间的信息融合起来。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Transformer/Multi_Head_Attention.webp" alt="Multi-Head Attention"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_head=<span class="number">8</span>, d_model=<span class="number">512</span>, d_k=<span class="number">64</span>, d_v=<span class="number">64</span>, droupout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment"># 论文中，参数分别为：8、512、64、64</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line"></span><br><span class="line">        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_head * d_v, d_model, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.attention = ScaledDotProductAttention(scale_factor=d_k ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(droupout)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>) <span class="comment"># LayerNorm层，用于归一化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head</span><br><span class="line">        batch_size, len_q, len_k, len_v = Q.size(<span class="number">0</span>), Q.size(<span class="number">1</span>), K.size(<span class="number">1</span>), V.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        residual = Q <span class="comment"># 保留输入用作残差连接</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将Q、K、V归一化后，分别通过线性映射到多头</span></span><br><span class="line">        <span class="comment"># Q: (batch_size, len_q, d_model) -&gt; (batch_size, len_q, n_head * d_k) -&gt; (batch_size, n_head, len_q, d_k)</span></span><br><span class="line">        <span class="comment"># Q：(batch_size, len_q, 512) -&gt; (batch_size, len_q, 8*64) -&gt; (batch_size, len_q, 8, 64)</span></span><br><span class="line">        Q = self.layer_norm(Q)</span><br><span class="line">        K = self.layer_norm(K)</span><br><span class="line">        V = self.layer_norm(V)</span><br><span class="line">        Q = self.w_qs(Q).view(batch_size, len_q, n_head, d_k)</span><br><span class="line">        K = self.w_ks(K).view(batch_size, len_k, n_head, d_k)</span><br><span class="line">        V = self.w_vs(V).view(batch_size, len_v, n_head, d_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 转置，使得第1和第2维交换位置，进行Attention计算</span></span><br><span class="line">        <span class="comment"># Q: (batch_size, len_q, n_head, d_k) -&gt; (batch_size, n_head, len_q, d_k)</span></span><br><span class="line">        <span class="comment"># Q：(batch_size, len_q, 8, 64) -&gt; (batch_size, 8, len_q, 64)</span></span><br><span class="line">        Q, K, V = Q.transpose(<span class="number">1</span>, <span class="number">2</span>), K.transpose(<span class="number">1</span>, <span class="number">2</span>), V.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>) <span class="comment"># 增加一个Head维度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Q=Softmax(Q*K/d + (1-S)σ)V，attn是QK/D</span></span><br><span class="line">        Q, attn = self.attention(Q, K, V, mask=mask) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q的形状：[batch_size, n_head, len_q, d_v] [2,8,5,64]</span></span><br><span class="line">        Q = Q.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, len_q, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        Q = self.dropout(self.fc(Q))</span><br><span class="line"></span><br><span class="line">        Q += residual <span class="comment"># 残差连接Add</span></span><br><span class="line">        Q = self.layer_norm(Q) <span class="comment"># LayerNorm</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Q, attn</span><br></pre></td></tr></table></figure><h3 id="1-5-Add-amp-Norm-残差连接和层归一化"><a href="#1-5-Add-amp-Norm-残差连接和层归一化" class="headerlink" title="1.5 Add &amp; Norm 残差连接和层归一化"></a>1.5 Add &amp; Norm 残差连接和层归一化</h3><p>Add残差链接就是将网络的输入和输出直接相加，主要是为了解决梯度消失问题。</p><p>Layer Normalization是对网络的输出进行归一化处理，加速训练。<br>使$b$的每一行，也就是每个句子，归一化为标准正态分布，输出为$\hat b$，归一化公式如下：</p><ul><li>均值：$\mu<em>i=\dfrac{1}{d}\sum</em>{j=1}^{d}b_{ij}$</li><li>方差：$\sigma<em>i^2=\dfrac{1}{d}\sum</em>{j=1}^{d}(b_{ij}-\mu_i)^2$</li><li>归一化：$\hat b<em>{ij}=\dfrac{b</em>{ij}-\mu_i}{\sqrt{\sigma_i^2+\epsilon}}*\gamma+\beta$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, eps=<span class="number">1e-12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(d_model))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(d_model))</span><br><span class="line">        self.eps = eps <span class="comment"># 防止分母为0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        var = x.var(-<span class="number">1</span>, unbiased=<span class="literal">False</span>, keepdim=<span class="literal">True</span>) <span class="comment"># unbiased=False表示方差计算非无偏估计（除以N而不是N-1）</span></span><br><span class="line"></span><br><span class="line">        out = (x - mean) / torch.sqrt(var + self.eps)</span><br><span class="line">        out = self.gamma * out + self.beta</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="1-6-Feed-Forward-前馈神经网络"><a href="#1-6-Feed-Forward-前馈神经网络" class="headerlink" title="1.6 Feed Forward 前馈神经网络"></a>1.6 Feed Forward 前馈神经网络</h3><p>Add &amp; Norm层后接一个全连接的前馈神经网络，用于对特征进行非线性变换</p><p>前馈神经网络的结构是两个全连接层，第一个全连接层的激活函数是ReLU，第二个全连接层没有激活函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_ff, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model, bias=<span class="literal">False</span>))</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>) <span class="comment"># LayerNorm层，用于归一化</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        resdual = inputs</span><br><span class="line">        output = self.fc(inputs)</span><br><span class="line">        output = self.layer_norm(output + resdual)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h3 id="1-7-Mask掉-停用词"><a href="#1-7-Mask掉-停用词" class="headerlink" title="1.7 Mask掉 停用词"></a>1.7 Mask掉 停用词</h3><p>句子中没有意义的占位符，例如“我是学生P”中的P是停止符，没有实际意义，需要将其mask掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    <span class="comment"># eq(zero)用于判断 seq_k 中哪些位置是填充符（通常填充符的值是 0），返回一个ByteTensor</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># (N, 1, len_k)</span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)  <span class="comment"># (N, len_q, len_k)</span></span><br></pre></td></tr></table></figure><h3 id="1-8-EncoderLayer"><a href="#1-8-EncoderLayer" class="headerlink" title="1.8 EncoderLayer"></a>1.8 EncoderLayer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention() <span class="comment"># 多头自注意力</span></span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward() <span class="comment"># 前馈神经网络</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向传播</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            enc_inputs: 编码器输入张量，形状为(batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            enc_self_attn_mask: 编码器自注意力掩码，形状为(batch_size, seq_len, seq_len)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            编码器输出张量，形状为(batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)</span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn</span><br></pre></td></tr></table></figure><h3 id="1-9-Encoder"><a href="#1-9-Encoder" class="headerlink" title="1.9 Encoder"></a>1.9 Encoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.src_emb = Embeddings(src_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="comment"># 1. 中文字索引进行Embedding，转换为512维的词向量</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs)</span><br><span class="line">        <span class="comment"># 2. 加上位置编码</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs)</span><br><span class="line">        <span class="comment"># 3. mask掉padding部分</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)</span><br><span class="line">        enc_self_attns = []</span><br><span class="line">        <span class="comment"># 4. 通过6层EncoderLayer，上一层的输出作为下一层的输入</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attns.append(enc_self_attn)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure><h2 id="2-Decoder"><a href="#2-Decoder" class="headerlink" title="2.Decoder"></a>2.Decoder</h2><p>Masked Multi-Head Attention：与MultiHead Attention类似，只是在计算Self Attention时，需要mask掉未来的信息</p><p>Multi-Head Attention：与Encoder中的MultiHead Attention相同</p><p>Decoder的输出预测：Decoder输出矩阵形状是[seq_len, word_dim]，经过nn.Linear全连接层，再通过softmax函数得到每个词的概率，然后选择概率最大的词作为预测结果。</p><h3 id="2-1-Decoder-Input-输入处理"><a href="#2-1-Decoder-Input-输入处理" class="headerlink" title="2.1 Decoder Input 输入处理"></a>2.1 Decoder Input 输入处理</h3><p>Decoder的输入是最后一个Encoder的输出，在训练时，同时输入目标句子的词向量，以便计算Loss。</p><p>“我是学生E”-&gt;“S I am a student”</p><ul><li>T0时刻：输入开始符“S”，输出预测的第一个词“I”</li><li>T1时刻：输入“S I”，输出预测的第二个词“am”</li><li>…</li></ul><p>输入使用上三角矩阵进行mask，避免Decoder看到未来的信息。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Transformer/Input_Mask.webp" alt="Input Mask"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequent_mask</span>(<span class="params">seq</span>): <span class="comment"># seq: [batch_size, tgt_len]</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>) <span class="comment"># 返回上三角矩阵</span></span><br><span class="line">    subsequent_mask = torch.from_numpy(subsequent_mask).byte() <span class="comment"># 转换为ByteTensor</span></span><br><span class="line">    <span class="keyword">return</span> subsequent_mask</span><br></pre></td></tr></table></figure><h3 id="2-2-DecoderLayer"><a href="#2-2-DecoderLayer" class="headerlink" title="2.2 DecoderLayer"></a>2.2 DecoderLayer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention() </span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dec_inputs: 解码器输入张量，形状为(batch_size, tgt_len, d_model)</span></span><br><span class="line"><span class="string">            enc_outputs: 编码器输出张量，形状为(batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            dec_self_attn_mask: 解码器自注意力掩码，形状为(batch_size, tgt_len, tgt_len)</span></span><br><span class="line"><span class="string">            dec_enc_attn_mask: 解码器-编码器注意力掩码，形状为(batch_size, tgt_len, seq_len)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            解码器输出张量，形状为(batch_size, tgt_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)</span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn</span><br></pre></td></tr></table></figure><h3 id="2-3-Decoder"><a href="#2-3-Decoder" class="headerlink" title="2.3 Decoder"></a>2.3 Decoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.tgt_emb = Embeddings(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dec_inputs: 解码器输入张量，形状为(batch_size, tgt_len)</span></span><br><span class="line"><span class="string">            enc_inputs: 编码器输入张量，形状为(batch_size, seq_len)</span></span><br><span class="line"><span class="string">            enc_outputs: 编码器输出张量，形状为(batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            解码器输出张量，形状为(batch_size, tgt_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 英文字索引进行Embedding，转换为512维的词向量，加上位置编码</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs)</span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs)</span><br><span class="line">        <span class="comment"># mask掉padding部分</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)</span><br><span class="line">        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)</span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), <span class="number">0</span>)</span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)</span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="comment"># 通过6层DecoderLayer，上一层的输出作为下一层的输入</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><h2 id="3-Transformer代码实现"><a href="#3-Transformer代码实现" class="headerlink" title="3. Transformer代码实现"></a>3. Transformer代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder()</span><br><span class="line">        self.decoder = Decoder()</span><br><span class="line">        <span class="comment"># 解码器最后的分类器，分类器的输入d_model是解码层每个token的输出维度大小</span></span><br><span class="line">        <span class="comment"># 需要将其转为词表大小，再计算softmax；计算哪个词出现的概率最大</span></span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向传播</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            enc_inputs: 编码器输入张量，形状为(batch_size, seq_len)</span></span><br><span class="line"><span class="string">            dec_inputs: 解码器输入张量，形状为(batch_size, tgt_len)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            解码器输出张量，形状为(batch_size * tgt_len, tgt_vocab_size)</span></span><br><span class="line"><span class="string">            enc_self_attns: 编码器自注意力张量列表</span></span><br><span class="line"><span class="string">            dec_self_attns: 解码器自注意力张量列表</span></span><br><span class="line"><span class="string">            dec_enc_attns: 解码器-编码器注意力张量列表</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line"></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]</span></span><br><span class="line">        dec_logits = self.projection(dec_outputs) </span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="4-Transformer模型训练"><a href="#4-Transformer模型训练" class="headerlink" title="4. Transformer模型训练"></a>4. Transformer模型训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">model = Transformer()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 词典</span></span><br><span class="line">word2idx = &#123;</span><br><span class="line">    <span class="string">&quot;S&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;我&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;是&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;学&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;生&quot;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&quot;I&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;am&quot;</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">&quot;a&quot;</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">&quot;student&quot;</span>: <span class="number">8</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">idx2word = &#123;i: w <span class="keyword">for</span> w, i <span class="keyword">in</span> word2idx.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入数据</span></span><br><span class="line">enc_inputs = torch.LongTensor([[word2idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> [<span class="string">&quot;我&quot;</span>, <span class="string">&quot;是&quot;</span>, <span class="string">&quot;学&quot;</span>, <span class="string">&quot;生&quot;</span>, <span class="string">&quot;S&quot;</span>]]])</span><br><span class="line">dec_inputs = torch.LongTensor([[word2idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> [<span class="string">&quot;S&quot;</span>, <span class="string">&quot;I&quot;</span>, <span class="string">&quot;am&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;student&quot;</span>]]])</span><br><span class="line">target_batch = torch.LongTensor([[word2idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> [<span class="string">&quot;S&quot;</span>, <span class="string">&quot;I&quot;</span>, <span class="string">&quot;am&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;student&quot;</span>]]])</span><br><span class="line"><span class="built_in">print</span>(enc_inputs, dec_inputs, target_batch)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)</span><br><span class="line">    loss = criterion(outputs, target_batch.view(-<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">predict, _, _, _ = model(enc_inputs, dec_inputs)</span><br><span class="line">predict = predict.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(enc_inputs, <span class="string">&#x27;-&gt;&#x27;</span>, [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict.squeeze()])</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[1, 2, 3, 4, 0]]) tensor([[0, 5, 6, 7, 8]]) tensor([[0, 5, 6, 7, 8]])Epoch: 0001 cost = 2.244576Epoch: 0002 cost = 0.970345Epoch: 0003 cost = 2.741649Epoch: 0004 cost = 3.185768Epoch: 0005 cost = 4.521887Epoch: 0006 cost = 3.544580Epoch: 0007 cost = 3.160498Epoch: 0008 cost = 2.823286Epoch: 0009 cost = 0.625107Epoch: 0010 cost = 0.672708Epoch: 0011 cost = 0.561514Epoch: 0012 cost = 0.959138Epoch: 0013 cost = 0.673696Epoch: 0014 cost = 0.326180Epoch: 0015 cost = 0.268545Epoch: 0016 cost = 0.215698Epoch: 0017 cost = 0.168979Epoch: 0018 cost = 0.057510Epoch: 0019 cost = 0.087601Epoch: 0020 cost = 0.056923tensor([[1, 2, 3, 4, 0]]) -&gt; [&#39;S&#39;, &#39;I&#39;, &#39;am&#39;, &#39;a&#39;, &#39;student&#39;]</code></pre>]]></content>
    
    
    <summary type="html">实现与测试Transformer模型</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="论文复现" scheme="https://www.cclmsy.cc/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    
    <category term="Pytorch" scheme="https://www.cclmsy.cc/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>深度学习笔记4-循环神经网络RNN</title>
    <link href="https://www.cclmsy.cc/posts/deep_learning04.html"/>
    <id>https://www.cclmsy.cc/posts/deep_learning04.html</id>
    <published>2025-02-26T16:00:00.000Z</published>
    <updated>2025-02-26T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="4-1-循环神经网络"><a href="#4-1-循环神经网络" class="headerlink" title="4.1 循环神经网络"></a>4.1 循环神经网络</h2><h3 id="4-1-1-序列模型"><a href="#4-1-1-序列模型" class="headerlink" title="4.1.1 序列模型"></a>4.1.1 序列模型</h3><p>序列模型：自然语言、音频、视频等序列数据的模型</p><ul><li>应用：语音识别、情感分类、机器翻译等</li></ul><p>为什么不使用CNN？</p><ul><li>序列数据的前后之间具有强关联性</li><li>输入输出长度不固定</li></ul><h3 id="4-1-2-循环神经网络"><a href="#4-1-2-循环神经网络" class="headerlink" title="4.1.2 循环神经网络"></a>4.1.2 循环神经网络</h3><p>循环（递归）神经网络（Recurrent Neural Network，RNN）是神经网络的一种，将“状态”在自身网络中循环传递，可以接受时间序列结构输入</p><h4 id="4-1-2-1-RNN类型"><a href="#4-1-2-1-RNN类型" class="headerlink" title="4.1.2.1 RNN类型"></a>4.1.2.1 RNN类型</h4><p><img src="https://source.cclmsy.cc/Posts/DL/Note/RNN结构.png" alt="RNN结构"></p><ul><li>一对一：固定输入到固定输出，如图像分类</li><li>一对多：固定输入到序列输出，如图像的文字描述</li><li>多对一：序列输入到固定输出，如情感分类</li><li>（异步）多对多：序列输入到序列输出，如机器翻译，称为Encoder-Decoder（编码-解码）结构</li><li>同步多对多：同步序列到同步输出，如文本生成、视频帧分类</li></ul><h4 id="4-1-2-2-基础循环神经网络"><a href="#4-1-2-2-基础循环神经网络" class="headerlink" title="4.1.2.2 基础循环神经网络"></a>4.1.2.2 基础循环神经网络</h4><p><img src="https://source.cclmsy.cc/Posts/DL/Note/基础RNN结构.png" alt="基础RNN结构"></p><ul><li>$x_t$：t时刻的输入</li><li>$o_t$：t时刻的输出</li><li>$s_t$：t时刻的隐层输出</li><li>所有单元的参数$U, V, W$共享</li></ul><p>统一公式（$f$采用TanH/RuLU，$g$采用Softmax/Sigmoid）：</p><script type="math/tex; mode=display">\begin{split}    & s_0 = 0 \\\\    & s_t = f(Ux_t + Ws_{t-1}) \\\\    & o_t = g(Vs_t)\end{split}</script><p>输出的$o<em>t$受前面时刻的隐层$s</em>{t-1}$影响，即RNN具有记忆功能</p><h4 id="4-1-2-3-序列生成案例"><a href="#4-1-2-3-序列生成案例" class="headerlink" title="4.1.2.3 序列生成案例"></a>4.1.2.3 序列生成案例</h4><p><img src="https://source.cclmsy.cc/Posts/DL/Note/序列例子.png" alt="序列例子"></p><ul><li>输入到网络当中的是一个个的分词结果，每一个词的输入是一个时刻</li><li>每一个时刻有一个输出，表示最可能的下一个词</li></ul><h4 id="4-1-2-4-词的表示"><a href="#4-1-2-4-词的表示" class="headerlink" title="4.1.2.4 词的表示"></a>4.1.2.4 词的表示</h4><p>为了能够让网络理解输入，需要将词进行向量表示。</p><ul><li>建立一个包含所有序列词的词典，每个词在词典中有唯一编号</li><li>记词典大小为$N$，任意一个词都可以用一个$N$维的One-Hot向量表示</li><li>得到一个高维稀疏矩阵</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/词向量表示.png" alt="词向量表示"></p><h4 id="4-1-2-4-输出的表示-Softmax"><a href="#4-1-2-4-输出的表示-Softmax" class="headerlink" title="4.1.2.4 输出的表示-Softmax"></a>4.1.2.4 输出的表示-Softmax</h4><p>RNN这种模型，每一个时刻的输出是下一个最可能的词，可以用概率表示，总长度为词的总数长度</p><ul><li>每个时刻的隐层输出$s_t$经过Softmax函数，得到概率分布</li></ul><h4 id="4-1-2-5-交叉熵损失"><a href="#4-1-2-5-交叉熵损失" class="headerlink" title="4.1.2.5 交叉熵损失"></a>4.1.2.5 交叉熵损失</h4><p>总损失定义：一整个序列（一个句子）作为一个训练实例，总误差是各个时刻误差的和</p><script type="math/tex; mode=display">\begin{split}    & E_t(y_t, \hat{y}_t) = -y_t \log(\hat{y}_t) \\\\    & E(y, \hat{y}) = \sum_t E_t(y_t, \hat{y}_t) = -\sum_t y_t \log(\hat{y}_t)\end{split}</script><ul><li>$y_t$：时刻t上正确的输出</li><li>$\hat{y}_t$：时刻t上预测的输出</li></ul><h4 id="4-1-2-6-时序反向传播算法（BPTT）"><a href="#4-1-2-6-时序反向传播算法（BPTT）" class="headerlink" title="4.1.2.6 时序反向传播算法（BPTT）"></a>4.1.2.6 时序反向传播算法（BPTT）</h4><p>Backpropagation Through Time，时序反向传播算法：对于RNN有一个时间概念，需要把梯度沿时间通道进行反向传播</p><p>需要更新的参数：$U, V, W, b_x, b_y$</p><ul><li>计算每个时间的梯度$dW_t$，相加作为每次$W$更新的梯度值</li><li>$s<em>t=tanh(Ux_t+Ws</em>{t-1}+b_x)$，$o_t=Softmax(Vs_t+b_y)$</li><li>利用链式法则，计算出每个时间下各参数的梯度</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/RNN反向传播总结.png" alt="RNN反向传播总结"></p><h4 id="4-1-2-7-梯度消失与梯度爆炸"><a href="#4-1-2-7-梯度消失与梯度爆炸" class="headerlink" title="4.1.2.7 梯度消失与梯度爆炸"></a>4.1.2.7 梯度消失与梯度爆炸</h4><p>由于RNN中也存在链式求导法则，因此也会发生梯度消失与梯度爆炸的问题</p><h3 id="4-1-8-RNN改进"><a href="#4-1-8-RNN改进" class="headerlink" title="4.1.8 RNN改进"></a>4.1.8 RNN改进</h3><p>通过门控机制控制信息的流动</p><h4 id="4-1-8-1-门控循环单元GRU"><a href="#4-1-8-1-门控循环单元GRU" class="headerlink" title="4.1.8.1 门控循环单元GRU"></a>4.1.8.1 门控循环单元GRU</h4><p>Gated Recurrent Unit，门控循环单元</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/GRU单元.png" alt="GRU单元"></p><p>GRU增加了两个门，一个重置门（reset gate）和一个更新门（update gate）</p><ul><li>重置门：决定如何将新的输入和前一时刻的输出相结合</li><li>更新门：定义前面的记忆保存到当前时间的量</li><li>重置门1，更新门0，即为标准RNN模型</li></ul><p>GRU本质解决的问题：</p><ul><li>解决短期问题，每个递归单元能够自适应捕捉不同尺度的依赖关系</li><li>处理了隐层输出，$h<em>t=(1-z_t)*h</em>{t-1}+z_t*\tilde{h}_t$，解决了梯度消失问题</li></ul><h4 id="4-1-8-2-长短时记忆网络LSTM"><a href="#4-1-8-2-长短时记忆网络LSTM" class="headerlink" title="4.1.8.2 长短时记忆网络LSTM"></a>4.1.8.2 长短时记忆网络LSTM</h4><p>Long Short Term Memory，长短记忆网络</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/LSTM.png" alt="LSTM"></p><ul><li>$h_t$：当前cell的输出</li><li>$c_t$：隐层的记忆</li><li>三个门：遗忘门f，输入门u，输出门o</li></ul><p>作用：便于记忆长更长距离的状态</p><h2 id="4-2-词嵌入与NLP"><a href="#4-2-词嵌入与NLP" class="headerlink" title="4.2 词嵌入与NLP"></a>4.2 词嵌入与NLP</h2><h3 id="4-2-1-在RNN中使用one-hot表示的问题"><a href="#4-2-1-在RNN中使用one-hot表示的问题" class="headerlink" title="4.2.1 在RNN中使用one-hot表示的问题"></a>4.2.1 在RNN中使用one-hot表示的问题</h3><ul><li>假设有n个词，每个词的one-hot表示是n维的，整体大小为$n*n$，非常稀疏</li><li>无法表示词之间的相似性，例如Apple对Banana的相似性远高于Monkey</li></ul><h3 id="4-2-2-词嵌入（Word-Embedding）"><a href="#4-2-2-词嵌入（Word-Embedding）" class="headerlink" title="4.2.2 词嵌入（Word Embedding）"></a>4.2.2 词嵌入（Word Embedding）</h3><p>把一个维数为$N$的高维空间嵌入到一个维数低的多的连续向量空间中，每个单词或词组被映射为实数域上的向量，例如：</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Man</th><th style="text-align:center">Woman</th><th style="text-align:center">King</th><th style="text-align:center">Queen</th><th style="text-align:center">Apple</th><th style="text-align:center">Banana</th></tr></thead><tbody><tr><td style="text-align:center">Gender</td><td style="text-align:center">1</td><td style="text-align:center">-1</td><td style="text-align:center">-0.95</td><td style="text-align:center">0.97</td><td style="text-align:center">0.01</td><td style="text-align:center">-0.02</td></tr><tr><td style="text-align:center">Royal</td><td style="text-align:center">0.01</td><td style="text-align:center">0.02</td><td style="text-align:center">0.98</td><td style="text-align:center">0.99</td><td style="text-align:center">-0.03</td><td style="text-align:center">0.04</td></tr><tr><td style="text-align:center">Age</td><td style="text-align:center">0.01</td><td style="text-align:center">0.02</td><td style="text-align:center">0.75</td><td style="text-align:center">0.69</td><td style="text-align:center">0.03</td><td style="text-align:center">-0.04</td></tr><tr><td style="text-align:center">Food</td><td style="text-align:center">0.01</td><td style="text-align:center">0.02</td><td style="text-align:center">-0.03</td><td style="text-align:center">0.04</td><td style="text-align:center">0.95</td><td style="text-align:center">0.97</td></tr></tbody></table></div><p>词嵌入的特点：能够体现词与词之间的关系</p><p>Man-Woman≈King-?(Queen!) </p><p>算法/工具：Skip-gram、CBOW、GenSim</p><h2 id="4-3-Seq2Seq与Attention机制"><a href="#4-3-Seq2Seq与Attention机制" class="headerlink" title="4.3 Seq2Seq与Attention机制"></a>4.3 Seq2Seq与Attention机制</h2><h3 id="4-3-1-Seq2Seq"><a href="#4-3-1-Seq2Seq" class="headerlink" title="4.3.1 Seq2Seq"></a>4.3.1 Seq2Seq</h3><p>Seq2Seq：Sequence to Sequence，由Google Brain团队和Yoshua Bengio 两个团队各自独立的提出来</p><h4 id="4-3-1-1-定义"><a href="#4-3-1-1-定义" class="headerlink" title="4.3.1.1 定义"></a>4.3.1.1 定义</h4><p>Seq2Seq模型是一个Encoder-Decoder结构的模型，输入是一个序列，输出也是一个序列</p><p>Encoder中将一个可变长度的信号序列变为固定长度的向量表达，<br>Decoder中将这个固定长度的向量表达变为可变长度的目标信号序列</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/seq2seq.png" alt="seq2seq"></p><ul><li>相当于把RNN模型的$s_0$输入变成一个Encoder</li></ul><h4 id="4-3-1-2-条件语言模型"><a href="#4-3-1-2-条件语言模型" class="headerlink" title="4.3.1.2 条件语言模型"></a>4.3.1.2 条件语言模型</h4><p>Encoder编码器的作用：</p><ul><li>将一个边长输入序列输出到一个编码状态$C$</li><li>解码器输出$y<em>t$的条件概率基于之前的输出序列$y_1y_2…y</em>{t-1}$和编码状态$C$</li><li>$argmaxP(y_1,y_2,…,y_T|x_1,x_2,…,x_T)$，给定输入的序列，最大化输出序列的概率</li></ul><p>根据最大似然估计，最大化输出序列的概率</p><script type="math/tex; mode=display">\begin{split}    & P(y_1,y_2,...,y_T|x_1,x_2,...,x_T) \\\\    & = \prod_{t=1}^T P(y_t|y_1,y_2,...,y_{t-1},x_1,x_2,...,x_T) \\\\    & = \prod_{t=1}^T P(y_t|y_1,y_2,...,y_{t-1},C)\end{split}</script><p>这个公式需要求出$P(y<em>1|C),P(y_2|y_1,C),…,P(y_T|y_1,y_2,…,y</em>{T-1},C)$，概率连乘极小，不利于存储，因此取对数进行计算，这样就将连乘式转换为累加式</p><script type="math/tex; mode=display">\log P(y_1,y_2,...,y_T|x_1,x_2,...,x_T) = \sum_{t=1}^T \log P(y_t|y_1,y_2,...,y_{t-1},C)</script><p>应用场景：机器翻译（NMT）</p><h3 id="4-3-2-Attention机制"><a href="#4-3-2-Attention机制" class="headerlink" title="4.3.2 Attention机制"></a>4.3.2 Attention机制</h3><h4 id="4-3-2-1-长句子问题"><a href="#4-3-2-1-长句子问题" class="headerlink" title="4.3.2.1 长句子问题"></a>4.3.2.1 长句子问题</h4><p>对于长句子，Seq2Seq模型的性能会下降，无法做到准确翻译。<br>句子非常长时，BLEU（Bilingual Evaluation Understudy）评价得分会很低</p><p>本质原因：在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征$C$再解码，$C$中必须包含原始序列中的所有信息，它的长度就成了模型性能的瓶颈。<br>当需要翻译的句子很长时，一个$C$可能存不下那么多信息，就会造成翻译精度的下降。</p><h4 id="4-3-2-2-Attention机制"><a href="#4-3-2-2-Attention机制" class="headerlink" title="4.3.2.2 Attention机制"></a>4.3.2.2 Attention机制</h4><ul><li>把Encoder的所有隐层输出$s_1,s_2,…,s_T$都保留下来，不再只保留最后一个隐层输出$C$</li><li>将这些信息提供给Decoder</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/Attention机制.png" alt="Attention机制"></p><h4 id="4-3-2-3-Attention机制的计算"><a href="#4-3-2-3-Attention机制的计算" class="headerlink" title="4.3.2.3 Attention机制的计算"></a>4.3.2.3 Attention机制的计算</h4><p>假设Encoder的时刻为$t$，Decoder的时刻为$t’$</p><ol><li>$c<em>{t’}=\sum</em>{t=1}^T \alpha_{t’t}s_t$<ul><li>$\alpha_{t’t}$：参数，训练得到，表示Decoder的$t’$时刻对Encoder的$t$时刻的注意力权重</li><li>理解：Encoder的每个时刻加权求和，得到Decoder的$t’$时刻的输入</li><li>$c<em>4=\alpha</em>{41}s<em>1+\alpha</em>{42}s<em>2+…+\alpha</em>{4T}s_T$</li></ul></li><li>$\alpha_{t’t}$的$N$个权重系数的由来<ul><li>权重系数通过Softmax函数得到，$\alpha<em>{t’t}=\frac{exp(e</em>{t’t})}{\sum<em>{k=1}^T exp(e</em>{t’k})}$</li><li>$e<em>{t’t}=g(s</em>{t’-1},h_t)=v^T \tanh(W_ss+W_hh)$<ul><li>$e_{t’t}$：由t时刻的编码器隐层状态输出和t’-1时刻的解码器隐层状态输出计算得到的一个值</li><li>s为Decoder的隐层输出，h为Encoder的隐层输出</li><li>$W_s, W_h, v$：参数，训练得到</li></ul></li></ul></li></ol>]]></content>
    
    
    <summary type="html">把输出和新的输入混合来处理序列数据的网络</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch|Dataset&amp;DataLoader</title>
    <link href="https://www.cclmsy.cc/posts/Dataset&amp;DataLoader.html"/>
    <id>https://www.cclmsy.cc/posts/Dataset&amp;DataLoader.html</id>
    <published>2025-02-21T16:00:00.000Z</published>
    <updated>2025-02-21T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Dataset-数据集"><a href="#Dataset-数据集" class="headerlink" title="Dataset 数据集"></a>Dataset 数据集</h2><p>Pytorch中表示数据集的抽象类//</p><p>任何自定义的数据集都需要继承这个类并覆写相关方法</p><p>Dataset的描述：</p><blockquote><p>所有表示从键到数据样本的映射的数据集都应继承它。<br>所有子类都应覆写<strong>getitem</strong>，以支持获取给定键的数据样本。<br>子类还可以选择性地覆盖<strong>len</strong>，许多~torch.utils.data.Sampler类实现和~torch.utils.data.DataLoader类的默认选项都希望它返回数据集的大小。<br>子类也可以选择实现<strong>getitems</strong>，以加快成批样本的加载速度。<br>此方法接受批次样本的索引列表，并返回样本列表。</p></blockquote><p>假设需要加载的是当前目录下<code>images</code>文件夹中的42张png图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path, processor=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Dataset类的初始化方法</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            path: 数据路径列表</span></span><br><span class="line"><span class="string">            processor: 数据预处理的函数，f:数据路径-&gt;目标数据</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.path = path</span><br><span class="line">        self.processor = processor</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回数据集的大小&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据索引返回处理后的数据&quot;&quot;&quot;</span></span><br><span class="line">        data_path = self.path[idx]</span><br><span class="line">        data = self.processor(data_path)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">processor</span>(<span class="params">data_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预处理函数，这里&quot;&quot;&quot;</span></span><br><span class="line">    img = Image.<span class="built_in">open</span>(data_path).size</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_images_path</span>(<span class="params">data_dir</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取data_dir目录下所有png图片的路径，不含子目录&quot;&quot;&quot;</span></span><br><span class="line">    images_path = [os.path.join(data_dir,image)</span><br><span class="line">                   <span class="keyword">for</span> image <span class="keyword">in</span> os.listdir(data_dir)</span><br><span class="line">                   <span class="keyword">if</span> image.endswith(<span class="string">&#x27;.png&#x27;</span>)]</span><br><span class="line">    <span class="keyword">return</span> images_path</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">&#x27;images&#x27;</span> <span class="comment"># 数据目录</span></span><br><span class="line">images_path = get_images_path(data_dir) <span class="comment"># 获取数据路径</span></span><br><span class="line">dataset = MyDataset(images_path, processor=processor) <span class="comment"># 创建数据集</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="number">0</span>]) <span class="comment"># 打印第一个数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(dataset)) <span class="comment"># 打印数据集大小</span></span><br></pre></td></tr></table></figure><pre><code>(367, 126)42</code></pre><h2 id="DataLoader-数据迭代器"><a href="#DataLoader-数据迭代器" class="headerlink" title="DataLoader 数据迭代器"></a>DataLoader 数据迭代器</h2><p>Pytorch加载和处理数据集的可迭代对象//</p><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">Default</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">dataset</td><td style="text-align:center">必选</td><td style="text-align:center">用于加载数据的数据集<br>必须是torch.utils.data.Dataset的子类实例</td></tr><tr><td style="text-align:center">batch_size</td><td style="text-align:center">1</td><td style="text-align:center">每个batch的样本数</td></tr><tr><td style="text-align:center">shuffle</td><td style="text-align:center">False</td><td style="text-align:center">是否在每个epoch开始时打乱数据</td></tr><tr><td style="text-align:center">sampler</td><td style="text-align:center">None</td><td style="text-align:center">定义从数据集中提取样本的策略<br>如果指定这个参数，则忽略shuffle参数</td></tr><tr><td style="text-align:center">batch_sampler</td><td style="text-align:center">None</td><td style="text-align:center">与sampler类似，但返回的是一个batch的索引<br>不能与batch_size、shuffle、sampler同时使用</td></tr><tr><td style="text-align:center">num_workers</td><td style="text-align:center">0</td><td style="text-align:center">用于数据加载的子进程数</td></tr><tr><td style="text-align:center">collate_fn</td><td style="text-align:center">None</td><td style="text-align:center">将多个样本组合成一个mini-batch的函数</td></tr><tr><td style="text-align:center">drop_last</td><td style="text-align:center">False</td><td style="text-align:center">如果数据集大小不能被batch_size整除，是否丢弃最后一个不完整的batch</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">data_loader = DataLoader(dataset,batch_size=<span class="number">5</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> data_loader:</span><br><span class="line">        <span class="built_in">print</span>(data)</span><br></pre></td></tr></table></figure><pre><code>[tensor([1866,  439,  970, 1378, 1416]), tensor([660, 159, 354, 554, 498])][tensor([2018, 1116, 1720,  812, 1500]), tensor([ 968,  556, 1302,  465, 1168])][tensor([2260, 1378, 1046, 1774, 1521]), tensor([648, 612, 514, 524, 375])][tensor([ 956,  962,  412, 1700,  978]), tensor([530, 542, 640, 904, 465])][tensor([1202, 1434, 1686, 1583, 3873]), tensor([1006,  598,  540,  844, 5041])][tensor([1402, 1824, 1580, 1422, 1638]), tensor([1414, 1454,  630,  570, 1044])][tensor([1944, 1390, 1408,  367, 1320]), tensor([592, 568, 864, 126, 364])][tensor([1983, 1844,  912, 1400, 1564]), tensor([482, 866, 501, 602, 514])][tensor([1144, 1594]), tensor([718, 494])]</code></pre>]]></content>
    
    
    <summary type="html">Pytorch中的数据处理类</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Pytorch" scheme="https://www.cclmsy.cc/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>深度学习笔记3-卷积神经网络CNN</title>
    <link href="https://www.cclmsy.cc/posts/deep_learning03.html"/>
    <id>https://www.cclmsy.cc/posts/deep_learning03.html</id>
    <published>2025-02-21T16:00:00.000Z</published>
    <updated>2025-02-21T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-1-图像数据与边缘检测"><a href="#3-1-图像数据与边缘检测" class="headerlink" title="3.1 图像数据与边缘检测"></a>3.1 图像数据与边缘检测</h2><h3 id="3-1-1-图像数据"><a href="#3-1-1-图像数据" class="headerlink" title="3.1.1 图像数据"></a>3.1.1 图像数据</h3><p>在CV领域，通常要做的是用机器程序代替人眼对目标图像进行识别、分析、处理。<br>深度学习在CV领域的应用非常广泛。</p><p>假设需要处理1024x1024的彩色图像，每个像素有RGB三个通道，共有1024x1024x3=3,145,728个特征。<br>假设第一个隐藏层有10个神经元，那么第一层的权重矩阵有3,145,728x10=31,457,280个参数，计算量极大，难以达到好的效果。</p><p>相比多层神经网络，卷积神经网络（CNN）更适合处理图像数据。</p><h3 id="3-1-2-感受野（Receptive-Field）"><a href="#3-1-2-感受野（Receptive-Field）" class="headerlink" title="3.1.2 感受野（Receptive Field）"></a>3.1.2 感受野（Receptive Field）</h3><p>1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了感受野的概念。<br>Fukushima基于感受野概念提出的神经认知机(Neocognitron)可以看作是卷积神经网络的第一个实现网络。</p><p>单个感受器与许多感觉神经纤维相联系，感觉信息是通过许多感受神经纤维发放总和性的空间与时间类型不同的冲动，相当于经过编码来传递。</p><h3 id="3-1-3-边缘运算"><a href="#3-1-3-边缘运算" class="headerlink" title="3.1.3 边缘运算"></a>3.1.3 边缘运算</h3><p>为了使用更少的参数检测出更多的信息，通常神经网络需要检测出物体最明显的垂直和水平边缘来区分物体。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/边缘检测.png" alt="边缘检测"></p><p>随着深度学习的发展，我们需要检测更复杂的图像中的边缘。<br>与其使用由人手工设计的过滤器，还可以将过滤器中的数值作为参数，通过反向传播来学习得到。<br>算法可以根据实际数据来选择合适的检测目标，无论是检测水平边缘、垂直边缘还是其他角度的边缘，并习得图像的低层特征。</p><h2 id="3-2-卷积神经网络原理"><a href="#3-2-卷积神经网络原理" class="headerlink" title="3.2 卷积神经网络原理"></a>3.2 卷积神经网络原理</h2><h3 id="3-2-1-卷积神经网络的组成"><a href="#3-2-1-卷积神经网络的组成" class="headerlink" title="3.2.1 卷积神经网络的组成"></a>3.2.1 卷积神经网络的组成</h3><p>CNN由一个或多个卷积层、池化层、全连接层组成。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/CNN结构.png" alt="CNN结构"></p><p>与其他深度学习结构相比，CNN在图像等方面能够给出更好的结果。<br>CNN也可以用反向传播算法进行训练。<br>与其他浅层或深层神经网络相比，CNN的参数更少，更容易训练。</p><h3 id="3-2-2-卷积层（Convolutions）"><a href="#3-2-2-卷积层（Convolutions）" class="headerlink" title="3.2.2 卷积层（Convolutions）"></a>3.2.2 卷积层（Convolutions）</h3><ul><li>目的：提取输入的不同特征<ul><li>某些卷积层可能只能提取一些低级特征，如线条、边缘等，更多层的网络能从低级特征中提取更高级更复杂的特征。</li></ul></li><li>参数<ul><li>size：卷积核（filter）大小，如3x3、5x5等</li><li>padding：0填充，保持输出和输入的大小一致，Valid/Same</li><li>stride：步长，卷积核每次移动的距离，通常为1</li></ul></li><li>计算公式<ul><li>四个超参数：Filter数量$K$、Filter大小$F$、步长$S$、0填充大小$P$</li><li>输入体积：$H_1 \times W_1 \times D_1$</li><li>输出体积：$H_2 \times W_2 \times D_2$<ul><li>$H_2 = \dfrac{H_1-F+2P}{S}+1$</li><li>$W_2 = \dfrac{W_1-F+2P}{S}+1$</li><li>$D_2 = K$</li></ul></li></ul></li></ul><h4 id="3-2-2-1-卷积运算"><a href="#3-2-2-1-卷积运算" class="headerlink" title="3.2.2.1 卷积运算"></a>3.2.2.1 卷积运算</h4><p>卷积运算（符号$\ast$）：将一个矩阵（卷积核）应用到另一个矩阵的所有位置，求出每个位置的点积，得到一个新的矩阵。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/卷积运算.png" alt="卷积运算"></p><p>在这个6x6的矩阵中，左边一半都是1，右边一半都是0，中间是一条非常明显的垂直边缘。<br>经过卷积后，得到一个4x4的矩阵，中间的值非常大，其他值非常小，表明检测到了垂直边缘。</p><p>卷积运算的产生的问题：边缘像素的信息丢失、输出的图像尺寸变小</p><h4 id="3-2-2-2-padding-零填充"><a href="#3-2-2-2-padding-零填充" class="headerlink" title="3.2.2.2 padding-零填充"></a>3.2.2.2 padding-零填充</h4><p>在图片像素的最外层加上$P$层0，使得卷积后的输出和输入的尺寸一致。</p><p>0对最终结果不产生影响，避免图片增加噪声。</p><ul><li>Valid卷积：不填充，输出尺寸减小</li><li>Same卷积（一般采用）：填充0以维持输出尺寸与原图一致</li></ul><h4 id="3-2-2-3-size-卷积核大小"><a href="#3-2-2-3-size-卷积核大小" class="headerlink" title="3.2.2.3 size-卷积核大小"></a>3.2.2.3 size-卷积核大小</h4><p>卷积核大小$F$通常为3x3、5x5、7x7等奇数，保证能够确定一个中心点。</p><p>小卷积核可以保留更多的信息，大卷积核可以检测更大的特征。</p><h4 id="3-2-2-4-stride-步长"><a href="#3-2-2-4-stride-步长" class="headerlink" title="3.2.2.4 stride-步长"></a>3.2.2.4 stride-步长</h4><p>步长$S$通常为1，即卷积核每次移动一个像素。</p><p>步长为2时，卷积核每次移动两个像素，输出尺寸减小，示例如下：</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/2步长卷积.png" alt="2步长卷积"></p><h4 id="3-2-2-5-多通道卷积"><a href="#3-2-2-5-多通道卷积" class="headerlink" title="3.2.2.5 多通道卷积"></a>3.2.2.5 多通道卷积</h4><p>当输入有多个通道（Channel）时（例如图片可以有 RGB 三个通道），卷积核需要拥有相同的通道数</p><p>但最终的输出只有一个通道，其结果是多个通道的卷积结果的和。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/多通道卷积.png" alt="多通道卷积"></p><h4 id="3-2-2-6-多卷积核"><a href="#3-2-2-6-多卷积核" class="headerlink" title="3.2.2.6 多卷积核"></a>3.2.2.6 多卷积核</h4><p>当有多个卷积核时，可以学习到多种不同的特征，输出结果的通道数等于卷积核的数量，多卷积核可以理解为多神经元</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/多卷积核.png" alt="多卷积核"></p><h4 id="3-2-2-7-卷积层的运算结果"><a href="#3-2-2-7-卷积层的运算结果" class="headerlink" title="3.2.2.7 卷积层的运算结果"></a>3.2.2.7 卷积层的运算结果</h4><script type="math/tex; mode=display">\begin{split}&Z^{[l]} = W^{[l]} \ast A^{[l-1]} + b^{[l]} \\\\&A^{[l]} = g(Z^{[l]}) \end{split}</script><h3 id="3-2-3-池化层（Pooling）"><a href="#3-2-3-池化层（Pooling）" class="headerlink" title="3.2.3 池化层（Pooling）"></a>3.2.3 池化层（Pooling）</h3><p>池化层主要对卷积层的输出进行下采样（Subsampling）处理，主要分为：</p><ul><li>最大池化（Max Pooling）：取池化窗口中的最大值</li><li>平均池化（Average Pooling）：取池化窗口中的平均值</li></ul><p>池化通常为2x2的filter，步长为2，即每次取2x2的窗口中的最大值或平均值。</p><p>特点：没有参数，不需要学习，只是对输入数据进行简单的处理。</p><p>目的：降低数据维度；减少计算量、提高计算速度；防止过拟合，提高了鲁棒性。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/池化层.png" alt="池化层"></p><h3 id="3-2-4-全连接层（Fully-Connected）与CNN结构"><a href="#3-2-4-全连接层（Fully-Connected）与CNN结构" class="headerlink" title="3.2.4 全连接层（Fully Connected）与CNN结构"></a>3.2.4 全连接层（Fully Connected）与CNN结构</h3><p>全连接层即此前提到的多层神经网络，每个神经元与上一层的所有神经元相连。</p><p>卷积层+激活层+池化层可以看成是CNN的特征学习/特征提取层，学习到的特征（Feature Map）最终应用于模型任务（分类、回归）</p><ul><li>先对所有Feature Map进行扁平化（Flatten），即转化为一维向量</li><li>再连接到一个或多个全连接层，进行分类或回归任务</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/CNN结构2.png" alt="CNN结构2"></p><h2 id="3-3-经典分类网络结构"><a href="#3-3-经典分类网络结构" class="headerlink" title="3.3 经典分类网络结构"></a>3.3 经典分类网络结构</h2><p>通常采用从现成的经典网络结构进行优化，而不是从头开始设计网络结构。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/经典网络结构.png" alt="经典网络结构"></p><ul><li>NIN：引入了1x1卷积核</li><li>VGG：参数量巨大（1.4亿），19层网络</li><li>GoogleNet：500万参数，22层网络<ul><li>2014年比赛冠军的model，证明了用更多的卷积、更深的层次可以得到更好的结构</li><li>引入了Inception模块，多个不同大小的卷积核</li></ul></li></ul><h3 id="3-3-1-LeNet-5"><a href="#3-3-1-LeNet-5" class="headerlink" title="3.3.1 LeNet-5"></a>3.3.1 LeNet-5</h3><h4 id="3-3-1-1-LeNet-5-网络结构"><a href="#3-3-1-1-LeNet-5-网络结构" class="headerlink" title="3.3.1.1 LeNet-5 网络结构"></a>3.3.1.1 LeNet-5 网络结构</h4><p><img src="https://source.cclmsy.cc/Posts/DL/Note/LeNet5.png" alt="LeNet-5"></p><p>LeNet-5最初的目的用于手写数字识别，当时使用的激活函数是Sigmoid和Tanh，还没有出现Relu</p><h4 id="3-3-1-2-参数形状"><a href="#3-3-1-2-参数形状" class="headerlink" title="3.3.1.2 参数形状"></a>3.3.1.2 参数形状</h4><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Shape</th><th style="text-align:center">Size</th><th style="text-align:center">Params</th></tr></thead><tbody><tr><td style="text-align:center">Input</td><td style="text-align:center">(32, 32, 1)</td><td style="text-align:center">1024</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">Conv1(f=5,s=1)</td><td style="text-align:center">(28, 28, 6)</td><td style="text-align:center">4704</td><td style="text-align:center">5x5（卷积核大小）x3（通道数）x6（卷积核数量）+6（偏置）=456</td></tr><tr><td style="text-align:center">Pool1</td><td style="text-align:center">(14, 14, 6)</td><td style="text-align:center">1176</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">Conv2(f=5,s=1)</td><td style="text-align:center">(10, 10, 16)</td><td style="text-align:center">1600</td><td style="text-align:center">5x5x6x16+16=2416</td></tr><tr><td style="text-align:center">Pool2</td><td style="text-align:center">(5, 5, 16)</td><td style="text-align:center">400</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">FC3</td><td style="text-align:center">(120, 1)</td><td style="text-align:center">120</td><td style="text-align:center">400x120+120=48120</td></tr><tr><td style="text-align:center">FC4</td><td style="text-align:center">(84, 1)</td><td style="text-align:center">84</td><td style="text-align:center">120x84+84=10164</td></tr><tr><td style="text-align:center">Output:Sofmax</td><td style="text-align:center">(10, 1)</td><td style="text-align:center">10</td><td style="text-align:center">84x10+10=850</td></tr></tbody></table></div><ul><li>中间特征值的大小变化不宜过大，否则会导致信息丢失</li></ul><h3 id="3-3-2-AlexNet"><a href="#3-3-2-AlexNet" class="headerlink" title="3.3.2 AlexNet"></a>3.3.2 AlexNet</h3><p><img src="https://source.cclmsy.cc/Posts/DL/Note/AlexNet.png" alt="AlexNet"></p><ul><li>总参数量：6000万，8层神经网络，5个卷积层+3个全连接层</li><li>使用了非线性激活函数ReLU</li><li>使用Dropout防止过拟合，数据扩充</li><li>使用批标准化（Batch Normalization）加速训练</li></ul><h3 id="3-3-3-Inception结构"><a href="#3-3-3-Inception结构" class="headerlink" title="3.3.3 Inception结构"></a>3.3.3 Inception结构</h3><p>Inception结构是GoogleNet中的一个模块，由多个不同大小的卷积核组成，可以提取不同尺度的特征。</p><h4 id="3-3-3-1-MLP卷积（1x1卷积）"><a href="#3-3-3-1-MLP卷积（1x1卷积）" class="headerlink" title="3.3.3.1 MLP卷积（1x1卷积）"></a>3.3.3.1 MLP卷积（1x1卷积）</h4><p>一种新的深度网络结构Network in Network（NIN）提出了MLP卷积取代传统线性卷积核</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/MLP卷积.png" alt="MLP卷积"></p><ul><li>1x1卷积核对每个像素点的所有通道进行了线性组合</li><li>激活函数将feature map由多通道的线性组合变为非线性组合（信息整合）</li><li>提高特征抽象能力（Multilayer Perceptron，缩写MLP，就是一个多层神经网络）</li><li>主要作用：调整通道数（升维降维）、减少参数量</li></ul><h4 id="3-3-3-2-Inception层"><a href="#3-3-3-2-Inception层" class="headerlink" title="3.3.3.2 Inception层"></a>3.3.3.2 Inception层</h4><p>也称盗梦空间结构）</p><p>目的：代替人决定，使用哪种卷积核，或是需要MaxPool层，由网络自己学习寻找合适的结构，节省计算</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/Inception层.png" alt="Inception层"></p><p>提供以下4种不同的卷积核：</p><ul><li>1x1卷积核（64个）</li><li>3x3卷积核（128个），padding=same</li><li>5x5卷积核（32个），padding=same</li><li>2x2最大池化（32个），stride=1，padding=same</li><li>最终结果为这4种卷积核的拼接，27x27x256，使用更少的参数达到和AlexNet相当的效果</li></ul><h4 id="3-3-3-3-Inception改进"><a href="#3-3-3-3-Inception改进" class="headerlink" title="3.3.3.3 Inception改进"></a>3.3.3.3 Inception改进</h4><p>上一节中，计算量还是太大，参数还是太多，需要进一步改进。以5x5卷积核为例：</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/Inception改进.png" alt="Inception改进"></p><ul><li>上面为原方法，参数：$5\times5\times192\times32=153600$</li><li>下面为改进方法，网络缩小后再扩大，参数：$1\times1\times192\times16+5\times5\times16\times32=15872$</li></ul><h4 id="3-3-3-4-GoogleNet"><a href="#3-3-3-4-GoogleNet" class="headerlink" title="3.3.3.4 GoogleNet"></a>3.3.3.4 GoogleNet</h4><p>Inception模块的堆叠，形成GoogleNet网络结构</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/Inception模块.png" alt="Inception模块"></p><p>详细结构略</p><h2 id="3-4-卷积神经网络实战技巧"><a href="#3-4-卷积神经网络实战技巧" class="headerlink" title="3.4 卷积神经网络实战技巧"></a>3.4 卷积神经网络实战技巧</h2><h3 id="3-4-1-学习特征可视化"><a href="#3-4-1-学习特征可视化" class="headerlink" title="3.4.1 学习特征可视化"></a>3.4.1 学习特征可视化</h3><p>可以将网络学习过程中产生的特征图可视化出来，并且对比原图来看看每一层都干了什么</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/LeNet5例.webp" alt="LeNet5例"></p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/学习特征可视化.png" alt="学习特征可视化"></p><ul><li>Layer1,2：颜色、边缘等基本特征</li><li>Layer3：纹理、形状等中级特征</li><li>Layer4：稍复杂的特征，如狗的头部形状</li><li>Layer5：高级特征，如关键性区分特征</li></ul><h3 id="3-4-2-迁移学习（Transfer-Learning）"><a href="#3-4-2-迁移学习（Transfer-Learning）" class="headerlink" title="3.4.2 迁移学习（Transfer Learning）"></a>3.4.2 迁移学习（Transfer Learning）</h3><p>如果需要做一个具体场景的计算机视觉任务，可以使用已经训练好的模型，然后在此基础上进行微调。</p><h4 id="3-4-2-1-介绍"><a href="#3-4-2-1-介绍" class="headerlink" title="3.4.2.1 介绍"></a>3.4.2.1 介绍</h4><p>迁移学习就是利用数据、任务或模型之间的相似性（例如都是图像分类任务），将在旧的领域学习过或训练好的模型，应用于新的领域的过程。</p><p>从以下两个方面考虑训练模型的现实问题：</p><ol><li>数据集大小：如果新任务的数据集很小，迁移学习可以帮助提高模型的泛化能力<ul><li>如果有海量的数据集支持，可以不需要迁移学习，直接从海量数据中训练出一个学习到一个鲁棒性很强的模型</li><li>但是，通常情况下，需要研究的领域数据集非常有限，导致模型的泛化能力极差</li></ul></li><li>训练成本：从头开始训练一个CNN模型需要大量的时间和计算资源</li></ol><h4 id="3-4-2-2-微调（Fine-tuning）"><a href="#3-4-2-2-微调（Fine-tuning）" class="headerlink" title="3.4.2.2 微调（Fine-tuning）"></a>3.4.2.2 微调（Fine-tuning）</h4><p>在一个已经训练好的模型（Pre-trained Model）上进行针对性优化，以提升模型在特定任务上的性能。</p><p>假设有两个任务$A$和$B$。<br>任务$A$拥有海量数据，以$a$为条件区分1000个类别，已经训练好了一个模型。<br>目标任务为$B$，以$b$为条件区分250个类别，数据集很小。</p><p>步骤：</p><ol><li>在$A$模型的基础上，将最后一层的输出层替换为250个类别的输出层，保持前面的参数不变</li><li>根据数据量，决定是否冻结（Freeze）前面的层（权重不变），只训练最后若干层并更新参数<ul><li>数据越多，就保留越多的层，从后往前逐渐解冻</li><li>数据很少，只保留输出层，其他层全部解冻</li></ul></li><li>重新训练模型</li></ol>]]></content>
    
    
    <summary type="html">利用卷积运算处理图像数据提取特征的网络</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记|改善深层神经网络</title>
    <link href="https://www.cclmsy.cc/posts/deep_learning_1.html"/>
    <id>https://www.cclmsy.cc/posts/deep_learning_1.html</id>
    <published>2025-02-20T16:00:00.000Z</published>
    <updated>2025-03-14T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、神经网络实践相关"><a href="#一、神经网络实践相关" class="headerlink" title="一、神经网络实践相关"></a>一、神经网络实践相关</h2><h3 id="1-1-数据集划分"><a href="#1-1-数据集划分" class="headerlink" title="1.1 数据集划分"></a>1.1 数据集划分</h3><p>数据集：</p><ul><li>训练集（Training Set）：用于模型的训练过程</li><li>验证集（Validation Set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行交叉验证，选择出最好的模型</li><li>测试集（Test Set）：用于评估模型的能力</li></ul><p>数据集划分比例：</p><ul><li>小数据量（小于10万）比例：无验证集7:3，有验证集6:2:2</li><li>大数据量比例：98:1:1、99.5:0.25:0.25、99.5:0.4:0.1</li></ul><h3 id="1-2-偏差和方差"><a href="#1-2-偏差和方差" class="headerlink" title="1.2 偏差和方差"></a>1.2 偏差和方差</h3><p>“偏差-方差分解”（bias-variance decomposition）是解释学习算法泛化性能的一种重要工具。</p><p>泛化误差可分解为偏差、方差与噪声，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。</p><ul><li>偏差（Bias）：度量学习算法的期望预测与真实结果的偏离程度，反映了<strong>模型本身的拟合能力</strong></li><li>方差（Variance）：度量同样大小的训练集的变动所导致的学习性能的变化，反映了<strong>模型的稳定性</strong></li><li>噪声：当前任务下任何学习算法所能达到的期望泛化误差的下界，反映了<strong>问题本身的难度</strong></li></ul><p>偏差、方差与数据集划分的关系及解决方法：</p><ol><li>训练集错误率小、测试集错误率大：高方差，可能出现了<strong>过拟合</strong><ul><li>增大数据集，使训练尽可能包含所有情况</li><li>寻找更合适的网络结构</li><li>正则化</li></ul></li><li>训练集错误率大、测试集错误率大：高偏差，可能出现了<strong>欠拟合</strong><ul><li>扩大网络规模，例如增加隐藏层或神经元数量</li><li>寻找更合适的网络结构，使用更大的网络</li><li>增加训练时间、迭代次数</li></ul></li><li>训练集错误率小、测试集错误率小：方差和偏差都小，模型效果较好</li></ol><h3 id="1-3-逻辑回归的L1和L2正则化"><a href="#1-3-逻辑回归的L1和L2正则化" class="headerlink" title="1.3 逻辑回归的L1和L2正则化"></a>1.3 逻辑回归的L1和L2正则化</h3><p><strong>正则化（Regularization）</strong>：在成本函数中加入一个正则化项（惩罚项），惩罚模型的复杂度，防止网络过拟合</p><p>逻辑回归中，参数$W$的数量由特征数决定，正则化如下：</p><ul><li>L1正则化：$J(W,b) = \dfrac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\dfrac{\lambda}{2m}||W||_1$</li><li>L2正则化：$J(W,b) = \dfrac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\dfrac{\lambda}{2m}||W||_2^2$<ul><li>L2范数：$\dfrac{\lambda}{2m}||W||<em>2^2 = \dfrac{\lambda}{2m}\sum\limits</em>{i=1}^{n}W_i^2=\dfrac{\lambda}{2m}W^TW$</li><li>解释：所有W参数的平方和</li></ul></li></ul><p>正则化因子$\lambda$：超参数，控制正则化项的权重，$\lambda$越大，正则化项的影响越大，模型越简单，防止过拟合。</p><p>L1正则化后，$W$的某些参数会变为0，使模型变稀疏，因此L2正则化更常用。</p><p>梯度下降的目的是减小损失函数$J(W,b)$值的大小。<br>在损失函数中增加了一项，导致$dW$增大，$W$减小的更多（多减去一项），因此L2范数也称为权重衰减（Weight Decay）。</p><h3 id="1-4-神经网络中的L2正则化、Frobenius范数"><a href="#1-4-神经网络中的L2正则化、Frobenius范数" class="headerlink" title="1.4 神经网络中的L2正则化、Frobenius范数"></a>1.4 神经网络中的L2正则化、Frobenius范数</h3><p>对每一层的权重矩阵$W^{[l]}$进行正则化，每一层都有若干个权重，可以理解为矩阵</p><script type="math/tex; mode=display">J(W^{[1]},b^{[1]},...,W^{[L]},b^{[L]}) = \dfrac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\dfrac{\lambda}{2m}\sum\limits_{l=1}^{L}||W^{[l]}||_F^2</script><p>其中，$||W^{[l]}||_F^2$为Frobenius范数，表示矩阵的所有元素的平方和。</p><h3 id="1-5-正则化减少过拟合的原理"><a href="#1-5-正则化减少过拟合的原理" class="headerlink" title="1.5 正则化减少过拟合的原理"></a>1.5 正则化减少过拟合的原理</h3><p>正则化因子设置的足够大的情况下，为了使损失函数最小化，权重矩阵会趋向于0，削弱隐藏层影响，使得模型变得简单。</p><p>选取一个适合的$\lambda$，可以使得模型的复杂度适中，防止过拟合。</p><h3 id="1-6-Dropout正则化"><a href="#1-6-Dropout正则化" class="headerlink" title="1.6 Dropout正则化"></a>1.6 Dropout正则化</h3><p>Dropout正则化：在训练过程中，随机关闭一些神经元，减少神经元之间的依赖关系，防止过拟合。</p><p>Inverted Dropout：在训练过程中，对每一层的神经元，以概率$keep_prob$保留。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span>  <span class="comment"># 保留概率</span></span><br><span class="line">dl = np.random.rand(A.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]) &lt; keep_prob  </span><br><span class="line">A = np.multiply(A, dl)  <span class="comment"># 保留神经元</span></span><br><span class="line"></span><br><span class="line"> 部分神经元被关闭，期望值预计下降为原来的$keep_prob$，需要缩放激活值保持期望值不变</span><br><span class="line">A = A / keep_prob </span><br></pre></td></tr></table></figure><p>加入了Droupout后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征。<br>通过传播过程，Dropout将产生和L2正则化相同的收缩权重的效果。</p><p>对于神经元较多的层，设置较小的keep_prob，对于神经元较少的层，设置keep_prob=1。</p><p>在CV领域，图像具有更多的特征，Dropout是一种非常有效的正则化方法。</p><p>缺点：</p><ol><li>因为每次会随机消除一部分神经元，成本函数无法被明确定义</li><li>参数无法确定具体是哪些，在反向传播的时候带来计算上的麻烦，无法保证当前网络是否损失函数下降的</li></ol><h3 id="1-7-其他正则化方法"><a href="#1-7-其他正则化方法" class="headerlink" title="1.7 其他正则化方法"></a>1.7 其他正则化方法</h3><h4 id="早停止法（Early-Stopping）"><a href="#早停止法（Early-Stopping）" class="headerlink" title="早停止法（Early Stopping）"></a>早停止法（Early Stopping）</h4><p>如果训练迭代的次数过高，会发生过拟合，损失函数图像如下：</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/早停止法.png" alt="早停止法"></p><p>早停止法：在测试集上的损失减少到一定程度后，停止训练，防止过拟合。</p><p>这种方法治标不治本，还是需要从根本上解决数据或模型的问题。</p><h4 id="数据增强（Data-Augmentation）"><a href="#数据增强（Data-Augmentation）" class="headerlink" title="数据增强（Data Augmentation）"></a>数据增强（Data Augmentation）</h4><p>算法在学习区分两种类别时，可能会寻找到一个最明显的特征。</p><p>例如在区分两种不同型号的车时，如果训练集中，型号1的车都朝左，型号2的车都朝右，那么模型可能会认为车的朝向是区分两种车的最重要特征。<br>在测试集中，如果出现了朝右的型号1车，模型可能认为是型号2车。<br>因此，需要减少数据集中不相关的特征的数量。</p><p>数据增强：通过对训练集进行一系列的随机变换（如剪切、旋转、翻转、缩放等），增加训练集的样本数量，提高模型的泛化能力。</p><p>在上面的例子中，可以通过水平翻转图像，以防止模型学习到不相关的模式。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/数据增强.png" alt="数据增强"></p><ul><li>离线增强：预先进行所有的必要转换，从根本上增大数据集规模（如水平翻转后，保存为新的图像，数据集增大为原来的两倍）</li><li>在线增强：在训练过程中，对即将输入模型的小批量数据进行相应变换，同一张图每次训练被随机执行一些变化操作，相当于不同数据集</li></ul><p>数据增强的效果如下</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/数据增强的效果.png" alt="数据增强的效果"></p><h3 id="1-8-正则化输入"><a href="#1-8-正则化输入" class="headerlink" title="1.8 正则化输入"></a>1.8 正则化输入</h3><p>对于输入数据进行正则化处理，使数据都服从同一分布，能够缓解梯度消失和梯度爆炸问题，并且加速算法的收敛。</p><ul><li>正则化公式：$X = \dfrac{X-\mu}{\sigma}$，其中$\mu$为均值，$\sigma$为标准差</li></ul><h3 id="1-9-梯度消失与梯度爆炸"><a href="#1-9-梯度消失与梯度爆炸" class="headerlink" title="1.9 梯度消失与梯度爆炸"></a>1.9 梯度消失与梯度爆炸</h3><p>由于链式法则是一个连乘的过程，当层数越深时，梯度以指数速度增长传播。</p><ul><li>梯度消失：梯度小于1，多次连乘后，梯度趋近于0，导致参数几乎不更新、模型难收敛</li><li>梯度爆炸：梯度大于1，多次连乘后，梯度趋近于无穷，导致参数更新过大甚至溢出</li></ul><p>局部最优解：损失函数可能存在鞍点/局部最小值</p><ul><li>较大的神经网络，局部最优解的可能性较小</li><li>鞍点附近的平稳段会使得学习非常缓慢，需要优化算法加速学习</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/局部最优解.gif" alt="局部最优解"></p><p>解决方法：</p><ol><li>初始化参数策略：将权重初始化为较小的随机数</li><li>批梯度下降、Mini-batch梯度下降、随机梯度下降</li><li>梯度下降算法内部优化：动量梯度下降、RMSProp算法、Adam算法</li><li>学习率衰减</li><li>标准化输入</li></ol><h3 id="1-10-权重初始化"><a href="#1-10-权重初始化" class="headerlink" title="1.10 权重初始化"></a>1.10 权重初始化</h3><p>为了避免对称性，权重初始化不能全为0，通常使用较小的随机数进行初始化。</p><p>在逻辑回归的笔记中，权重初始化为<code>np.random.randn(k, n) * 0.01</code>。</p><p>Xavier初始化：$w^{[l]} = np.random.randn(k, n) \times \sqrt{\dfrac{1}{n^{[l-1]}}}$，其中$n^{[l-1]}$为上一层的神经元数量。</p><p>如果使用ReLU激活函数，权重初始化为 $w^{[l]} = np.random.randn(k, n) \times \sqrt{\dfrac{2}{n^{[l-1]}}}$。</p><h2 id="二、优化算法"><a href="#二、优化算法" class="headerlink" title="二、优化算法"></a>二、优化算法</h2><h3 id="2-1-批梯度下降、Mini-batch梯度下降、随机梯度下降"><a href="#2-1-批梯度下降、Mini-batch梯度下降、随机梯度下降" class="headerlink" title="2.1 批梯度下降、Mini-batch梯度下降、随机梯度下降"></a>2.1 批梯度下降、Mini-batch梯度下降、随机梯度下降</h3><p>批梯度下降（Batch Gradient Descent）：同时处理整个训练集</p><ul><li>在更新参数前，必须先处理整个训练参数集，才能进行一步梯度下降</li><li>如果训练集很大，计算量会很大，训练速度很慢</li><li>噪声低，代价函数值平滑减小</li><li>训练样本的大小较小（小于2048）时，选择Batch梯度下降</li></ul><p>Mini-batch梯度下降：将训练集分为多个固定大小的批次，每次只处理一个批次的数据</p><ul><li>训练样本的大小较大时，选择Mini-batch梯度下降，通常为64、128、256、512等</li></ul><p>随机梯度下降（Stochastic Gradient Descent, SGD）：mini-batch大小为1，每次只处理一个样本</p><ul><li>训练速度快，但丢失了向量化编程的优势</li><li>代价函数值波动大，噪声大，总体向全局最小值靠近，但难以收敛，容易在鞍点震荡</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/梯度下降代价函数值变化趋势.png" alt="梯度下降代价函数值变化趋势"></p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/梯度下降代价函数值变化趋势2.png" alt="梯度下降代价函数值变化趋势2"></p><h3 id="2-2-指数加权平均"><a href="#2-2-指数加权平均" class="headerlink" title="2.2 指数加权平均"></a>2.2 指数加权平均</h3><p>指数加权平均（Exponentially Weight Average）是一种常用的序列数据处理方式，通常用在序列场景，如金融序列分析、温度变化序列分析。</p><script type="math/tex; mode=display">S_t = \begin{cases}x_t &, t=1 \\\\ \beta S_{t-1}+(1-\beta)x_t &, t>1\end{cases}</script><p>理解：上一结果的权重为$\beta$，当前数据的权重为$1-\beta$。</p><p>下图，黄色$\beta=0.5$，红色$\beta=0.9$，绿色$\beta=0.98$</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/指数加权平均.png" alt="指数加权平均"></p><h3 id="2-3-动量梯度下降"><a href="#2-3-动量梯度下降" class="headerlink" title="2.3 动量梯度下降"></a>2.3 动量梯度下降</h3><p>动量梯度下降（Momentum Gradient Descent）：利用梯度的指数加权平均来更新参数</p><script type="math/tex; mode=display">\begin{split}&S_{dW} = \beta S_{dW}+(1-\beta)dW \\\\ &S_{db} = \beta S_{db}+(1-\beta)db \\\\ &W = W-\alpha S_{dW} \\\\ &b = b-\alpha S_{db}\end{split}</script><p>利用累加的梯度值，减少梯度下降的震荡，加速收敛</p><ul><li>前后梯度方向不一致时，梯度值减小，减少震荡</li><li>前后梯度方向一致时，梯度值增大，加速收敛</li></ul><h3 id="2-2-5-RMSProp算法"><a href="#2-2-5-RMSProp算法" class="headerlink" title="2.2.5 RMSProp算法"></a>2.2.5 RMSProp算法</h3><p>RMSProp算法（Root Mean Square Propagation）：再对梯度进行指数加权平均的基础上，引入平方和平方根</p><script type="math/tex; mode=display">\begin{split}&S_{dW} = \beta S_{dW}+(1-\beta)dW^2 \\\\ &S_{db} = \beta S_{db}+(1-\beta)db^2 \\\\ &W = W-\alpha \dfrac{dW}{\sqrt{S_{dW}+\epsilon}} \\\\ &b = b-\alpha \dfrac{db}{\sqrt{S_{db}+\epsilon}}\end{split}</script><p>其中，$\epsilon$是一个很小的数，避免分母过小导致数值不稳定。</p><p>RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率$\alpha$，加快算法学习速度。</p><h3 id="2-2-6-Adam算法"><a href="#2-2-6-Adam算法" class="headerlink" title="2.2.6 Adam算法"></a>2.2.6 Adam算法</h3><p>Adam算法（Adaptive Moment Estimation，自适应矩估计）：结合了Momentum和RMSProp算法，同时考虑梯度的一阶矩估计和二阶矩估计</p><p>假设用一个mini-batch计算$dW$和$db$，第$t$次迭代时，计算动量梯度结果：</p><script type="math/tex; mode=display">\begin{split}&V_{dW} = \beta_1 V_{dW}+(1-\beta_1)dW \\\\ &V_{db} = \beta_1 V_{db}+(1-\beta_1)db \\\\ &V_{dW}^{corrected} = \dfrac{V_{dW}}{1-\beta_1^t} \end{split}</script><p>计算RMSProp结果：</p><script type="math/tex; mode=display">\begin{split}&S_{dW} = \beta_2 S_{dW}+(1-\beta_2)dW^2 \\\\ &S_{db} = \beta_2 S_{db}+(1-\beta_2)db^2 \\\\ &S_{dW}^{corrected} = \dfrac{S_{dW}}{1-\beta_2^t}\end{split}</script><p>计算移动平均数时，使用系数$\dfrac{1}{1-\beta_1^t}$进行修正。<br>例如$m_0=0,m_1=0.9m_0+0.1m_1$，导致$m_1$的值过小，修正后恰好为$m_1$的值。<br>随着迭代次数增加，修正系数趋近于1，保证了移动平均数的准确性。</p><p>Adam算法更新参数：</p><script type="math/tex; mode=display">\begin{split}&W = W-\alpha \dfrac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\epsilon} \\\\ &b = b-\alpha \dfrac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\epsilon}\end{split}</script><h3 id="2-2-7-学习率衰减"><a href="#2-2-7-学习率衰减" class="headerlink" title="2.2.7 学习率衰减"></a>2.2.7 学习率衰减</h3><p>如果随着时间慢慢减少学习率$\alpha$的大小，在初期$\alpha$较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小$\alpha$的值，即减小步长，有助于算法的收敛，更容易接近最优解。</p><p>最常用的学习率衰减方法：$\alpha = \dfrac{1}{1+decay_ rate \times epoch_ num} \times \alpha_0$</p><ul><li>$\alpha_0$：初始学习率</li><li>$decay_ rate$：衰减率，超参数</li><li>$epoch_ num$：迭代次数</li></ul><p>一种指数衰减学习率：$\alpha = 0.95^{epoch_ num} \times \alpha_0$</p><h2 id="三、超参数调试、Batch正则化和编程框架"><a href="#三、超参数调试、Batch正则化和编程框架" class="headerlink" title="三、超参数调试、Batch正则化和编程框架"></a>三、超参数调试、Batch正则化和编程框架</h2><h3 id="3-1-神经网络调优"><a href="#3-1-神经网络调优" class="headerlink" title="3.1 神经网络调优"></a>3.1 神经网络调优</h3><p>算法层面：</p><ul><li>学习率$\alpha$</li><li>$\beta_1,\beta_2,\epsilon$：Adam算法的超参数，常用值$\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$</li><li>正则化参数$\lambda$</li></ul><p>模型层面：</p><ul><li>hidden units：隐藏层神经元数</li><li>layers：隐藏层层数</li></ul><p>调参技巧</p><ul><li>网格搜索：遍历所有可能的参数组合，测试每一组的效果，选择效果最好的参数组合。</li><li>尽量让每一组差别明显，避免重复测试</li><li>合理的参数设置：为超参数选择合适的范围<ul><li>学习率$\alpha$：通常设置为0.0001、0.001、0.01、0.1等</li><li>动量梯度因子$\beta$: 通常设置为0.999、0.9995、0.9999等，尽可能接近1（指数增加效应）</li></ul></li></ul><p>问题：调参过程麻烦、训练时间长</p><h3 id="2-4-2-批标准化（批标准化）"><a href="#2-4-2-批标准化（批标准化）" class="headerlink" title="2.4.2 批标准化（批标准化）"></a>2.4.2 批标准化（批标准化）</h3><p>论文地址：<a href="https://arxiv.org/abs/1502.03167">批标准化: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><blockquote><p>训练深度神经网络很复杂，因为在训练期间每层输入的分布发生变化，因为前一层的参数发生了变化。<br>这通过要求较低的学习率和仔细的参数初始化来减慢训练速度，并且使得训练具有饱和非线性的模型变得非常困难。<br>我们将这种现象称为<strong>内部协变量偏移</strong>，并通过<strong>标准化层</strong>输入来解决问题。<br>我们的方法的优势在于使标准化成为模型体系结构的一部分，并为每个培训小批量执行标准化。<br>批标准化允许我们使用更高的学习率并且不太关心初始化。<br>它还可以充当调节器，在某些情况下可以消除对Dropout的需求。<br>应用于最先进的图像分类模型，批量标准化实现了相同的精度，培训步骤减少了14倍，并且显着地超过了原始模型。<br>使用批量标准化网络的集合，我们改进了ImageNet分类的最佳发布结果：达到4.9％的前5个验证错误（和4.8％的测试错误），超出了人类评估者的准确性。</p></blockquote><p>批标准化：在神经网络的每一层的<strong>激活函数之前</strong>，对每一层的<strong>输入</strong>进行标准化处理，使得每一层的输入数据服从均值为0、方差为1的正态分布。</p><p>批标准化公式：</p><script type="math/tex; mode=display">\begin{split}&\mu = \dfrac{1}{m}\sum\limits_{i=1}^{m}Z^{(i)} \\\\ &\sigma^2 = \dfrac{1}{m}\sum\limits_{i=1}^{m}(Z^{(i)}-\mu)^2 \\\\ &Z_{norm}^{(i)} = \dfrac{Z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}\end{split}</script><p>其中，$\mu$为均值，$\sigma^2$为方差，$\epsilon$为一个很小的数，避免分母为0。</p><p>如果各隐藏层的输入均值在靠近0的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。<br>因此添加两个可学习的参数$\gamma$和$\beta$，对标准化后的数据进行缩放和平移。</p><script type="math/tex; mode=display">\tilde{Z}^{(i)} = \gamma Z_{norm}^{(i)}+\beta</script><p><img src="https://source.cclmsy.cc/Posts/DL/Note/批标准化过程.png" alt="批标准化过程"></p><p>批标准化优化训练过程的原理：</p><p>数据的分布会随着不同数据集改变。<br>网络的参数会因训练集数据分布的变化而变化；测试的数据分布与训练集的数据分布不同，也会导致准确性下降。</p><p>批标准化的作用就是减小了数据分布的变化带来的影响，让模型更健壮，鲁棒性更强。<br>即使输入的值改变，由于批标准化的作用，均值和方差的变化会被消除，后续的学习更加容易。<br>批标准化减少了各层W和b之间的耦合性，让各层更加独立，实现自我训练学习的效果</p><p>批标准化也起到微弱的正则化效果，但是不能将批标准化作为正则化的手段，而是当作加速学习的方式。<br>批标准化 主要解决的还是反向传播过程中的梯度问题（梯度消失和爆炸）。</p>]]></content>
    
    
    <summary type="html">超参数调试、正则化以及优化</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记|神经网络和深度学习</title>
    <link href="https://www.cclmsy.cc/posts/deep_learning_1.html"/>
    <id>https://www.cclmsy.cc/posts/deep_learning_1.html</id>
    <published>2025-02-19T16:00:00.000Z</published>
    <updated>2025-03-13T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、深度学习概论"><a href="#一、深度学习概论" class="headerlink" title="一、深度学习概论"></a>一、深度学习概论</h2><h3 id="1-1-什么是深度学习"><a href="#1-1-什么是深度学习" class="headerlink" title="1.1 什么是深度学习"></a>1.1 什么是深度学习</h3><p>简单来说，深度学习（Deep Learning）就是更复杂的神经网络（Neural Network）</p><p>人工神经网络包含：输入层、隐藏层、输出层，每层包含多个神经元，每个神经元包含激活函数</p><p>神经网络需要从大量的数据中学习，学习的过程就是调整网络中的参数，使得网络的输出结果与实际结果尽可能接近。</p><p>学习的目标是，建立起一个特殊的函数，输入一些数据就能输出想要的结果。</p><h3 id="1-2-深度学习的应用"><a href="#1-2-深度学习的应用" class="headerlink" title="1.2 深度学习的应用"></a>1.2 深度学习的应用</h3><p>监督式学习（Supervised Learning）与非监督式学习本质区别就是：训练样本是否已知的输出y</p><p>不同的任务通常交给不同的神经网络：</p><ul><li>分类/回归任务：神经网络（Neural Network, NN）</li><li>图像识别任务：卷积神经网络（Convolutional Neural Network, CNN）</li><li>文本/语音等序列任务：循环神经网络（Recurrent Neural Network, RNN）</li><li>生成任务：生成对抗网络（Generative Adversarial Network, GAN）</li></ul><p>机器学习应用于结构化数据（Structured Data）和非结构化数据（Unstructured Data）</p><ul><li>结构化数据：数据的数据库，意味着每个特征都有清晰的定义，比较容易理解。</li><li>非结构化数据：通常指的是比较抽象的数据，比如音频、原始音频、图像、文本。</li></ul><p>正是因为神经网络，计算机现在能更好地解释非结构化数据，甚至在某些方面优于人类。例如，语音识别，图像识别，自然语音处理等。</p><h3 id="1-3-深度学习的优点"><a href="#1-3-深度学习的优点" class="headerlink" title="1.3 深度学习的优点"></a>1.3 深度学习的优点</h3><p>深度学习兴起的原因：数据（Data）、计算（Computation）、算法（Algorithm）</p><p>深度学习的优点：</p><ul><li>不需要人工处理设计特征，只需通过神经网络输出结果</li><li>更适用于难提取特征的任务：图像、语音、自然语言处理</li><li>能够应对处理更大规模数据</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/data_performance.jpeg" alt="性能随数据量增大的变化"></p><h2 id="二、神经网络基础"><a href="#二、神经网络基础" class="headerlink" title="二、神经网络基础"></a>二、神经网络基础</h2><h3 id="2-1-逻辑回归（Logistic-Regression-LR）"><a href="#2-1-逻辑回归（Logistic-Regression-LR）" class="headerlink" title="2.1 逻辑回归（Logistic Regression, LR）"></a>2.1 逻辑回归（Logistic Regression, LR）</h3><p>逻辑回归是一种用于解决二分类问题的分类算法，给定一个输入x，输出y=1的预测概率 $\hat{y}=P(y=1|x)$</p><p>记输入层特征数$n$，参数：</p><ul><li>输入$x \in R^{n}$，x是一个n维的特征向量</li><li>权重$w \in R^{n}$，w是一个n维的权重向量</li><li>标签$y \in {0,1}$，y是一个二分类标签</li><li>偏置$b \in R$，b是一个标量</li><li>输出$\hat{y} = \sigma(w^{T}x+b)=\sigma(w_1x_1+w_2x_2+…+w_nx_n+b)$<ul><li>激活函数：Sigmoid函数：$\sigma(t)=\dfrac{1}{1+e^{-t}}$</li><li>t非常大时，s接近1；t非常小时，s接近0；t=0时，s等于0.5</li></ul></li></ul><h3 id="2-2-与梯度下降算法（Gradient-Descent）"><a href="#2-2-与梯度下降算法（Gradient-Descent）" class="headerlink" title="2.2 与梯度下降算法（Gradient Descent）"></a>2.2 与梯度下降算法（Gradient Descent）</h3><p>通过迭代更新参数w和b，使成本函数$J(w,b)$找到最小值（损失函数和成本函数见2.7）</p><p>梯度下降算法：函数的梯度（gradient）指出了函数的最陡增长方向。梯度的方向走，函数增长得就越快。那么按梯度的负方向走，函数值自然就降低得最快了</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/损失函数图.png" alt="损失函数图"></p><p>参数更新：</p><ul><li>$w:=w-\alpha \dfrac{\partial J(w,b)}{\partial w}$</li><li>$b:=b-\alpha \dfrac{\partial J(w,b)}{\partial b}$</li><li>$\alpha$：学习率（Learning Rate），控制参数更新的步长，太大会导致震荡，太小会导致收敛速度慢</li></ul><h3 id="2-3-逻辑回归的梯度下降"><a href="#2-3-逻辑回归的梯度下降" class="headerlink" title="2.3 逻辑回归的梯度下降"></a>2.3 逻辑回归的梯度下降</h3><p>以2维样本 $x_1,x_2$ 为例，参数$w_1,w_2,b$，计算梯度下降</p><p>已知：</p><script type="math/tex; mode=display">\begin{split}&z = w_1x_1+w_2x_2+b \\\\&记 a = \hat{y} = \sigma(z) \\\\&J(a,y) = -y\log(a)-(1-y)\log(1-a) \\\\\end{split}</script><p>计算J对z的导数：</p><script type="math/tex; mode=display">\begin{split}&\frac{\partial J}{\partial a} = \frac{-y}{a}+\frac{1-y}{1-a} \\\\&\frac{\partial a}{\partial z} = a(1-a) \\\\&dz = \frac{\partial J}{\partial a} \cdot \frac{\partial a}{\partial z} = a-y\end{split}</script><p>这样可以求出总损失相对于$w_1,w_2,b$的导数</p><ul><li>$dw_1 = \frac{\partial J}{\partial z} \cdot \frac{\partial z}{\partial w_1} = x_1(a-y)$</li><li>$dw_2 = \frac{\partial J}{\partial z} \cdot \frac{\partial z}{\partial w_2} = x_2(a-y)$</li><li>$db = \frac{\partial J}{\partial z} \cdot \frac{\partial z}{\partial b} = a-y = dz$</li></ul><p>然后更新参数：</p><ul><li>$w_1:=w_1-\alpha dw_1$</li><li>$w_2:=w_2-\alpha dw_1$</li><li>$b:=b-\alpha dw_1$</li></ul><h3 id="2-4-前向传播和反向传播"><a href="#2-4-前向传播和反向传播" class="headerlink" title="2.4 前向传播和反向传播"></a>2.4 前向传播和反向传播</h3><ul><li>前向传播：从前往后计算梯度和损失的过程</li><li>反向传播：从后往前计算参数的更新梯度值</li></ul><h3 id="2-5-向量化-逻辑回归实现"><a href="#2-5-向量化-逻辑回归实现" class="headerlink" title="2.5 向量化/逻辑回归实现"></a>2.5 向量化/逻辑回归实现</h3><p>向量化编程的优点：多样本下，向量计算比循环计算快的多，代码更简洁</p><ol><li>输入层$X$：形状$n \times m$，$n$为特征数，$m$为样本数</li><li>权重参数$W$：形状$n \times 1$</li><li>偏置参数$b$：标量</li><li>输出层$Z$：$Z=W^{T}X+b$，形状$(1,n) \times (n,m) + b = (1,m)$  </li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">n = <span class="number">20</span>  <span class="comment"># 特征数</span></span><br><span class="line">m = <span class="number">100</span>  <span class="comment"># 样本数</span></span><br><span class="line">alpha = <span class="number">0.001</span>  <span class="comment"># 学习率</span></span><br><span class="line">iterations = <span class="number">1000</span>  <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line">X = np.random.randn(n, m)  <span class="comment"># 生成特征矩阵</span></span><br><span class="line">Y = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, [<span class="number">1</span>, m])  <span class="comment"># 生成标签</span></span><br><span class="line"></span><br><span class="line">J = <span class="number">0</span>  <span class="comment"># 损失函数值</span></span><br><span class="line"></span><br><span class="line"> 初始化权重和偏置</span><br><span class="line">W = np.random.randn(n, <span class="number">1</span>) * <span class="number">0.01</span></span><br><span class="line">b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">t</span>): <span class="comment"># Sigmoid函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-t))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_loss</span>(<span class="params">A, Y</span>):  <span class="comment"># 对数损失函数</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span> / m * np.<span class="built_in">sum</span>(Y * np.log(A) + (<span class="number">1</span> - Y) * np.log(<span class="number">1</span> - A))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_accuracy</span>(<span class="params">A, Y</span>):  <span class="comment"># 计算准确率</span></span><br><span class="line">    A = np.where(A &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> np.mean(A == Y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    Z = np.dot(W.T, X) + b</span><br><span class="line">    A = sigmoid(Z)  <span class="comment"># 激活值（预测）</span></span><br><span class="line">    J = log_loss(A, Y)  <span class="comment"># 计算损失函数值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    dZ = A - Y</span><br><span class="line">    dW = <span class="number">1</span> / m * np.dot(X, dZ.T) <span class="comment"># 1/m * X * dZ</span></span><br><span class="line">    db = <span class="number">1</span> / m * np.<span class="built_in">sum</span>(dZ) <span class="comment"># 1/m * dZ</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新权重和偏置</span></span><br><span class="line">    W = W - alpha * dW</span><br><span class="line">    b = b - alpha * db</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loss:&quot;</span>, J)  <span class="comment"># 打印损失函数值</span></span><br><span class="line"></span><br><span class="line"> 训练集上的准确率</span><br><span class="line">Z = np.dot(W.T, X) + b</span><br><span class="line">A = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-Z))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span>, calc_accuracy(A, Y))</span><br></pre></td></tr></table></figure><h3 id="2-6-激活函数（Activation-Function）"><a href="#2-6-激活函数（Activation-Function）" class="headerlink" title="2.6 激活函数（Activation Function）"></a>2.6 激活函数（Activation Function）</h3><p>涉及到网络的优化时候，会有不同的激活函数选择。<br>有一个问题是神经网络的隐藏层和输出单元用什么激活函数。<br>在逻辑回归中选用了Sigmoid函数，但有时其他函数的效果会好得多。<br>大多数结论通过实践得来，没有很好的解释性。</p><p>为什么使用非线性的激活函数：使用线性函数，在这一层上的神经元的输出仅仅是输入的线性组合，失去了效果。</p><script type="math/tex; mode=display">a^{[1]} = W^{[1]}x+b^{[1]}</script><script type="math/tex; mode=display">\begin{align}    b^{[1]} &= W^{[2]}a^{[1]}+b^{[2]} \\\\    &= W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]} \\\\    &= (W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]}) \\\\    &= Wx+b\end{align}</script><p>本节提及的几种常用激活函数：Sigmoid函数、tanH函数、ReLU函数</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/激活函数.png" alt="激活函数"></p><p>Sigmoid函数</p><script type="math/tex; mode=display">\begin{split} &\sigma(t)=\dfrac{1}{1+e^{-t}} \\\\&\sigma'(t)=\sigma(t)(1-\sigma(t)) \\\\&t\in(-\infty,+\infty), \sigma(t)\in(0,1)\end{split}</script><p><img src="https://source.cclmsy.cc/Posts/DL/Note/Sigmoid函数.png" alt="Sigmoid函数"></p><p>双曲正切函数（Hyperbolic Tangent, tanH）</p><p>效果比Sigmoid函数好，因为函数输出在(-1,1)之间，收敛速度更快</p><p>存在和Sigmoid函数一样的缺点：当t趋紧无穷，导数的梯度（即函数的斜率）就趋紧于 0，这使得梯度算法的速度会减慢。</p><script type="math/tex; mode=display">\begin{split} &tanh(t)=\dfrac{e^{t}-e^{-t}}{e^{t}+e^{-t}} \\\\&tanh'(t)=1-tanh^2(t)\end{split}</script><p><img src="https://source.cclmsy.cc/Posts/DL/Note/tanH函数.png" alt="tanH函数"></p><p>ReLU函数：修正线性单元（Rectified Linear Unit, ReLU）</p><p>当 $t&gt;0$ 时，梯度始终为1，从而提高神经网络基于梯度算法的运算速度，收敛速度远大于Sigmoid和tanH函数</p><script type="math/tex; mode=display">\begin{split}&f(t)=max(0,t) \\\\&f'(t)=\begin{cases}0 & \text{if } t<0 \\\\1 & \text{if } t \geq 0\end{cases}\end{split}</script><p><img src="https://source.cclmsy.cc/Posts/DL/Note/ReLU函数.png" alt="ReLU函数"></p><h3 id="2-7-损失函数（Loss-Function）与成本函数（Cost-Function）"><a href="#2-7-损失函数（Loss-Function）与成本函数（Cost-Function）" class="headerlink" title="2.7 损失函数（Loss Function）与成本函数（Cost Function）"></a>2.7 损失函数（Loss Function）与成本函数（Cost Function）</h3><p>损失函数用于衡量预测结果与真实值之间的误差。</p><p>平方差损失函数：$L(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^2$</p><ul><li>最简单的损失函数</li><li>具有多个局部最小值，不适合逻辑回归</li></ul><p>对数损失函数：$L(\hat{y},y)=-(y\log(\hat{y})+(1-y)\log(1-\hat{y}))$</p><ul><li>逻辑回归通常采用的损失函数</li><li>y=1时，损失函数为$-log(\hat{y})$，$\hat{y}$越大，损失越小</li><li>y=0时，损失函数为$log(1-\hat{y})$，$\hat{y}$越小，损失越小</li></ul><p>损失函数：衡量了在单个训练样本上的表现</p><p>成本函数（Cost Function）：$J(w,b)=\dfrac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})$</p><ul><li>所有训练样本的损失平均值</li><li>衡量在全体训练样本上的表现、参数w和b的效果</li></ul><h2 id="三、浅层神经网络"><a href="#三、浅层神经网络" class="headerlink" title="三、浅层神经网络"></a>三、浅层神经网络</h2><h3 id="3-1-浅层神经网络"><a href="#3-1-浅层神经网络" class="headerlink" title="3.1 浅层神经网络"></a>3.1 浅层神经网络</h3><p>神经网络（Neural Network, NN）是一种模拟人脑神经元工作方式的计算模型，包含输入层、隐藏层、输出层，每层包含多个神经元</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/浅层神经网络.png" alt="浅层神经网络"></p><p>以上图（单隐藏层神经网络）为例，输入层有$n=3$个特征，隐藏层一层，有$4$个神经元。记隐藏层 $[1]$ ，输出层 $[2]$ ，则：</p><ul><li>输入层：$x \in R^{3}$，$x$是一个$3$维的特征向量<ul><li>形状：(3, m)，$m$为样本数</li></ul></li><li>隐藏层具有4行3列的权重矩阵$W^{[1]} \in R^{4 \times 3}$，偏置向量$b^{[1]} \in R^{4}$<ul><li>隐藏层的每个神经元$i$具有权重$W^{[1]}<em>{i} \in R^{3}$，偏置$b^{[1]}</em>{i} \in R$</li><li>形状：输入(3, m) * 权重(4, 3) + 偏置(4, 1) = 输出(4, m)</li></ul></li><li>输出层具有1行4列的权重矩阵$W^{[2]} \in R^{1 \times 4}$，偏置$b^{[2]} \in R$<ul><li>形状：输入(4, m) * 权重(1, 4) + 偏置(1, 1) = 输出(1, m)、</li></ul></li></ul><p>总结：第i层的权重矩阵$W^{[i]}$的形状为$(n^{[i]}, n^{[i-1]})$，偏置$b^{[i]}$的形状为$(n^{[i]}, 1)$</p><h3 id="3-2-前向传播"><a href="#3-2-前向传播" class="headerlink" title="3.2 前向传播"></a>3.2 前向传播</h3><script type="math/tex; mode=display">\begin{split}&Z^{[1]} = W^{[1]}X+b^{[1]} \\\\&A^{[1]} = tanh(Z^{[1]}) \\\\&Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]} \\\\&A^{[2]} = \sigma(Z^{[2]})\end{split}</script><h3 id="3-3-反向传播"><a href="#3-3-反向传播" class="headerlink" title="3.3 反向传播"></a>3.3 反向传播</h3><p>输出层以Sigmoid函数作为激活函数，根据<a href="#14-逻辑回归的梯度下降">逻辑回归的梯度下降</a>的推导，可以得到：</p><script type="math/tex; mode=display">\begin{split}&dZ^{[2]} = A^{[2]}-Y \\\\&dW^{[2]} = \dfrac{1}{m}dZ^{[2]}A^{[1]T} \\\\&db^{[2]} = \dfrac{1}{m}np.sum(dZ^{[2]}, axis=1)\end{split}</script><p>隐藏层以tanh函数作为激活函数，已知：</p><script type="math/tex; mode=display">\begin{split}&tanh'(t) = 1-tanh^2(t) \\\\&Z^{[1]} = W^{[1]}X+b^{[1]} \\\\&A^{[1]} = tanh(Z^{[1]}) \\\\&J(A^{[2]},Y) = -Y\log(A^{[2]})-(1-Y)\log(1-A^{[2]})\end{split}</script><p>根据链式求导法则（步骤略），可以得到：</p><script type="math/tex; mode=display">\begin{split}&dZ^{[1]} = \frac{\partial J}{\partial A^{[2]}} \cdot \frac{\partial A^{[2]}}{\partial Z^{[2]}} \cdot \frac{\partial Z^{[2]}}{\partial A^{[1]}} \cdot \frac{\partial A^{[1]}}{\partial Z^{[1]}} = W^{[2]T}dZ^{[2]} \cdot (1-A^{[1]2}) \\\\&dW^{[1]} = \dfrac{1}{m}dZ^{[1]}X^T \\\\&db^{[1]} = \dfrac{1}{m}np.sum(dZ^{[1]}, axis=1)\end{split}</script><h3 id="实践：浅层神经网络实现"><a href="#实践：浅层神经网络实现" class="headerlink" title="实践：浅层神经网络实现"></a>实践：浅层神经网络实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">n = <span class="number">20</span>  <span class="comment"># 特征数</span></span><br><span class="line">m = <span class="number">100</span>  <span class="comment"># 样本数</span></span><br><span class="line">k = <span class="number">4</span>  <span class="comment"># 隐藏层神经元数</span></span><br><span class="line">alpha = <span class="number">0.001</span>  <span class="comment"># 学习率</span></span><br><span class="line">iterations = <span class="number">1000</span>  <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line">X = np.random.randn(n, m)  <span class="comment"># 生成特征矩阵</span></span><br><span class="line">Y = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, [<span class="number">1</span>, m])  <span class="comment"># 生成标签</span></span><br><span class="line"></span><br><span class="line">J = <span class="number">0</span>  <span class="comment"># 损失函数值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 隐藏层权重和偏置（随机初始化）</span></span><br><span class="line">W1 = np.random.randn(k, n) * <span class="number">0.01</span></span><br><span class="line">b1 = np.zeros([k, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出层权重和偏置</span></span><br><span class="line">W2 = np.random.randn(<span class="number">1</span>, k) * <span class="number">0.01</span></span><br><span class="line">b2 = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">t</span>): <span class="comment"># Sigmoid函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-t))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">t</span>): <span class="comment"># tanH函数</span></span><br><span class="line">    <span class="keyword">return</span> (np.exp(t) - np.exp(-t)) / (np.exp(t) + np.exp(-t))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_loss</span>(<span class="params">A, Y</span>):  <span class="comment"># 对数损失函数</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span> / m * np.<span class="built_in">sum</span>(Y * np.log(A) + (<span class="number">1</span> - Y) * np.log(<span class="number">1</span> - A))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_accuracy</span>(<span class="params">A, Y</span>):  <span class="comment"># 计算准确率</span></span><br><span class="line">    A = np.where(A &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> np.mean(A == Y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1 <span class="comment"># [k, m]</span></span><br><span class="line">    A1 = tanh(Z1)  <span class="comment"># 隐藏层激活值 [k, m]</span></span><br><span class="line">    Z2 = np.dot(W2, A1) + b2 <span class="comment"># [1, m]</span></span><br><span class="line">    A2 = sigmoid(Z2)  <span class="comment"># 输出层激活值（预测） [1, m]</span></span><br><span class="line">    J = log_loss(A2, Y)  <span class="comment"># 计算损失函数值 </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    <span class="comment"># 输出层</span></span><br><span class="line">    dZ2 = A2 - Y <span class="comment"># [1, m]</span></span><br><span class="line">    dW2 = <span class="number">1</span> / m * np.dot(dZ2, A1.T) <span class="comment"># [1, k]</span></span><br><span class="line">    db2 = <span class="number">1</span> / m * np.<span class="built_in">sum</span>(dZ2) <span class="comment"># [1, 1]</span></span><br><span class="line">    <span class="comment"># 隐藏层</span></span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) * (<span class="number">1</span> - A1 ** <span class="number">2</span>)</span><br><span class="line">    dW1 = <span class="number">1</span> / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span> / m * np.<span class="built_in">sum</span>(dZ1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新权重和偏置</span></span><br><span class="line">    W1 = W1 - alpha * dW1</span><br><span class="line">    b1 = b1 - alpha * db1</span><br><span class="line">    W2 = W2 - alpha * dW2</span><br><span class="line">    b2 = b2 - alpha * db2</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loss:&quot;</span>, J)  <span class="comment"># 打印损失函数值</span></span><br><span class="line"></span><br><span class="line"> 训练集上的准确率</span><br><span class="line">Z1 = np.dot(W1, X) + b1</span><br><span class="line">A1 = tanh(Z1)</span><br><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-Z2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span>, calc_accuracy(A2, Y))</span><br></pre></td></tr></table></figure><h2 id="四、深层神经网络"><a href="#四、深层神经网络" class="headerlink" title="四、深层神经网络"></a>四、深层神经网络</h2><h3 id="4-1-深层神经网络"><a href="#4-1-深层神经网络" class="headerlink" title="4.1 深层神经网络"></a>4.1 深层神经网络</h3><p>为什么需要深层神经网络：</p><ul><li>神经网络从第一层开始，从原始数据中提取特征</li><li>下一层将上一层习得的信息组合起来，形成更高级的特征</li><li>随着层数增多，特征从简单到复杂，学习的能力更强</li></ul><p><img src="https://source.cclmsy.cc/Posts/DL/Note/深层神经网络.jpeg" alt="深层神经网络"> </p><h3 id="4-2-前向传播"><a href="#4-2-前向传播" class="headerlink" title="4.2 前向传播"></a>4.2 前向传播</h3><script type="math/tex; mode=display">\begin{split}&x=a^{[0]} \\\\&z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]} \\\\&a^{[L]}=g^{[L]}(z^{[L]})\end{split}</script><p>输入$a^{[L-1]}$，输出$a^{[L]}$</p><h3 id="4-3-反向传播"><a href="#4-3-反向传播" class="headerlink" title="4.3 反向传播"></a>4.3 反向传播</h3><p><img src="https://source.cclmsy.cc/Posts/DL/Note/深层神经网络反向传播.png" alt="深层神经网络反向传播"></p><script type="math/tex; mode=display">\begin{split}&dZ^{[L]} = \frac{\partial J}{\partial A^{[L]}} \cdot \frac{\partial A^{[L]}}{\partial Z^{[L]}} = dA^{[L]} \cdot g^{[L]'}(Z^{[L]}) \\\\&dW^{[L]} = \frac{\partial J}{\partial Z^{[L]}} \cdot \frac{\partial Z^{[L]}}{\partial W^{[L]}} = \frac{1}{m}dZ^{[L]}A^{[L-1]T} \\\\&db^{[L]} = \frac{1}{m}np.sum(dZ^{[L]}, axis=1) \\\\&dA^{[L]} = W^{[L+1]T}dZ^{[L+1]}\end{split}</script><h3 id="4-4-参数和超参数"><a href="#4-4-参数和超参数" class="headerlink" title="4.4 参数和超参数"></a>4.4 参数和超参数</h3><p>参数（Parameters）：在训练过程中希望模型学习到的信息，模型自己调整的参数</p><ul><li>权重W通常使用随机初始化，避免对称性，$randn*0.01$<ul><li>对称性：如果所有的神经元都具有相同的权重，那么在反向传播过程中，所有的神经元都会学习到相同的特征</li><li>乘系数0.01：使用Sigmoid函数或者tanH函数作为激活函数时，W比较小，则Z=WX+b所得的值趋近于0，梯度较大，能够提高算法的更新速度；ReLU函数则没有这个问题</li></ul></li><li>偏置b没有对称性问题，通常初始化为0</li></ul><p>超参数（Hyper parameters）：通过人的经验判断、手动调整的网络信息，会影响最终的参数</p><ul><li>典型的超参数：学习速率$\alpha$、迭代次数$N$、隐藏层数$L$、每层神经元数$n_i$、激活函数$g^{[i]}()$的选择</li><li>开发新应用时，很难预先准确知道最佳的超参数，需要通过不同的尝试和调整来找到最佳的超参数</li></ul><h2 id="五、多分类与Softmax回归"><a href="#五、多分类与Softmax回归" class="headerlink" title="五、多分类与Softmax回归"></a>五、多分类与Softmax回归</h2><h3 id="5-1-Softmax回归"><a href="#5-1-Softmax回归" class="headerlink" title="5.1 Softmax回归"></a>5.1 Softmax回归</h3><p>对于多分类问题，种类个数C，则输出层的神经元个数必须为C，每个神经元的输出依次对应为每个类别的概率。</p><p><img src="https://source.cclmsy.cc/Posts/DL/Note/多分类问题.png" alt="多分类问题"></p><p>输出层：$Z^{[L]}=W^{[L]}A^{[L-1]}+b^{[L]}$$</p><p>Softmax公式：$a<em>i^{[L]} = \dfrac{e^{Z_i^{[L]}}}{\sum\limits</em>{j=1}^{C}e^{Z<em>j^{[L]}}}$，满足$\sum\limits</em>{i=1}^{C}a_i^{[L]}=1$</p><p>理解：$e^{z_i}$的占比</p><h3 id="5-2-交叉熵损失与One-hot编码"><a href="#5-2-交叉熵损失与One-hot编码" class="headerlink" title="5.2 交叉熵损失与One-hot编码"></a>5.2 交叉熵损失与One-hot编码</h3><p>对于Softmax回归，使用交叉熵损失（Cross Entropy Loss）函数：</p><script type="math/tex; mode=display">L(\hat{y},y)=-\sum\limits_{i=1}^{C}y_i\log(\hat{y}_i)</script><p>$C=2$时，即对应逻辑回归的对数损失函数$L(\hat{y},y)=-(y\log(\hat{y})+(1-y)\log(1-\hat{y}))$</p><p>one-hot编码（独热编码）：将标签转换为向量，只有一个元素为1，其他元素为0。</p><p>以图2.1.1为例，$y=7$，则one-hot编码对应为$y=[0,0,0,0,0,0,0,1,0,0]$。</p><p>由于除了正确类别外，其他类别$y_i=0$，因此可以简单计算交叉熵：$L(\hat{y},y)=-1\times\log(0.10)$</p><p>这一项的预测值越接近1，交叉熵越接近0，模型效果越好</p>]]></content>
    
    
    <summary type="html">深度学习基础、从浅层网络到深层网络</summary>
    
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://www.cclmsy.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://www.cclmsy.cc/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>ST表</title>
    <link href="https://www.cclmsy.cc/posts/sparse_table.html"/>
    <id>https://www.cclmsy.cc/posts/sparse_table.html</id>
    <published>2025-02-07T16:00:00.000Z</published>
    <updated>2025-03-11T02:54:43.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ST表"><a href="#ST表" class="headerlink" title="ST表"></a>ST表</h1><p>实现的功能：固定数组询问区间最值RMQ/区间GCD</p><p>原数组v下标从0开始；ST表下标从1开始，首元素为0</p><p>洛谷模板题：<a href="https://www.luogu.com.cn/problem/P3865">Luogu P3865</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> int long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> endl <span class="string">&#x27;\n&#x27;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ST表</span></span><br><span class="line"><span class="comment">// 实现的功能：固定数组 区间最值询问RMQ/区间GCD</span></span><br><span class="line"><span class="comment">// 下标从0开始</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">SparseTable</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    vector&lt;T&gt; a; <span class="comment">// 原数组</span></span><br><span class="line">    vector&lt;vector&lt;T&gt;&gt; st; <span class="comment">// st[i][j]表示[i,i+2^j-1]的最值</span></span><br><span class="line">    <span class="function">T <span class="title">op</span><span class="params">(<span class="type">const</span> T &amp;a, <span class="type">const</span> T &amp;b)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">max</span>(a, b); &#125; <span class="comment">// 可选：max,min,__gcd</span></span><br><span class="line">    <span class="built_in">SparseTable</span>(vector&lt;T&gt; _v)&#123;</span><br><span class="line">        a = _v;</span><br><span class="line">        n = a.<span class="built_in">size</span>();</span><br><span class="line">        st.<span class="built_in">resize</span>(n+<span class="number">5</span>);</span><br><span class="line">        <span class="type">int</span> len = <span class="built_in">log2</span>(n)+<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) st[i].<span class="built_in">resize</span>(<span class="number">21</span>, <span class="number">0</span>); <span class="comment">// n&lt;=2e6开21</span></span><br><span class="line">        <span class="built_in">solve</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) st[i][<span class="number">0</span>] = a[i];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; (<span class="number">1</span> &lt;&lt; j) &lt;= n; j++)&#123;</span><br><span class="line">            <span class="type">int</span> len = (<span class="number">1</span> &lt;&lt; j);</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i + len - <span class="number">1</span> &lt; n; i++)&#123;</span><br><span class="line">                st[i][j] = <span class="built_in">op</span>(st[i][j - <span class="number">1</span>], st[i + (<span class="number">1</span> &lt;&lt; (j - <span class="number">1</span>))][j - <span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">T <span class="title">query</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">        <span class="type">int</span> j = <span class="built_in">log2</span>(r - l + <span class="number">1</span>);<span class="comment">//已向下取整</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">op</span>(st[l][j], st[r - (<span class="number">1</span> &lt;&lt; j) + <span class="number">1</span>][j]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>); cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> n, q;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; q;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">v</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;i : v) cin &gt;&gt; i;</span><br><span class="line">    <span class="function">SparseTable&lt;<span class="type">int</span>&gt; <span class="title">st</span><span class="params">(v)</span></span>;</span><br><span class="line">    <span class="keyword">while</span> (q--)&#123;</span><br><span class="line">        <span class="type">int</span> l, r;</span><br><span class="line">        cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">        cout &lt;&lt; st.<span class="built_in">query</span>(l<span class="number">-1</span>, r<span class="number">-1</span>) &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">求区间最大/最小/GCD</summary>
    
    
    
    <category term="算法模板" scheme="https://www.cclmsy.cc/categories/%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF/"/>
    
    
    <category term="算法模板" scheme="https://www.cclmsy.cc/tags/%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF/"/>
    
    <category term="数据结构" scheme="https://www.cclmsy.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>《计算机网络与通信》课程笔记</title>
    <link href="https://www.cclmsy.cc/posts/computer_networks_and_communications.html"/>
    <id>https://www.cclmsy.cc/posts/computer_networks_and_communications.html</id>
    <published>2025-01-04T16:00:00.000Z</published>
    <updated>2025-01-04T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>因特网服务提供者 ISP(Internet Service Provider)</p><p>ARPANET：最早的因特网，1969年，使用分组交换技术，后来TCP/IP协议成为标准协议</p><p>互联网的组成：边缘（用户）+核心（路由器、链路）</p><p>两种通信方式：客户-服务器方式 C/S(Client/Server)、对等方式 P2P(Peer-to-Peer)</p><p>三种交换方式：电路交换、分组交换、报文交换</p><ul><li>电路交换：面向连接<ul><li>三阶段：建立连接、通信、释放连接</li></ul></li><li>分组交换<ul><li>可以无连接也可以面向连接</li><li>划分成较短的、固定长度的数据段</li><li>存储转发</li></ul></li><li>报文交换<ul><li>存储转发传输整个报文</li></ul></li></ul><p>时延=传输时延+处理时延+排队时延+传播时延</p><ul><li>发送时延：数据块长度/发送速率</li><li>处理时延：交换结点为存储转发而进行一些必要的处理所花费的时间</li><li>排队时延：结点缓存队列中分组排队所经历的时延</li><li>传播时延：信号在传输媒体上传播所需的时间（取决于信道距离）<ul><li>传输速率可以提高，但传播速率是物理特性决定的</li></ul></li></ul><p>时延带宽积：传播时延与带宽的乘积，链路上最大的比特数量</p><p>信道利用率：信道被利用的时间占比，非越高越好</p><p>网络利用率U：信道利用率加权平均值</p><p>$当前时延D=\dfrac{空闲时延D_0}{1-U}$</p><p>网络协议三要素：语法、语义、时序</p><p>实体(Entity)：任何可发送或接收信息的硬件或软件进程</p><p>协议(Protocol)：控制对等实体之间的通信的规则集</p><p>第n层协议向第n+1层提供服务，是第n-1层的用户</p><h3 id="OSI-TCP-IP"><a href="#OSI-TCP-IP" class="headerlink" title="OSI/TCP-IP"></a>OSI/TCP-IP</h3><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/1_1.png" alt="OSI和TCP/IP"></p><h2 id="二、物理层"><a href="#二、物理层" class="headerlink" title="二、物理层"></a>二、物理层</h2><p>主要任务：确定与传输媒体的接口的一些特性，包括：机械特性、电气特性、功能特性、过程特性</p><p>数据通信系统可划分为三大部分：源系统(发送端)、传输系统(传输网络)、目的系统(接收端)。</p><p>信道(channel)分类：单工通信（只能单向传输）、半双工通信（双向传输但不能同时）、全双工通信（双向传输且可以同时）</p><p>导引型传输媒体：双绞线、同轴电缆、光纤</p><p>非导引型传输媒体：无线电波、微波、红外线</p><p>宽带接入技术：非对称数字用户线ADSL(Asymmetric Digital Subscriber Line)、光纤同轴混合网HFC(Hybrid Fiber Coax)、光纤到…FTTx(Fiber To The x)</p><h3 id="2-1-调制"><a href="#2-1-调制" class="headerlink" title="2.1 调制"></a>2.1 调制</h3><p>基本带通调制方式：调幅AM、调频FM、调相PM</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/2_2.png" alt="基本带通调制方式"></p><p>一种多元制的振幅相位混合调制方法：正交振幅调制(Quadrature Amplitude Modulation，QAM)</p><ul><li>可供选择的相位有12种，而对于每一种相位有1或2种振幅可供选择。</li><li>共有16种不同的组合，每种组合可以用4bit二进制数表示，因此这16个点中的每个点可对应于一种4bit的编码。</li></ul><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/2_3.png" alt="正交振幅调制">、</p><h3 id="2-2-信道极限容量"><a href="#2-2-信道极限容量" class="headerlink" title="2.2 信道极限容量"></a>2.2 信道极限容量</h3><p>奈氏准则-奈奎斯特：在带宽为W(Hz)的低通信道中，若不考虑噪声影响，则码元传输的最高速率是2W(码元/秒 波特Baud)</p><p>信噪比(Signal-to-Noise Ratio, SNR)：信号功率与噪声功率之比</p><p>$SNR=10\log_{10}(\dfrac{S}{N})$(单位：dB)</p><p>香农定理：$C=W\log_2(1+\dfrac{S}{N})$(单位：bps)</p><p>W为信道的带宽(Hz)，S为信道内所传信号的平均功率(数据传输速率=码元传输速率*码元携带的比特数)，N为信道内部的高斯噪声功率</p><h4 id="信道极限容量例题"><a href="#信道极限容量例题" class="headerlink" title="信道极限容量例题"></a>信道极限容量例题</h4><ol><li>信道带宽为3000Hz，信噪比为30dB，则最大数据速率为多少？<ul><li>根据信噪比公式，$SNR=10log_{10}(\frac{S}{N})=30dB \Rightarrow \frac{S}{N}=10^3$</li><li>根据香农公式，$C=W\log_2(1+\frac{S}{N})=3000\log_2(1+10^3) \approx 3000\times9.97 \approx 30000bps$</li></ul></li><li>下列因素中,不会影响信道数据传输率的是( )<ul><li>A. 信噪比 B. 频率宽带 C. 调制速率 D. 信号传播速度</li><li>A：$\frac{S}{N}$, B：W, C：S的因子, D：不影响</li></ul></li><li>若信道在无噪声情况下的极限数据传输速率不小于信噪比为30dB条件下的极限数据传输速率，则信号状态数至少是？<ul><li>记信道带宽为W，一个码元所占位数k。</li><li>根据信噪比公式，$SNR=10log_{10}(\frac{S}{N})=30dB \Rightarrow \frac{S}{N}=10^3$</li><li>根据奈氏准则和香农公式：$2W\times k \ge W\log_2(1+10^3) \Rightarrow 2k \ge \log_2(1+10^3)\approx 10 \Rightarrow k \ge 5$</li><li>状态数至少是$2^5=32$</li></ul></li></ol><h3 id="2-3-信道复用技术"><a href="#2-3-信道复用技术" class="headerlink" title="2.3 信道复用技术"></a>2.3 信道复用技术</h3><ul><li>复用器(multiplexer)：复用开始前，多个低速信道合成一个高速信道的设备</li><li>分用器(de-multiplexer)：复用结束后，将高速信道分解为多个低速信道的设备</li></ul><p><strong>频分复用FDM</strong>(Frequency Division Multiplexing)：将频带分成若干窄带，各路信号分别搬移到适当的频率位置，彼此互不干扰。<br>频分复用的所有用户在同样的时间占用不同的带宽资源，这里的“带宽”是频率带宽而不是数据的发送速率。</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/2_4.png" alt="频分复用"></p><p><strong>时分复用TDM</strong>(Time Division Multiplexing)：将时间分成若干时隙，每个时隙用来传输一个信号，然后把这些时隙信号叠加在一起传输。<br>时分复用可能会造成线路资源的浪费，解决方法：统计时分复用STDM（Statistical TDM）</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/2_5.png" alt="时分复用"></p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/2_6.png" alt="时分复用可能会造成线路资源的浪费"></p><p><strong>波分复用WDM</strong>(Wavelength Division Multiplexing)：光的频分复用，使用一根光纤来同时传输多个光载波信号。</p><p><strong>码分多址CDMA</strong>(Code Division Multiple Access)：多个不同地址的用户共享码分复用(CDM, Code Division Multiplexing)信道。</p><p>码片(chip)：每一个比特时间划分为m个短的间隔，称为</p><p>码片序列：每个站被指派一个唯一的m bit码片序列</p><p>以$S=00011011, T=00101110$为例</p><ul><li>S发送1：发送码片序列$S=00011011$</li><li>S发送0：发送码片序列的反码$S_x=11100100$</li><li>按照惯例，1记为+1，0记为-1，码片序列S为-1 -1 -1 +1 +1 -1 +1 +1</li><li>码片特点：<ul><li>规格化内积$S\cdot S=\frac{1}{m}\sum_{i=1}^{m}S_iS_i=1$</li><li>规格化内积$S\cdot S<em>x=\frac{1}{m}\sum</em>{i=1}^{m}S<em>iS</em>{xi}=-1$</li></ul></li></ul><p>CDMA的重要特点</p><ul><li>每个站分配的码片序列不仅必须各不相同，并且还必须互相正交(规格化内积为0)<ul><li>S和T正交：$S\cdot T=\frac{1}{m}\sum_{i=1}^{m}S_iT_i=0$</li></ul></li><li>在实用的系统中是使用伪随机码序列</li></ul><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/2_7.png" alt="码分复用"></p><h2 id="三、数据链路层"><a href="#三、数据链路层" class="headerlink" title="三、数据链路层"></a>三、数据链路层</h2><p>数据链路(data link)：除了物理线路外，还必须有通信协议来控制这些数据的传输。<br>数据链路由物理链路和实现这些协议的硬件和软件构成。</p><p>数据链路层使用的信道：点对点信道、广播信道</p><p>数据链路层的三个基本问题：封装成帧、透明传输、差错检测</p><ol><li>封装成帧：在一段数据的前后分别添加首部和尾部，然后就构成了一个帧，确定帧的界限。</li><li>透明传输：使数据链路层的协议对上层是透明的，网络层不必关心数据链路层的具体实现。</li><li>差错检测：在传输过程中可能会产生比特差错，为了保证数据传输的可靠性，在计算机网络传输数据时，必须采用各种差错检测措施</li></ol><p>使用载波监听多点接入/碰撞检测协议CSMA/CD的以太网不能进行全双工通信而只能进行双向交替通信(半双工通信)</p><ul><li>多点接入CSMA：许多计算机以多点接入的方式连接在一根总线上</li><li>碰撞检测CD：每一个站在发送数据之前先要检测一下总线上是否有其他计算机在发送数据，如果有，则暂时不要发送数据，以免发生碰撞</li></ul><h3 id="3-1-循环冗余校验CRC"><a href="#3-1-循环冗余校验CRC" class="headerlink" title="3.1 循环冗余校验CRC"></a>3.1 循环冗余校验CRC</h3><p>循环冗余校验CRC(Cyclic Redundancy Check)是一种差错检测方法，通过在数据末端添加冗余位，确定数据是否在传输过程中发生变化。</p><p>模二运算：一种不考虑进位的二进制按位运算。</p><ul><li>加法/减法：相加减而不考虑进位/借位，即二进制按位异或运算</li><li>乘法：列竖式，逐位相乘，按位异或</li><li>除法：列竖式，商上余数的首位，按位相减</li></ul><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/3_2.png" alt="模二乘法"><br><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/3_3.png" alt="模二除法"></p><p>CRC冗余码的计算和校验：</p><ol><li>原始二进制数据$M=1100$</li><li>将原始数据左移$r=3$位（冗余位位数）得到$M\times 2^r=1100|000$</li><li>约定一个$r+1=4$位生成多项式$G=1011$作为除数<ul><li>生成多项式的最高位和最低位都必须是1</li><li>当传输的数据X的任何一位发生错误时，被G除的余数不为0</li><li>数据X的不同位发生错误时，被G除的余数不同</li></ul></li><li>模二除法：$(M \times 2^r) \div G = Q \cdots R$<ul><li>$1100000 \div 1011 = 1110 \cdots 010$</li><li>商$Q=1100$，余数$R=010$</li><li>此时有$M \times 2^r = Q \times G + R$ 成立</li></ul></li><li>将余数R添加到原始数据M后面，得到传输的数据$X=1100|010$<ul><li>$M \times 2^r + R = Q \times G + R + R = Q \times G$</li></ul></li><li>接收端将接收到的数据X对G进行模二除法，若余数为0则无差错，否则有差错</li></ol><p>差错定位/纠正：</p><div class="table-container"><table><thead><tr><th style="text-align:center">X=A1~A7</th><th style="text-align:center">余数</th><th style="text-align:center">出错位</th></tr></thead><tbody><tr><td style="text-align:center">1100010</td><td style="text-align:center">010</td><td style="text-align:center">无差错</td></tr><tr><td style="text-align:center">110001<strong>1</strong></td><td style="text-align:center">001</td><td style="text-align:center">7</td></tr><tr><td style="text-align:center">11000<strong>0</strong>0</td><td style="text-align:center">000</td><td style="text-align:center">6</td></tr><tr><td style="text-align:center">1100<strong>1</strong>10</td><td style="text-align:center">110</td><td style="text-align:center">5</td></tr><tr><td style="text-align:center">110<strong>1</strong>010</td><td style="text-align:center">011</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">11<strong>1</strong>0010</td><td style="text-align:center">110</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">1<strong>0</strong>0010</td><td style="text-align:center">111</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center"><strong>0</strong>10010</td><td style="text-align:center">101</td><td style="text-align:center">1</td></tr></tbody></table></div><h3 id="3-2-帧检验序列FCS"><a href="#3-2-帧检验序列FCS" class="headerlink" title="3.2 帧检验序列FCS"></a>3.2 帧检验序列FCS</h3><p>帧检验序列FCS(Frame Check Sequence)是数据链路层的差错检测技术，用于检测数据在传输过程中是否发生了变化。</p><p>CRC和FCS的关系：CRC是一种常用的差错检测方法，而FCS是添加在数据后面的冗余码。FCS可以用CRC这种方法得出，但CRC并非用来获得FCS的唯一方法。</p><p>无差错接受：接收端对收到的每一帧进行CRC检验，若余数为0则无差错，接受，否则丢弃</p><p>码距：同一编码中，任意两个合法编码之间不同二进数位数的最小值。</p><ul><li>码距越大，抗干扰能力、纠错能力越强，数据冗余越大，编码效率越低</li><li>检错位数=码距//2，纠错位数=(码距-1)//2</li><li>特殊情况：码距=3，检错1纠错1，或检错2纠错0</li></ul><p>校验码中增加冗余项的目的就是为了增大码距</p><p>仅用循环冗余检验CRC差错检测技术只能做到无差错接受，要做到“可靠传输”（即发送什么就收到什么）就必须再加上确认和重传机制。</p><ul><li>需要考虑的问题：帧重复、帧丢失、帧乱序</li><li>解决方法：帧编号、确认和重传</li></ul><p>OSI/RM模型的观点：数据链路层要做成无传输差错的。但这种理念目前不被接受</p><h3 id="3-2-点对点协议PPP"><a href="#3-2-点对点协议PPP" class="headerlink" title="3.2 点对点协议PPP"></a>3.2 点对点协议PPP</h3><p>PPP是现在全世界使用的最多的数据链路层协议，用户使用拨号电话线接入互联网时，一般都是使用PPP协议。</p><p>PPP协议的组成部分（自下而上）：</p><ol><li>一个将IP数据报封装到串行链路的方法。既支持异步链路，也支持同步链路</li><li>一个用来建立、配置和测试数据链路连接的链路控制协议LCP(Link Control Protocol)</li><li>一套网络控制协议NCP(Network Control Protocol)。允许在点到点连接上使用多种网络层协议</li></ol><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/3_5.png" alt="PPP协议的工作状态"></p><h4 id="PPP的帧格式"><a href="#PPP的帧格式" class="headerlink" title="PPP的帧格式"></a>PPP的帧格式</h4><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/3_4.png" alt="PPP的帧格式"></p><div class="table-container"><table><thead><tr><th style="text-align:center">字段</th><th style="text-align:center">长度</th><th style="text-align:center">值</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">帧开始标志F</td><td style="text-align:center">1B</td><td style="text-align:center">01111110(0x7E)</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">地址A</td><td style="text-align:center">1B</td><td style="text-align:center">11111111(0xFF)</td><td style="text-align:center">规定值，无作用</td></tr><tr><td style="text-align:center">控制C</td><td style="text-align:center">1B</td><td style="text-align:center">00000011(0x03)</td><td style="text-align:center">规定值，无作用</td></tr><tr><td style="text-align:center">协议P</td><td style="text-align:center">2B</td><td style="text-align:center">IP数据报：0x0021<br>PPP链路控制数据：0xC021<br>网络控制数据：0x8021</td><td style="text-align:center">表示上层协议类型</td></tr><tr><td style="text-align:center">信息字段</td><td style="text-align:center">0~1500B</td><td style="text-align:center">IP数据报等数据</td><td style="text-align:center">可变长度，不包括FCS</td></tr><tr><td style="text-align:center">帧校验序列FCS</td><td style="text-align:center">2B</td><td style="text-align:center">CRC校验码</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">帧结束标志F</td><td style="text-align:center">1B</td><td style="text-align:center">01111110(0x7E)</td></tr></tbody></table></div><h3 id="3-3-透明传输实现：字节填充和比特填充"><a href="#3-3-透明传输实现：字节填充和比特填充" class="headerlink" title="3.3 透明传输实现：字节填充和比特填充"></a>3.3 透明传输实现：字节填充和比特填充</h3><p>PPP异步传输（逐字符）：字节填充</p><ul><li>字节填充转义符：0x7D</li><li>0x7E转义为0x7D 0x5E</li><li>0x7D转义为0x7D 0x5D</li><li>ASCII码小于0x20,+0x20转义。如0x03转义为0x7D 0x23</li></ul><p>字节填充例题：一个PPP帧的信息字段为 7E 7D 5E FE 27 7D 5D 7D 27 7E，求真正的信息字段</p><ul><li>标出转义字符：76 (7D 5E) FE 27 (7D 5D) (7D 27) 7E</li><li>字符转义：76 (7E) FE 27 (7D) (07) 7E</li></ul><p>PPP同步传输（逐比特位连续发送）：比特填充</p><ul><li>在发送端，只要发现有5个连续的1，则立即填入一个0</li><li>在接收端，只要发现有5个连续的1，且后面紧跟一个0，则删除这个0</li></ul><p>比特填充例题：若接收端收到的 PPP 的数据部分是0001110111110111110110 ，问删除发送端加入的零比特后变成怎样的比特串</p><ul><li>标出111110：0001110(111110)(111110)110</li><li>删除零比特：0001110(11111)(11111)110</li></ul><h3 id="3-4-使用广播信道的数据链路层"><a href="#3-4-使用广播信道的数据链路层" class="headerlink" title="3.4 使用广播信道的数据链路层"></a>3.4 使用广播信道的数据链路层</h3><p>局域网拓扑结构</p><ul><li>总线网：所有站点共享一条总线。<ul><li>匹配电阻：避免总线有害电磁波反射</li></ul></li><li>星形网：所有站点都连接到一个中心站点。<ul><li>集线器：中心站点的功能是转发数据</li></ul></li><li>环形网：所有站点连接成一个环形。<ul><li>干线耦合器：环形网的数据传输方向是固定的（顺时针/逆时针）</li></ul></li></ul><p>媒体共享技术</p><ul><li>静态划分信道：TDM、FDM、WDM、CDM<ul><li>代价高，不适合局域网</li></ul></li><li>动态媒体接入控制（多点接入）<ul><li>随机接入：随机发送信息，要有冲突检测和冲突解决机制</li><li>受控接入：多点线路探询(polling)或轮询</li></ul></li></ul><p>局域网的数据链路层的两个子层：辑链路控制LLC(Logical Link Control)子层、媒体接入控制MAC(Media Access Control)子层</p><p>网卡的作用：进行串行/并行转换、<strong>实现以太网协议</strong>、存储MAC地址</p><h4 id="3-4-以太局域网（以太网）"><a href="#3-4-以太局域网（以太网）" class="headerlink" title="3.4 以太局域网（以太网）"></a>3.4 以太局域网（以太网）</h4><p>以太网单程端到端时延 $\tau$ ，争用期长度 $2\tau$ ，帧长 $L(bit)$ ，数据发送速率 $C(bps)$ ，帧发送时间 $T_0=\frac{L}{C}$</p><p>发送一帧的平均时间：$k\cdot 2\tau + T_0 + \tau$（发送完成后等待一个$\tau$的传播时间）</p><p>信道利用率：$S=\dfrac{T_0}{k\cdot 2\tau + T_0+\tau}$</p><p>要提高以太网的信道利用率，就必须减小$\tau$与$T_0$之比：$a=\frac{\tau}{T_0}$。<br>a越大，表明争用期所占的比例增大，每发生一次碰撞就浪费许多信道资源，使得信道利用率明显降低</p><p>理想情况下的极限信道利用率$S_{max}=\dfrac{T_0}{T_0+\tau}=\dfrac{1}{1+a}$</p><p>以太网MAC帧格式（DIX Ethernet V2 标准）：</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/3_6.png" alt="以太网MAC帧格式"></p><div class="table-container"><table><thead><tr><th style="text-align:center">字段</th><th style="text-align:center">长度</th><th style="text-align:center">值</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">前同步码</td><td style="text-align:center">7B</td><td style="text-align:center">固定为10101010交替</td><td style="text-align:center">调整时钟频率</td></tr><tr><td style="text-align:center">帧开始定界符</td><td style="text-align:center">1B</td><td style="text-align:center">10101011</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">目的地址</td><td style="text-align:center">6B</td><td style="text-align:center">目的MAC地址</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">源地址</td><td style="text-align:center">6B</td><td style="text-align:center">源MAC地址</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">类型</td><td style="text-align:center">2B</td><td style="text-align:center">上层协议类型</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">数据</td><td style="text-align:center">46~1500B</td><td style="text-align:center">IP数据报</td><td style="text-align:center">可变长度，不包括FCS</td></tr><tr><td style="text-align:center">FCS</td><td style="text-align:center">4B</td><td style="text-align:center">CRC校验码</td></tr></tbody></table></div><h3 id="3-5-HDLC协议"><a href="#3-5-HDLC协议" class="headerlink" title="3.5 HDLC协议"></a>3.5 HDLC协议</h3><p>HDLC(High-level Data Link Control)是一种数据链路层协议，是ISO组织制定的一种数据链路层协议，是一种面向比特的协议。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">HDLC协议</th><th style="text-align:center">PPP协议</th></tr></thead><tbody><tr><td style="text-align:center">信道</td><td style="text-align:center">点对点</td><td style="text-align:center">点对点</td></tr><tr><td style="text-align:center">面向</td><td style="text-align:center">比特</td><td style="text-align:center">字节</td></tr><tr><td style="text-align:center">透明性</td><td style="text-align:center">0比特填充</td><td style="text-align:center">字节填充</td></tr><tr><td style="text-align:center">通信</td><td style="text-align:center">全双工、半双工</td><td style="text-align:center">全双工</td></tr><tr><td style="text-align:center">可靠性</td><td style="text-align:center">使用编号和确认机制，提供可靠传输</td><td style="text-align:center">只保证无差错接收CRC检验</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">多一个2字节的协议字段。当协议字段值为0x0021时，表示信息字段是IP数据报</td></tr></tbody></table></div><h2 id="四、网络层"><a href="#四、网络层" class="headerlink" title="四、网络层"></a>四、网络层</h2><h3 id="4-1-网络层的基本概念"><a href="#4-1-网络层的基本概念" class="headerlink" title="4.1 网络层的基本概念"></a>4.1 网络层的基本概念</h3><h4 id="4-1-1-网络层提供的两种服务"><a href="#4-1-1-网络层提供的两种服务" class="headerlink" title="4.1.1 网络层提供的两种服务"></a>4.1.1 网络层提供的两种服务</h4><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">虚电路服务</th><th style="text-align:center">数据报服务</th></tr></thead><tbody><tr><td style="text-align:center">思路</td><td style="text-align:center">可靠通信应当由网络来保证</td><td style="text-align:center">通信的可靠性应当由用户主机来保证</td></tr><tr><td style="text-align:center">建立的连接</td><td style="text-align:center">必须有</td><td style="text-align:center">不需要</td></tr><tr><td style="text-align:center">终点地址</td><td style="text-align:center">仅在连接建立阶段使用，每个分组使用短的虚电路号</td><td style="text-align:center">每个分组都有终点的完整地址</td></tr><tr><td style="text-align:center">分组的转发</td><td style="text-align:center">属于同一条虚电路的分组均按照同一路由进行转发</td><td style="text-align:center">每个分组独立选择路由进行转发</td></tr><tr><td style="text-align:center">结点出故障时</td><td style="text-align:center">所有通过出故障的结点的虚电路均不能工作</td><td style="text-align:center">出故障的结点可能会丢失分组，一些路由可能会发生变化</td></tr><tr><td style="text-align:center">分组的顺序</td><td style="text-align:center">总是按发送顺序到达终点</td><td style="text-align:center">到达终点时不一定按发送顺序</td></tr><tr><td style="text-align:center">端到端的差错处理和流量控制</td><td style="text-align:center">可以由网络负责，也可以由用户主机负责</td><td style="text-align:center">由用户主机负责</td></tr></tbody></table></div><h4 id="4-1-2-中间设备-中继系统"><a href="#4-1-2-中间设备-中继系统" class="headerlink" title="4.1.2 中间设备/中继系统"></a>4.1.2 中间设备/中继系统</h4><ul><li>物理层：转发器(repeater)</li><li>数据链路层：网桥(bridge)</li><li>网络层：路由器(router)</li><li>网络层以上：网关(gateway)</li></ul><h3 id="4-2-IP协议"><a href="#4-2-IP协议" class="headerlink" title="4.2 IP协议"></a>4.2 IP协议</h3><p>网际协议IP是TCP/IP体系中两个最主要的协议之一。与IP协议配套使用的还有三个协议：</p><ul><li>地址解析协议ARP(Address Resolution Protocol)</li><li>网际控制报文协议ICMP(Internet Control Message Protocol)</li><li>网际组管理协议IGMP(Internet Group Management Protocol)</li></ul><h4 id="4-2-1-IP地址分类"><a href="#4-2-1-IP地址分类" class="headerlink" title="4.2.1 IP地址分类"></a>4.2.1 IP地址分类</h4><p>IP地址分为A、B、C、D、E五类，每类地址的网络号和主机号的划分不同。</p><div class="table-container"><table><thead><tr><th style="text-align:center">类别</th><th style="text-align:center">前缀</th><th style="text-align:center">网络号</th><th style="text-align:center">主机号</th><th style="text-align:center">第一字节范围</th><th style="text-align:center">子网掩码</th><th style="text-align:center">主机数</th><th style="text-align:center">预留私有地址网段（RFC 1918）</th></tr></thead><tbody><tr><td style="text-align:center">A类</td><td style="text-align:center">0</td><td style="text-align:center">7位</td><td style="text-align:center">24位</td><td style="text-align:center">0~127</td><td style="text-align:center">/8</td><td style="text-align:center">2^24-2=16777214</td><td style="text-align:center">10.0.0.0~10.255.255.255</td></tr><tr><td style="text-align:center">B类</td><td style="text-align:center">10</td><td style="text-align:center">14位</td><td style="text-align:center">16位</td><td style="text-align:center">128~191</td><td style="text-align:center">/16</td><td style="text-align:center">2^16-2=65534</td><td style="text-align:center">172.16.0.0~172.31.255.255</td></tr><tr><td style="text-align:center">C类</td><td style="text-align:center">110</td><td style="text-align:center">21位</td><td style="text-align:center">8位</td><td style="text-align:center">192~223</td><td style="text-align:center">/24</td><td style="text-align:center">2^8-2=254</td><td style="text-align:center">192.168.0.0~192.168.255.255</td></tr><tr><td style="text-align:center">D类</td><td style="text-align:center">1110</td><td style="text-align:center">多播地址28位</td><td style="text-align:center">-</td><td style="text-align:center">224~239</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">E类</td><td style="text-align:center">1111</td><td style="text-align:center">保留地址</td><td style="text-align:center">-</td><td style="text-align:center">240~255</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr></tbody></table></div><h3 id="4-2-2-地址解析协议ARP"><a href="#4-2-2-地址解析协议ARP" class="headerlink" title="4.2.2 地址解析协议ARP"></a>4.2.2 地址解析协议ARP</h3><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/4_1.png" alt="ARP工作原理"></p><p>当主机A欲向本局域网上的某个主机B发送IP数据报时，就先在其ARP高速缓存中查看有无主机B的IP地址。<br>如有，就可查出其对应的硬件地址，再将此硬件地址写入MAC，然后通过局域网将该MAC发往此硬件地址。<br>如没有，ARP进程在本局域网上广播发送一个ARP请求分组收到ARP响应分组后，将得到的P地址到硬件地址的映射写入ARP高速缓存。</p><h3 id="4-2-3-IP数据报的封装格式"><a href="#4-2-3-IP数据报的封装格式" class="headerlink" title="4.2.3 IP数据报的封装格式"></a>4.2.3 IP数据报的封装格式</h3><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/4_2.png" alt="IP数据报的封装格式"></p><p>IP数据报的封装格式：</p><div class="table-container"><table><thead><tr><th style="text-align:center">字段</th><th style="text-align:center">长度</th><th style="text-align:left">值</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:center">版本</td><td style="text-align:center">4b</td><td style="text-align:left">IPv4为4</td><td style="text-align:left">IP协议的版本号</td></tr><tr><td style="text-align:center">首部长度</td><td style="text-align:center">4b</td><td style="text-align:left">首部长度</td><td style="text-align:left">最大为1111(15)，单位为4字节，即首部最大为60字节</td></tr><tr><td style="text-align:center">区分服务</td><td style="text-align:center">8b</td><td style="text-align:left"></td><td style="text-align:left">服务类型，一般不使用</td></tr><tr><td style="text-align:center">总长度</td><td style="text-align:center">16b</td><td style="text-align:left">IP数据报的总长度</td><td style="text-align:left">单位为字节，包括首部和数据部分</td></tr><tr><td style="text-align:center">标识</td><td style="text-align:center">16b</td><td style="text-align:left">标识符</td><td style="text-align:left">用于标识发送的数据报</td></tr><tr><td style="text-align:center">标志</td><td style="text-align:center">3b</td><td style="text-align:left">标志位</td><td style="text-align:left">MF：1位，1表示还有分片<br>DF：1位，1表示不允许分片<br>保留位：1位</td></tr><tr><td style="text-align:center">片偏移</td><td style="text-align:center">13b</td><td style="text-align:left">片偏移</td><td style="text-align:left">单位为8字节，表示该片相对于原始数据报的偏移量</td></tr><tr><td style="text-align:center">生存时间</td><td style="text-align:center">8b</td><td style="text-align:left">TTL</td><td style="text-align:left">数据报在网络中可通过的路由器数的最大值，每经过一个路由器减1，为0时丢弃</td></tr><tr><td style="text-align:center">协议</td><td style="text-align:center">8b</td><td style="text-align:left">协议号</td><td style="text-align:left">指出此数据报携带的数据使用何种协议，如TCP为6，UDP为17</td></tr><tr><td style="text-align:center">首部校验和</td><td style="text-align:center">16b</td><td style="text-align:left">首部校验和</td><td style="text-align:left">只检验数据报的首部不检验数据部分<br>这里不采用CRC检验码而采用简单的计算方法</td></tr><tr><td style="text-align:center">源地址</td><td style="text-align:center">32b</td><td style="text-align:left">源IP地址</td><td style="text-align:left">发送数据报的主机的IP地址</td></tr><tr><td style="text-align:center">目的地址</td><td style="text-align:center">32b</td><td style="text-align:left">目的IP地址</td><td style="text-align:left">接收数据报的主机的IP地址</td></tr><tr><td style="text-align:center">选项</td><td style="text-align:center">可变</td><td style="text-align:left"></td><td style="text-align:left">选项字段的长度是可变的，最长为40字节</td></tr></tbody></table></div><p>IP数据报分片例：一数据报的总长度为3820字节，其数据部分的长度为3800字节(使用固定首部)，MTU=1420字节</p><ul><li>首部长度：20字节</li><li>分片最长数据长度：1420-20=1400字节</li><li>分片数据长度：3800=1400+1400+1000</li><li>分片1：偏移0，MF=1</li><li>分片2：偏移1400/8=175，MF=1</li><li>分片3：偏移2800/8=350，MF=0</li></ul><h3 id="4-3-IP层分组转发过程"><a href="#4-3-IP层分组转发过程" class="headerlink" title="4.3 IP层分组转发过程"></a>4.3 IP层分组转发过程</h3><h4 id="4-3-1-分组转发算法"><a href="#4-3-1-分组转发算法" class="headerlink" title="4.3.1 分组转发算法"></a>4.3.1 分组转发算法</h4><ol><li>从数据报的首部提取目的主机D的IP地址,得出目的网络地址为N</li><li>若网络N与此路由器直接相连，则把数据报直接交付目的主机D；否则是间接交付，执行下一步</li><li>若路由表中有目的地址为D的特定主机路由，则把数据报传送给路由表中所指明的下一跳路由器；否则，执行下一步</li><li>若路由表中有到达网络 N 的路由，则把数据报传送给路由表指明的下一跳路由器；否则，执行下一步</li><li>若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器，否则，报告转发分组出错</li></ol><h4 id="4-3-2-子网划分"><a href="#4-3-2-子网划分" class="headerlink" title="4.3.2 子网划分"></a>4.3.2 子网划分</h4><p>借位：从主机最高位开始借位变为新的子网位，剩余部分仍为主机位</p><p>例如192.168.1.0/24可划分为：</p><ul><li>2个子网：192.168.1.0/25、192.168.1.128/25</li><li>4个子网：192.168.1.0/26、192.168.1.64/26、192.168.1.128/26、192.168.1.192/26</li></ul><p>192.168.1.55/26 的网络号：将IP地址转换为二进制，取前26位即可。<br>192.168.1.55(00|110111)=&gt;192.168.1.0</p><h4 id="4-3-3-划分子网的分组转发算法"><a href="#4-3-3-划分子网的分组转发算法" class="headerlink" title="4.3.3 划分子网的分组转发算法"></a>4.3.3 划分子网的分组转发算法</h4><ol><li>从数据报的首部提取目的主机D的IP地址</li><li>先用直接相连的各网络的子网掩码和D逐位相“与”，看是否和相应的网络地址匹配。若匹配，则将分组直接交付；否则就是间接交付，执行下一步</li><li>若路由表中有目的地址为D的特定主机路由，则将分组传送给指明的下一跳路由器；否则，执行下一步</li><li>对路由表中的每一行的子网掩码和 D 逐位相“与”，若其结果与该行的目的网络地址匹配，则将分组传送给该行指明的下一跳路由器；否则，执行下一步</li><li>若路由表中有一个默认路由，则将分组传送给指明的默认路由器；否则，报告转发分组出错</li></ol><h4 id="4-3-4-无分类IP地址CIDR"><a href="#4-3-4-无分类IP地址CIDR" class="headerlink" title="4.3.4 无分类IP地址CIDR"></a>4.3.4 无分类IP地址CIDR</h4><p>一个 CIDR 地址块可以表示很多地址，这种地址的聚合常称为路由聚合，它使得路由表中的一个项目可以表示很多个（例如上千个）原来传统分类地址的路由。</p><p>最长前缀匹配：在路由表中查找一个与目的地址最匹配的路由表项，即查找最长的前缀匹配。</p><h3 id="4-4-网际控制报文协议ICMP"><a href="#4-4-网际控制报文协议ICMP" class="headerlink" title="4.4 网际控制报文协议ICMP"></a>4.4 网际控制报文协议ICMP</h3><p>ICMP 允许主机或路由器报告差错情况和提供有关异常情况的报告。<br>ICMP 不是高层协议，而是IP层的协议。<br>ICMP 报文作为IP层数据报的数据，加上数据报的首部，组成IP数据报发送出去。</p><p>ICMP差错报文：终点不可达、时间超限、参数问题、改变路由(重定向)、源抑制</p><p>ICMP询问报文：回送请求和回答报文、时间戳请求和回答报文</p><p>ICMP应用：PING、Traceroute</p><h3 id="4-5-路由选择协议"><a href="#4-5-路由选择协议" class="headerlink" title="4.5 路由选择协议"></a>4.5 路由选择协议</h3><p>内部网关协议 IGP:具体的协议有多种，如RIP和OSPF等</p><p>外部网关协议 EGP:目前使用的协议就是BGP</p><h4 id="4-5-1-RIP协议"><a href="#4-5-1-RIP协议" class="headerlink" title="4.5.1 RIP协议"></a>4.5.1 RIP协议</h4><p>RIP是一种基于距离矢量（Distance-Vector）算法的协议，它使用跳数（Hop Count）作为度量值来衡量到达目的地址的距离。</p><p>在RIP网络中，缺省情况下，设备到与它直接相连网络的跳数为0，通过一个设备可达的网络的跳数为1，其余依此类推。也就是说，度量值等于从本网络到达目的网络间的设备数量。</p><p>为限制收敛时间，RIP规定度量值取0～15之间的整数，大于或等于16的跳数被定义为无穷大，即目的网络或主机不可达。由于这个限制，使得RIP不可能在大型网络中得到应用。</p><p>RIP路由器工作流程：</p><ul><li>RIP路由器A,B初始的路由表中只有自己的直连路由。</li><li>每30秒，向相邻路由器发送自己的路由表。</li><li>收到相邻路由器的路由表后，更新自己的路由表：<ul><li>新增：如果收到的路由表中有自己没有的路由，则添加到自己的路由表中。</li><li>更新：如果收到的路由表中有自己已有的路由，且新的跳数更小，则更新自己的路由表的跳数。</li></ul></li><li>RIP计时器：<ul><li>更新计时器：每30秒左右发送一次路由表。</li><li>失效计时器：180秒（6倍更新时间）未更新，标记为不可达。</li><li>刷新计时器：无效路由240秒未更新，从路由表中删除。</li></ul></li></ul><p>RIP协议的缺点：坏消息传播慢、收敛时间长、不适合大型网络</p><h4 id="4-5-2-OSPF协议"><a href="#4-5-2-OSPF协议" class="headerlink" title="4.5.2 OSPF协议"></a>4.5.2 OSPF协议</h4><p>使用了 Dijkstra 提出的最短路径算法SPF(Shortest Path First)，是分布式的链路状态协议</p><p>支持可变长度的子网划分和无分类编址 CIDR</p><p>五种分组类型：问候分组Hello、数据库描述分组Database Description、链路状态请求分组Link State Request、链路状态更新分组Link State Update、链路状态确认分组Link State Acknowledgement</p><p>当互联网规模很大时，OSPF 协议要比距离向量协议 RIP 好得多，且没有“坏消息传播得慢”的问题</p><h4 id="4-5-3-BGP协议"><a href="#4-5-3-BGP协议" class="headerlink" title="4.5.3 BGP协议"></a>4.5.3 BGP协议</h4><p>BGP 只能是力求寻找一条能够到达目的网络且比较好的路由（不能兜圈子），而并非要寻找一条最佳路由</p><p>一个BGP 发言人与其他自治系统中的BGP发言人要交换路由信息就要先建立TCP连接，然后在此连接上交换BGP报文以建立BGP会话(session)，利用 BGP 会话交换路由信息。</p><p>BGP协议的特点：支持CIDR、只在发生变化时更新有变化的部分</p><p>四种BGP报文：打开报文Open（与相邻BGP发言人建立联系）、更新报文Update（发送路由信息）、通知报文Notification（发送差错）、保持报文Keepalive（确认Open和保持连接）</p><h3 id="4-6-路由器的构成"><a href="#4-6-路由器的构成" class="headerlink" title="4.6 路由器的构成"></a>4.6 路由器的构成</h3><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/4_3.png" alt="路由器的结构"></p><p>常用交换方法：通过存储器、通过总线、通过纵横交换结构</p><h4 id="4-7-IP多播"><a href="#4-7-IP多播" class="headerlink" title="4.7 IP多播"></a>4.7 IP多播</h4><p>TCP/IP 协议使用的以太网多播地址块的范围是：从 00-00-5E-00-00-00 到 00-00-5E-FF-FF-FF</p><p>为了使路由器知道多播组成员的信息，需要利用网际组管理协议IGMP (Internet Group Management Protocol)，同时也需要路由器多播选择协议</p><p>IGMP阶段：加入多播组、探寻成员变化</p><p>多播路由选择协议转发多播数据报时使用三种方法：洪泛与剪除（适合于较小的多播组，RPB）、隧道技术、基于核心的发现技术</p><p>反向路径广播RPB(Reverse Path Broadcasting)：路由器收到多播数据报时，检查数据报的源地址，如果数据报的源地址是最短路径树上的成员，则转发数据报，否则丢弃</p><h2 id="五、传输层-运输层"><a href="#五、传输层-运输层" class="headerlink" title="五、传输层/运输层"></a>五、传输层/运输层</h2><p>运输层向它上面的应用层提供通信服务</p><p>只有位于网络边缘部分的主机的协议栈才有运输层，而网络核心部分中的路由器在转发分组时都只用到下三层的功能</p><p>应用进程之间的通信又称为端到端的通信，运输层提供应用进程间的逻辑通信</p><div class="table-container"><table><thead><tr><th style="text-align:center">运输层</th><th style="text-align:center">网络层</th></tr></thead><tbody><tr><td style="text-align:center">为应用进程之间提供端到端的逻辑通信</td><td style="text-align:center">为主机之间提供逻辑通信</td></tr><tr><td style="text-align:center">对报文进行差错检测</td><td style="text-align:center">对首部进行差错检测</td></tr><tr><td style="text-align:center">需要两种不同的运输协议，即面向连接的TCP和无连接的UDP</td><td style="text-align:center">无连接的IP协议</td></tr></tbody></table></div><p>协议端口号：用于标识主机上的应用进程，简称端口。16位，范围0~65535</p><ul><li>服务端：0~1023为熟知端口号，1024~49151为登记端口号</li><li>客户端：49152(65536/4*3)~65535为短暂端口号</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">协议</th><th style="text-align:center">FTP</th><th style="text-align:center">Telnet</th><th style="text-align:center">SMTP</th><th style="text-align:center">DNS</th><th style="text-align:center">HTTP</th><th style="text-align:center">SMTP</th><th style="text-align:center">HTTPS</th></tr></thead><tbody><tr><td style="text-align:center">熟知端口号</td><td style="text-align:center">21/20</td><td style="text-align:center">23</td><td style="text-align:center">25</td><td style="text-align:center">53</td><td style="text-align:center">80</td><td style="text-align:center">110</td><td style="text-align:center">443</td></tr></tbody></table></div><h3 id="5-1-用户数据报协议UDP-v-s-传输控制协议TCP"><a href="#5-1-用户数据报协议UDP-v-s-传输控制协议TCP" class="headerlink" title="5.1 用户数据报协议UDP v.s. 传输控制协议TCP"></a>5.1 用户数据报协议UDP v.s. 传输控制协议TCP</h3><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">用户数据报协议UDP(User Datagram Protocol)</th><th style="text-align:center">传输控制协议TCP(Transmission Control Protocol)</th></tr></thead><tbody><tr><td style="text-align:center">连接服务</td><td style="text-align:center">无连接的协议，提供无连接服务</td><td style="text-align:center">面向连接的协议，提供面向连接服务</td></tr><tr><td style="text-align:center">可靠性</td><td style="text-align:center">不可靠</td><td style="text-align:center">全双工的可靠信道</td></tr><tr><td style="text-align:center">运输协议数据单元TPDU</td><td style="text-align:center">UDP报文/用户数据报</td><td style="text-align:center">TCP报文段</td></tr><tr><td style="text-align:center">传输方式</td><td style="text-align:center">单播、多播、广播</td><td style="text-align:center">点对点单播，不支持多播和广播</td></tr><tr><td style="text-align:center">面向</td><td style="text-align:center">报文</td><td style="text-align:center">字节流</td></tr><tr><td style="text-align:center">复杂性</td><td style="text-align:center">简单</td><td style="text-align:center">复杂</td></tr><tr><td style="text-align:center">应用</td><td style="text-align:center">多媒体</td><td style="text-align:center">文件传输、电子邮件、WWW</td></tr><tr><td style="text-align:center">应用层协议</td><td style="text-align:center">DNS、TFTP、SNMP、NFS</td><td style="text-align:center">HTTP、FTP、Telnet、SMTP</td></tr><tr><td style="text-align:center">拥塞控制</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">首部开销</td><td style="text-align:center">8字节</td><td style="text-align:center">20字节</td></tr></tbody></table></div><h3 id="5-2-用户数据报协议UDP"><a href="#5-2-用户数据报协议UDP" class="headerlink" title="5.2 用户数据报协议UDP"></a>5.2 用户数据报协议UDP</h3><p>只在IP数据报之上增加了复用和分用（端口）、差错检测功能</p><h4 id="5-2-1-UDP首部格式"><a href="#5-2-1-UDP首部格式" class="headerlink" title="5.2.1 UDP首部格式"></a>5.2.1 UDP首部格式</h4><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_1.png" alt="UDP首部格式"></p><div class="table-container"><table><thead><tr><th style="text-align:center">字段</th><th style="text-align:center">长度</th><th style="text-align:center">值</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">伪首部</td><td style="text-align:center">12B</td><td style="text-align:center">源IP地址、目的IP地址、协议号、UDP长度</td><td style="text-align:center">用于计算校验和</td></tr><tr><td style="text-align:center">源端口</td><td style="text-align:center">2B</td><td style="text-align:center">0~65535</td><td style="text-align:center">标识发送方应用进程</td></tr><tr><td style="text-align:center">目的端口</td><td style="text-align:center">2B</td><td style="text-align:center">0~65535</td><td style="text-align:center">标识接收方应用进程</td></tr><tr><td style="text-align:center">长度</td><td style="text-align:center">2B</td><td style="text-align:center">8~65535</td><td style="text-align:center">UDP报文长度</td></tr><tr><td style="text-align:center">校验和</td><td style="text-align:center">2B</td><td style="text-align:center">0~65535</td><td style="text-align:center">校验UDP首部和数据</td></tr></tbody></table></div><h4 id="5-2-2-UDP校验和"><a href="#5-2-2-UDP校验和" class="headerlink" title="5.2.2 UDP校验和"></a>5.2.2 UDP校验和</h4><p>要加上12字节的伪首部，包含：源IP地址、目的IP地址、协议号、UDP长度</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_2.png" alt="校验和计算"></p><ol><li>二进制反码求和：<ol><li>统计出每一位上1的个数：24135445 04247779</li><li>从末尾开始，模二进位；首位进位加到末位<ul><li>24135445 042477(7+9/2=11)1</li><li>24135445 04247(7+11/2=12)11</li><li>24135445 0424(7+12/2=13)011</li><li>…</li><li>(2+6/2=5)0010110 11001011</li><li>10010110 1100101(1+5/2=3)</li><li>10010110 110010(1+3/2=2)1</li><li>10010110 11001(0+2/2=1)01</li><li>10010110 11001101</li></ul></li></ol></li><li>取反码：01101001 00110010，即为校验和</li></ol><h3 id="5-3-传输控制协议TCP"><a href="#5-3-传输控制协议TCP" class="headerlink" title="5.3 传输控制协议TCP"></a>5.3 传输控制协议TCP</h3><p>套接字(socket)：IP地址:端口号</p><p>每一条 TCP 连接唯一地被通信两端的两个端点（即两个套接字）确定。</p><h4 id="5-3-1-TCP首部格式"><a href="#5-3-1-TCP首部格式" class="headerlink" title="5.3.1 TCP首部格式"></a>5.3.1 TCP首部格式</h4><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_4.png" alt="TCP首部格式"></p><div class="table-container"><table><thead><tr><th style="text-align:center">字段</th><th style="text-align:center">长度</th><th style="text-align:center">值</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">源端口</td><td style="text-align:center">2B</td><td style="text-align:center">0~65535</td><td style="text-align:center">标识发送方应用进程</td></tr><tr><td style="text-align:center">目的端口</td><td style="text-align:center">2B</td><td style="text-align:center">0~65535</td><td style="text-align:center">标识接收方应用进程</td></tr><tr><td style="text-align:center">序号</td><td style="text-align:center">4B</td><td style="text-align:center">0~4294967295</td><td style="text-align:center">每个字节都被编号，这是本报文段的第一个字节的序号</td></tr><tr><td style="text-align:center">确认号ack</td><td style="text-align:center">4B</td><td style="text-align:center">0~4294967295</td><td style="text-align:center">期望收到对方的下一个报文段的数据的第一个字节的序号，=序号+数据长度</td></tr><tr><td style="text-align:center">数据偏移</td><td style="text-align:center">4b</td><td style="text-align:center">0~15</td><td style="text-align:center">TCP首部长度，单位为4字节，最大为60字节</td></tr><tr><td style="text-align:center">保留</td><td style="text-align:center">6b</td><td style="text-align:center">0</td><td style="text-align:center">保留位</td></tr><tr><td style="text-align:center">控制位</td><td style="text-align:center">6b</td><td style="text-align:center">URG、ACK、PSH、RST、SYN、FIN</td><td style="text-align:center">URG：紧急数据优先传送<br>ACK：确认号字段有效<br>PSH：接收方尽快将累积数据交给应用层<br>RST：连接差错，重置连接<br>SYN：连接请求/接受<br>FIN：发送完毕，释放连接</td></tr><tr><td style="text-align:center">窗口rwnd</td><td style="text-align:center">16b</td><td style="text-align:center">0~65535</td><td style="text-align:center">接收窗口大小，单位为字节</td></tr><tr><td style="text-align:center">校验和</td><td style="text-align:center">16b</td><td style="text-align:center">0~65535</td><td style="text-align:center">首部和数据的校验和</td></tr><tr><td style="text-align:center">紧急指针</td><td style="text-align:center">16b</td><td style="text-align:center">0~65535</td><td style="text-align:center">紧急数据的长度（字节），紧急数据最后一位=序号+紧急指针-1</td></tr><tr><td style="text-align:center">选项</td><td style="text-align:center">可变</td><td style="text-align:center"></td><td style="text-align:center">最大报文段长度MSS、窗口扩大选项（3字节）、时间戳选项（10字节）、选择确认选项</td></tr><tr><td style="text-align:center">填充</td><td style="text-align:center">可变</td><td style="text-align:center"></td><td style="text-align:center">使首部长度为4的倍数</td></tr></tbody></table></div><h4 id="5-3-2-TCP可靠传输工作原理"><a href="#5-3-2-TCP可靠传输工作原理" class="headerlink" title="5.3.2 TCP可靠传输工作原理"></a>5.3.2 TCP可靠传输工作原理</h4><p>自动重传请求ARQ(Automatic Repeat reQuest)：使用确认和重传机制，我们就可以在不可靠的传输网络上实现可靠的通信。<br>重传是自动进行的，不需要接收方发送重传请求。</p><p>TCP连接的每一端都必须设有两个窗口：一个发送窗口和一个接收窗口。TCP 两端的四个窗口经常处于动态变化之中。<br>TCP 的可靠传输机制用字节的序号进行控制，TCP所有的确认都是基于序号而不是基于报文段。<br>TCP连接的往返时间RTT也不是固定不变的，需要使用特定的算法估算较为合理的重传时间。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">停止等待协议</th><th style="text-align:center">连续ARQ协议</th></tr></thead><tbody><tr><td style="text-align:center">发送分组数量</td><td style="text-align:center">1</td><td style="text-align:center">多个</td></tr><tr><td style="text-align:center">控制传输</td><td style="text-align:center">停止-等待</td><td style="text-align:center">滑动窗口协议</td></tr><tr><td style="text-align:center">确认方式</td><td style="text-align:center">单独确认</td><td style="text-align:center">单独确认+累积确认</td></tr><tr><td style="text-align:center">超时定时</td><td style="text-align:center">每个发送的分组</td><td style="text-align:center">每个发送的分组</td></tr><tr><td style="text-align:center">分组编号</td><td style="text-align:center">每个分组</td><td style="text-align:center">每个分组</td></tr><tr><td style="text-align:center">重传机制</td><td style="text-align:center">一个分组</td><td style="text-align:center">回退N</td></tr></tbody></table></div><h4 id="5-3-3-停止等待协议"><a href="#5-3-3-停止等待协议" class="headerlink" title="5.3.3 停止等待协议"></a>5.3.3 停止等待协议</h4><p>停止等待协议是一种简单的ARQ协议。<br>在发送完一个分组后，必须暂时保留己发送的分组的副本。<br>分组和确认分组都必须进行编号。</p><p>优点：简单、易于实现</p><p>缺点：信道利用率低 $U=\dfrac{数据发送时间T_D}{T_D+往返时间RTT+ACK读取时间T_A}$</p><ol><li>无差错<ul><li>A发送分组1，启动定时器，等待B确认（ACK）</li><li>B收到分组1，发送ACK</li><li>A收到ACK，发送分组2</li></ul></li><li>分组差错<ul><li>A发送分组1，启动定时器，等待B确认</li><li>分组1损坏丢弃/中途丢失</li><li>A超时未收到ACK，重发分组1</li><li>B收到分组1，发送ACK</li></ul></li><li>确认差错/迟到<ul><li>A发送分组1，启动定时器，等待B确认</li><li>B收到分组1，发送ACK</li><li>ACK中途丢失</li><li>A超时未收到ACK，重发分组1</li><li>B收到分组1，重复，丢弃，重发ACK</li><li>A收到重发的ACK，发送分组2</li></ul></li></ol><h4 id="5-3-4-连续ARQ协议"><a href="#5-3-4-连续ARQ协议" class="headerlink" title="5.3.4 连续ARQ协议"></a>5.3.4 连续ARQ协议</h4><p>连续ARQ协议是一种流水线协议，允许发送方发送多个分组而不必等待接收方的确认。</p><p>确认方式：接收方累积确认：不必对收到的分组逐个发送确认，而是对按序到达的最后一个分组发送确认，表示：到这个分组为止的所有分组都已正确收到了。</p><p>累积确认的优点是：容易实现，即使确认丢失也不必重传。</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_3.png" alt="累积确认"></p><p>重传方式：回退N：5个分组，第3个出错，接收方只对前2个分组发送确认，发送方收到确认后，重传第3个分组及后面的分组。</p><h3 id="5-3-5-TCP可靠通信具体实现"><a href="#5-3-5-TCP可靠通信具体实现" class="headerlink" title="5.3.5 TCP可靠通信具体实现"></a>5.3.5 TCP可靠通信具体实现</h3><ul><li>假定A向B发送数据，当前A收到了B发来的确认报文，其中窗口=20，确认号=31（表示期望收到A的下一个报文的序号为31，且30以前的报文都已收到）。</li><li>此时A的发送窗口[31,50]，B的接收窗口[31,50]。A发送31~41</li><li>假设B收到31~33，发送确认号34，窗口=20的ACK，同时B的接收窗口滑动至[34,53]。</li><li>A收到ACK，窗口滑动至[34,53]。</li><li>未收到的数据在超时后重传。</li></ul><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_5.png" alt="TCP可靠通信具体实现"></p><ul><li>发送缓存用于存储：已发送但未收到确认的数据、准备发送的数据</li><li>接收缓存用于存储：已接收但未被使用的数据、未按序到达的数据</li></ul><h4 id="5-3-6-超时重传时间RTO的选择"><a href="#5-3-6-超时重传时间RTO的选择" class="headerlink" title="5.3.6 超时重传时间RTO的选择"></a>5.3.6 超时重传时间RTO的选择</h4><ul><li>报文往返时间RTT：从发送方发送数据报到接收方收到确认报文的时间</li><li>加权移动平均往返时间RTTs：当监测到一个新的RTT时，计算新的RTTs<ul><li>$RTT_S=(1-\alpha)RTT_S+\alpha RTT$，其中$\alpha$一般取$\dfrac{1}{8}$</li></ul></li><li>平均偏差RTTd：初值为第一次RTT的一半，之后每次RTT的偏差为<ul><li>$RTT_D=(1-\beta)RTT_D+\beta|RTT_S-RTT|$，其中$\beta$一般取$\dfrac{1}{4}$</li></ul></li><li>超时重传时间RTO：略大于RTTs，$RTO=RTT_S+4*RTT_D$</li></ul><p>针对超时重传无法准确计算RTT的情况，可以使用Karn算法：</p><ol><li>只要报文段重传了，就不采用其往返时间RTT样本</li><li>报文段每重传一次，就把超时重传时间RTO增大一些，一般加倍</li></ol><h3 id="5-4-TCP流量控制"><a href="#5-4-TCP流量控制" class="headerlink" title="5.4 TCP流量控制"></a>5.4 TCP流量控制</h3><p>端到端的问题</p><h4 id="5-4-1-滑动窗口实现流量控制"><a href="#5-4-1-滑动窗口实现流量控制" class="headerlink" title="5.4.1 滑动窗口实现流量控制"></a>5.4.1 滑动窗口实现流量控制</h4><p>流量控制：让发送方发送速率不要太快，要让接收方来得及接收。</p><p>下图的传输流程通过3次更改接收窗口大小来实现流量控制。</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_6.png" alt="TCP流量控制"></p><p>死锁：<br>B向A发送rwnd=0的ACK后，A不再发送数据。<br>B又有了一些缓存，向A发送rwnd&gt;0的ACK，但是丢失了。<br>A等待B的ACK，B等待A的数据，形成死锁。</p><p>解决：收到rwnd=0后启动持续计时器，每次到期发送一个“零窗口探测报文段”</p><h4 id="5-4-2-控制发送时机"><a href="#5-4-2-控制发送时机" class="headerlink" title="5.4.2 控制发送时机"></a>5.4.2 控制发送时机</h4><ol><li>缓存存放的数据达到最大报文段长度MSS字节时，发送一个TCP报文段</li><li>由发送方进程通过PSH、URG标志位控制发送时机</li><li>发送方计时器到期，发送一个报文段</li></ol><p>Nagle算法：应用逐字节发送数据时，通过延迟发送小报文段来提高网络利用率。</p><ul><li>发送方发送第一个数据字节，缓存后续字节</li><li>收到接收方的ACK后，发送缓存中的数据</li><li>当到达的数据达到发送窗口大小一半或MSS时，发送缓存中的数据</li></ul><p>糊涂窗口综合症：交换的数据段大小不是全长而是一些较小的数据。由于每个数据段的有用数据（数据部分）较少，因而消耗的资源也更多，相应的传输效率也更低。</p><p>解决：接收方等待缓存有足够空间后，再发送rwnd&gt;0的ACK</p><h3 id="5-5-TCP拥塞控制"><a href="#5-5-TCP拥塞控制" class="headerlink" title="5.5 TCP拥塞控制"></a>5.5 TCP拥塞控制</h3><p>流量控制:是端到端的问题(接收端控制发送端)，点对点通信量的控制。抑制发送端发送速率，以便使接收端来得及接收。</p><p>拥塞控制:是一个全局性的过程，涉及到与降低网络传输性能有关的所有因素。防止过多数据注入到网络，使网络中的路由器或链路不致过载。</p><h4 id="5-5-1-拥塞控制的基本原理"><a href="#5-5-1-拥塞控制的基本原理" class="headerlink" title="5.5.1 拥塞控制的基本原理"></a>5.5.1 拥塞控制的基本原理</h4><p>出现资源拥塞的条件：对资源需求的总和&gt;可用资源</p><p>原因：链路容量不足、资源分配不均、路由器缓存空间和流量分布不均、处理机性能不足等</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_7.png" alt="拥塞控制的作用"></p><p>开环控制：设计网络时就考虑到拥塞控制，力求避免拥塞的发生</p><p>闭环控制：基于反馈环路</p><ul><li>监测网络系统以便检测到拥塞在何时、何处发生</li><li>将拥塞发生的信息传送到可采取行动的地方</li><li>调整网络系统的运行以解决出现的问题</li></ul><p>拥塞通知的传递策略：显示拥塞通告ECN</p><h4 id="5-5-2-TCP拥塞控制算法"><a href="#5-5-2-TCP拥塞控制算法" class="headerlink" title="5.5.2 TCP拥塞控制算法"></a>5.5.2 TCP拥塞控制算法</h4><p>TCP采用基于窗口的方法进行拥塞控制，属于闭环控制。</p><p>TCP发送方维持一个拥塞窗口 CWND(Congestion Window):</p><ul><li>拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化</li><li>发送端利用拥塞窗口根据网络的拥塞情况调整发送的数据量</li><li>网络没有出现拥塞，拥塞窗口增大一些，以便发送更多的分组，提高网络的利用率</li><li>网络出现拥塞或有可能出现拥塞，拥塞窗口减小一些，减少注入到网络中的分组数</li></ul><p>TCP拥塞判断依据：重传定时器超时</p><p>TCP进行拥塞控制的算法有4种：慢开始、拥塞避免、快重传、快恢复</p><h4 id="5-5-3-慢开始和拥塞避免"><a href="#5-5-3-慢开始和拥塞避免" class="headerlink" title="5.5.3 慢开始和拥塞避免"></a>5.5.3 慢开始和拥塞避免</h4><p>发送方最大报文段SMSS(Sender Maximum Segment Size)：MTU/MSS</p><p>初始拥塞窗口cwnd大小：</p><ul><li>旧规定：1~2个SMSS</li><li>新规定RFC5681：<ul><li>$SMSS&gt;2190B$：$cwnd=2SMSS$，&lt;=2个报文段</li><li>$2190B\ge SMSS&gt;1095B$：$cwnd=3SMSS$，&lt;=3个报文段</li><li>$1095B\ge SMSS$：$cwnd=4SMSS$，&lt;=4个报文段</li></ul></li><li>慢开始门限ssthresh（状态变量）：防止cwnd增长过快引起网络拥塞<ul><li>$cwnd&lt;ssthresh$：慢开始算法</li><li>$cwnd\ge ssthresh$：拥塞避免算法</li></ul></li></ul><p>慢开始算法：cwnd从1开始，2，4，8，16指数增长，若超出ssthresh则取ssthresh</p><p>避免拥塞：cwnd&gt;=ssthresh后，线性增长，每个RTT增加1</p><p>遇到超时：ssthresh=cwnd/2，cwnd=1，慢开始</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_8.png" alt="慢开始和拥塞避免"></p><h4 id="5-5-3-快重传和快恢复"><a href="#5-5-3-快重传和快恢复" class="headerlink" title="5.5.3 快重传和快恢复"></a>5.5.3 快重传和快恢复</h4><p>快重传FR(Fast Retransmission)算法可以让发送方尽早知道发生了个别报文段的丢失。</p><p>发送方只要一连收到三个重复确认，应当立即进行重传(即“快重传”)，这样就不会出现超时，发送方不会误认为出现了网络拥塞。</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_9.png" alt="快重传"></p><p>快恢复算法：在快重传的基础上，将ssthresh减半，cwnd=ssthresh，然后进入拥塞避免状态</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/5_10.png" alt="TCP拥塞控制算法"></p><h3 id="5-6-TCP传输连接管理"><a href="#5-6-TCP传输连接管理" class="headerlink" title="5.6 TCP传输连接管理"></a>5.6 TCP传输连接管理</h3><p>TCP协议工作流程：</p><ol><li>建立连接：三次握手<ol><li>发起方向接收方发送SYN报文，请求建立连接<ul><li>SYN=1，seq=x，表示发送方本次报文的第一个字节的序号是x</li></ul></li><li>接收方收到SYN报文后，回复SYN+ACK报文，表示同意建立连接<ul><li>SYN=1，ACK=1</li><li>seq=y，表示接收方本报文的第一个字节的序号是y</li><li>ack=x+1，表示接收方期望收到的下一个报文的序号是x+1，因为SYN报文占了一个序号</li></ul></li><li>发起方收到SYN+ACK报文后，回复ACK报文，表示连接建立成功<ul><li>ACK=1</li><li>seq=x+1，表示发送方本报文的第一个字节的序号是x+1</li><li>ack=y+1，表示发送方期望收到的下一个报文的序号是y+1，因为SYN+ACK报文占了一个序号</li></ul></li><li>三次握手完成，连接建立成功</li></ol></li><li>数据传输：数据传输阶段</li><li>断开连接：四次挥手<ol><li>发起方向接收方发送FIN报文，请求断开连接<ul><li>FIN=1，seq=u</li></ul></li><li>接收方收到FIN报文后，回复ACK报文，表示收到断开请求<ul><li>ACK=1，seq=v，ack=u+1</li></ul></li><li>接收方向发起方发送FIN+ACK报文，请求断开连接<ul><li>FIN=1，ACK=1，seq=w，ack=u+1</li></ul></li><li>发起方收到FIN+ACK报文后，回复ACK报文，表示收到断开请求，等待2MSL后断开连接<ul><li>ACK=1，seq=u+1，ack=w+1</li></ul></li><li>四次挥手完成，连接断开成功</li></ol></li></ol><ul><li>谁想建立或断开连接，谁就是发起方。发起方就是客户端，接收方就是服务器端。</li><li>MSL(Maximum Segment Lifetime)：报文最大生存时间，2MSL是为了保证网络中的所有报文都已经消失，不会再次出现在网络中，保证TCP协议的全双工连接能够可靠关闭</li></ul><p>TCP四个计时器：超时重传计时器、持续计时器(0窗口报文探测计时器)、时间等待计时器(2MSL关闭连接)、保活计时器、TCP发送报文计时器。</p><h2 id="六、应用层"><a href="#六、应用层" class="headerlink" title="六、应用层"></a>六、应用层</h2><h3 id="6-1-域名系统DNS"><a href="#6-1-域名系统DNS" class="headerlink" title="6.1 域名系统DNS"></a>6.1 域名系统DNS</h3><p>域名服务器类型：</p><ol><li>根域名服务器：管理顶级域名的服务器</li><li>顶级域名服务器：管理在该顶级域名服务器注册的二级域名服务器</li><li>权威域名服务器：管理自己的域名</li><li>本地域名服务器：提供域名解析服务，也称默认域名服务器</li></ol><p>域名查询方式：</p><ol><li>递归查询：主机向本地域名服务器发出查询请求，本地域名服务器不知道答案，则以DNS客户身份向根域名服务器发出查询请求</li><li>迭代查询：本地域名服务器向根域名服务器发出查询请求，根域名服务器告知本地域名服务器下一步应该向哪个域名服务器查询，本地域名服务器再向下一级域名服务器发出查询请求</li></ol><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications/6_1.png" alt="DNS查询过程"></p><h3 id="6-2-文件传输协议FTP"><a href="#6-2-文件传输协议FTP" class="headerlink" title="6.2 文件传输协议FTP"></a>6.2 文件传输协议FTP</h3><p>文件传送协议 FTP (File Transfer Protocol) 是因特网上使用得最广泛的文件传送协议</p><p>实现了通过网络实现异构计算机间的文件“拷贝”、提供交互式的访问、屏蔽了各计算机系统的细节</p><p>FTP使用2个TCP连接：控制连接和数据连接</p><ul><li>控制连接（TCP21）：用于传送请求，在整个会话期间保持打开状态</li><li>数据连接（TCP20）：用于传送文件内容，每次传送文件时打开，传送完毕后关闭</li><li>端口号：服务器使用熟知端口TCP21/20，客户端使用其他端口号&gt;1023</li></ul><p>FTP传输模式：文本模式/ASCII模式、二进制模式/Binary模式</p><h3 id="6-3-简单文件传输协议TFTP"><a href="#6-3-简单文件传输协议TFTP" class="headerlink" title="6.3 简单文件传输协议TFTP"></a>6.3 简单文件传输协议TFTP</h3><p>TFTP(Trivial File Transfer Protocol)是一个很小易于实现的文件传输协议</p><p>只支持传输文件，不支持交互，使用UDP数据报传输，端口号UDP69</p><p>文件长度恰好为 512 字节的整数倍，则在文件传送完毕后，还必须在最后发送一个只含首部而无数据的数据PDU</p><h3 id="6-4-远程终端协议Telnet"><a href="#6-4-远程终端协议Telnet" class="headerlink" title="6.4 远程终端协议Telnet"></a>6.4 远程终端协议Telnet</h3><p>C/S模式，TCP连接，NVT格式，端口号默认TCP23</p><p>Telnet协议的缺点：明文传送</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Telnet</th><th style="text-align:center">SSH</th></tr></thead><tbody><tr><td style="text-align:center">功能</td><td style="text-align:center">远程登录</td><td style="text-align:center">远程登录</td></tr><tr><td style="text-align:center">运输层协议</td><td style="text-align:center">TCP</td><td style="text-align:center">TCP</td></tr><tr><td style="text-align:center">加密</td><td style="text-align:center">明文</td><td style="text-align:center">RSA加密、支持压缩</td></tr><tr><td style="text-align:center">端口号</td><td style="text-align:center">23</td><td style="text-align:center">22</td></tr></tbody></table></div><h3 id="6-5-万维网WWW"><a href="#6-5-万维网WWW" class="headerlink" title="6.5 万维网WWW"></a>6.5 万维网WWW</h3><p>超文本传输协议HTTP(HyperText Transfer Protocol)是万维网的应用层协议，面向事务，本身无连接，是可靠交换文件的基础</p><p>HTTP协议永远都是客户端发起请求，服务器端响应请求，是无状态的协议，默认端口：TCP80</p><p>URL：&lt;协议&gt;://&lt;主机&gt;:&lt;端口&gt;/&lt;路径&gt;</p><p>HTTP报文：</p><ul><li>请求报文：请求行、请求头、空行、请求体</li><li>响应报文：状态行、响应头、空行、响应体</li></ul><p>状态码：三位数字</p><ul><li>1xx：信息性状态码，接收到请求，继续处理</li><li>2xx：成功状态码，请求成功接收、理解、接受</li><li>3xx：重定向状态码，需要进一步操作</li><li>4xx：客户端错误状态码，请求包含语法错误或无法完成</li><li>5xx：服务器错误状态码，服务器无法完成明显有效的请求</li></ul><h3 id="6-6-电子邮件"><a href="#6-6-电子邮件" class="headerlink" title="6.6 电子邮件"></a>6.6 电子邮件</h3><p>两个重要标准：简单邮件传送协议、互联网报文交换格式(Internet Message Format)</p><p>三个主要构件：用户代理、邮件服务器、SMTP和POP3协议</p><p>发送邮件流程：</p><ol><li>发件人UA向发件人邮件服务器发送邮件<ul><li>SMTP，TCP25</li></ul></li><li>发件人邮件服务器以客户的身份向收件人邮件服务器发送邮件<ul><li>SMTP，TCP25</li></ul></li><li>收件人从收件人邮件服务器接收邮件<ul><li>POP3，TCP110</li></ul></li></ol><p>SMTP三阶段：握手、传输、结束</p><p>POP3：仅客户端内读取</p><p>IMAP：客户端与邮件更新同步</p><h3 id="6-7-DHCP协议"><a href="#6-7-DHCP协议" class="headerlink" title="6.7 DHCP协议"></a>6.7 DHCP协议</h3><ol><li>DHCP Discover报文<ul><li>客户端启动时，客户主机-&gt;DHCP服务器：谁能给我一个IP地址？</li><li>源地址：0.0.0.0</li><li>源端口：UDP 68</li><li>目的地址：255.255.255.255(广播)</li><li>目的端口：UDP 67</li></ul></li><li>DHCP Offer报文<ul><li>服务器接收到广播数据包后，DHCP服务器-&gt;客户主机：我能给你分配IP地址192.168.1.2</li><li>源地址：服务器IP地址</li><li>源端口：UDP 67</li><li>目的地址：255.255.255.255(广播)</li><li>目的端口：UDP 68</li></ul></li><li>DHCP Request报文<ul><li>客户端接收到DHCP服务器分配的IP地址后，客户主机-&gt;DHCP服务器：好的，我接受你分配的IP地址192.168.1.2，请求确认</li><li>源地址：0.0.0.0</li><li>源端口：UDP 68</li><li>目的地址：255.255.255.255(广播)</li><li>目的端口：UDP 67</li></ul></li><li>DHCP ACK报文<ul><li>服务器接收到广播数据包后，DHCP服务器-&gt;客户主机：好的，我已经确认分配给你IP地址</li><li>源地址：服务器IP地址</li><li>源端口：UDP 67</li><li>目的地址：255.255.255.255(广播)</li><li>目的端口：UDP 68</li></ul></li></ol><p>DHCP端口号：UDP 67(服务器)和UDP 68(客户端)</p>]]></content>
    
    
    <summary type="html">地球村互联的基本原理</summary>
    
    
    
    <category term="课程笔记" scheme="https://www.cclmsy.cc/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="课程笔记" scheme="https://www.cclmsy.cc/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《计算机网络与通信-实验》课程笔记</title>
    <link href="https://www.cclmsy.cc/posts/computer_networks_and_communications_lab.html"/>
    <id>https://www.cclmsy.cc/posts/computer_networks_and_communications_lab.html</id>
    <published>2024-12-17T16:00:00.000Z</published>
    <updated>2024-12-17T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="零-概述"><a href="#零-概述" class="headerlink" title="零. 概述"></a>零. 概述</h2><h3 id="0-1-网络分层OSI参考模型"><a href="#0-1-网络分层OSI参考模型" class="headerlink" title="0.1 网络分层OSI参考模型"></a>0.1 网络分层OSI参考模型</h3><p>OSI参考模型是国际标准化组织(ISO)制定的一个用于计算机网络体系结构的标准框架。</p><p>OSI分为7层，自上而下分别是：</p><ul><li>7.应用层：应用软件</li><li>6.表示层：数据格式转换</li><li>5.会话层：决定采用哪种传输方式</li><li>4.传输层：提供端到端的通信，主机连接建立和断开，保证数据传输的可靠性</li><li>3.网络层：根据目标地址实现通信</li><li>2.数据链路层：根据物理地址实现数据的传输</li><li>1.物理层：传输介质</li></ul><h3 id="0-2-TCP-IP参考模型"><a href="#0-2-TCP-IP参考模型" class="headerlink" title="0.2 TCP/IP参考模型"></a>0.2 TCP/IP参考模型</h3><p>TCP/IP参考模型是实际应用最广泛的网络协议体系结构，它只有4层，自上而下分别是：</p><ul><li>4.应用层(包含了OSI的应用层、表示层、会话层)<ul><li>数据</li><li>协议：HTTP、HTTPS、SSH、DNS、FTP、POP3、SMTP…</li></ul></li><li>3.传输层<ul><li>TCP首部+数据</li><li>TCP：面向连接的可靠传输协议，支持应用层所有协议</li><li>UDP：无连接的不可靠传输协议，支持DNS、DHCP、SMB等</li></ul></li><li>2.网络层<ul><li>IP首部+TCP首部+数据</li><li>IP：负责数据包的传输</li><li>ICMP：负责网络故障诊断</li><li>ARP：负责地址解析</li></ul></li><li>1.网络接口层(包含了OSI的数据链路层、物理层)<ul><li>数据链路层<ul><li>MAC首部+LLC首部+IP首部+TCP首部+数据+FCS(帧校验序列)</li><li>MAC子层协议：将数据包封装成帧，通过物理层传输</li></ul></li><li>物理层<ul><li>传输介质</li></ul></li></ul></li></ul><h3 id="0-3-网络层"><a href="#0-3-网络层" class="headerlink" title="0.3 网络层"></a>0.3 网络层</h3><p>网络层关注的问题：如何找到合适的网络路径，将数据包从源主机传输到目的主机。</p><h4 id="0-3-1-数据包分组与封装"><a href="#0-3-1-数据包分组与封装" class="headerlink" title="0.3.1 数据包分组与封装"></a>0.3.1 数据包分组与封装</h4><p>网络层主要协议：IP、ICMP、ARP</p><ul><li>分组交换：以分组为单位的<strong>存储转发</strong>的传输方式，将长的报文分割成若干短分组进行多次传输。</li><li>路由转发：源与目的主机之间可能存在多条相通的路径，网络层选择一条“最佳”路径完成数据转发。</li><li>拥塞控制：合理分配数据包的转发路径，提高转发效率。当产生网络拥塞时，及时更换传输路径。</li><li>异种网络的互连：当源主机和目标主机的网络不属于同一种网络类型时，为了解决不同网络在寻址、分组大小、协议等方面的差异，要求在不同种类网络交界处的路由器能够对分组进行处理，使得分组能够在不同网络上传输。不同的网络类型对分组大小要求不一样，需要重新分组。</li></ul><h4 id="0-3-2-网络层与数据链路层的关系"><a href="#0-3-2-网络层与数据链路层的关系" class="headerlink" title="0.3.2 网络层与数据链路层的关系"></a>0.3.2 网络层与数据链路层的关系</h4><ul><li>通信双方：<ul><li>数据链路层实现在同一局域网内利用MAC地址进行通信</li><li>网络层实现在不同局域网内利用IP地址进行通信</li></ul></li><li>解决的问题：<ul><li>数据链路层实现的是保证两端链路的连通性，可以说数据链路层不能分辨异构的网络</li><li>网络层要解决异构网络互联的问题，按照不同网络协议的格式完成数据的重新封装</li></ul></li></ul><h4 id="0-3-3-IP地址、子网掩码（Subnet-Mask）、网关（Gateway）"><a href="#0-3-3-IP地址、子网掩码（Subnet-Mask）、网关（Gateway）" class="headerlink" title="0.3.3 IP地址、子网掩码（Subnet Mask）、网关（Gateway）"></a>0.3.3 IP地址、子网掩码（Subnet Mask）、网关（Gateway）</h4><p>IP地址：主机在Internet上的一个全世界范围内唯一32位标识符，用点分十进制表示，如192.168.0.1。</p><p>子网掩码：用来划分网络和主机的32位二进制数，用来指明一个IP地址的哪些位标识网络地址，哪些位标识主机地址。</p><p>表示方法：</p><ul><li>CIDR表示法：/子网掩码位数，如/24表示前24位是1，其余8位是0</li><li>点分十进制：xxx.xxx.xxx.xxx，如255.255.255.0</li><li>转换例：255.255.254.0 -&gt; 11111111.11111111.11111110.00000000 -&gt; /23</li></ul><p>网关：一个网络的出口。当一个主机要将数据发送给其他网络的主机时，通常首先将数据发往网关。</p><h4 id="0-3-4-IP地址分类"><a href="#0-3-4-IP地址分类" class="headerlink" title="0.3.4 IP地址分类"></a>0.3.4 IP地址分类</h4><p>IP地址分为A、B、C、D、E五类，每类地址的网络号和主机号的划分不同。</p><div class="table-container"><table><thead><tr><th>类别</th><th>前缀</th><th>网络号</th><th>主机号</th><th>第一字节范围</th><th>子网掩码</th><th>主机数</th><th>预留私有地址网段（RFC 1918）</th></tr></thead><tbody><tr><td>A类</td><td>0</td><td>7位</td><td>24位</td><td>0~127</td><td>/8</td><td>2^24-2=16777214</td><td>10.0.0.0~10.255.255.255</td></tr><tr><td>B类</td><td>10</td><td>14位</td><td>16位</td><td>128~191</td><td>/16</td><td>2^16-2=65534</td><td>172.16.0.0~172.31.255.255</td></tr><tr><td>C类</td><td>110</td><td>21位</td><td>8位</td><td>192~223</td><td>/24</td><td>2^8-2=254</td><td>192.168.0.0~192.168.255.255</td></tr><tr><td>D类</td><td>1110</td><td>多播地址28位</td><td>-</td><td>224~239</td><td>-</td><td>-</td><td>-</td></tr><tr><td>E类</td><td>1111</td><td>保留地址</td><td>-</td><td>240~255</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></div><h4 id="0-3-5-子网划分"><a href="#0-3-5-子网划分" class="headerlink" title="0.3.5 子网划分"></a>0.3.5 子网划分</h4><p>借位：从主机最高位开始借位变为新的子网位，剩余部分仍为主机位</p><p>例如192.168.1.0/24可划分为：</p><ul><li>2个子网：192.168.1.0/25、192.168.1.128/25</li><li>4个子网：192.168.1.0/26、192.168.1.64/26、192.168.1.128/26、192.168.1.192/26</li></ul><div STYLE="page-break-after: always;"></div><h2 id="一-传输介质"><a href="#一-传输介质" class="headerlink" title="一. 传输介质"></a>一. 传输介质</h2><h3 id="1-1-传输介质分类"><a href="#1-1-传输介质分类" class="headerlink" title="1.1 传输介质分类"></a>1.1 传输介质分类</h3><h4 id="1-1-1-有线传输介质"><a href="#1-1-1-有线传输介质" class="headerlink" title="1.1.1 有线传输介质"></a>1.1.1 有线传输介质</h4><p>有线传输介质是指在两个通信设备之间实现的物理连接部分，它能将信号从一方传输到另一方。</p><p>有线传输介质主要有<strong>双绞线</strong>、同轴电缆和光纤。双绞线和同轴电缆传输电信号，光纤传输光信号。</p><h4 id="1-1-2-无线传输介质"><a href="#1-1-2-无线传输介质" class="headerlink" title="1.1.2 无线传输介质"></a>1.1.2 无线传输介质</h4><p>在自由空间传输的电磁波根据频谱可将其分为无线电波、微波、红外线、激光等，信息被加载在电磁波上进行传输。在局域网中，通常只使用无线电波和红外线作为传输介质。</p><h4 id="1-1-3-直连线和交叉线的区别"><a href="#1-1-3-直连线和交叉线的区别" class="headerlink" title="1.1.3 直连线和交叉线的区别"></a>1.1.3 直连线和交叉线的区别</h4><p>直连线：两端的线序相同，用于连接不同设备。</p><ul><li>PC&lt;-&gt;Hub/Switch</li></ul><p>交叉线：两端的线序不同，用于连接相同设备。</p><h3 id="1-2-双绞线制作"><a href="#1-2-双绞线制作" class="headerlink" title="1.2 双绞线制作"></a>1.2 双绞线制作</h3><h4 id="1-2-1-双绞线制作标准：EIA-TIA-568"><a href="#1-2-1-双绞线制作标准：EIA-TIA-568" class="headerlink" title="1.2.1 双绞线制作标准：EIA/TIA 568"></a>1.2.1 双绞线制作标准：EIA/TIA 568</h4><ul><li>A：白绿/绿/白橙/蓝/白蓝/橙/白棕/棕</li><li>B：白橙/橙/白绿/蓝/白蓝/绿/白棕/棕</li></ul><h4 id="1-2-2-双绞线制作步骤"><a href="#1-2-2-双绞线制作步骤" class="headerlink" title="1.2.2 双绞线制作步骤"></a>1.2.2 双绞线制作步骤</h4><ol><li>剥线：用压线钳将双绞线一端的外皮剥去3cm，露出8根线。</li><li>理线：将4对导线分别绕开，8根导线按EIA/TIA 568B标准顺序排列，将线芯撸直并拢。</li><li>剪线：将芯线放到压线钳切刀处，8根线芯要在同一平面上并拢，而且尽量直，留下一定的线芯长度约1.5CM处剪齐。</li><li>插线：将双绞线插入RJ45水晶头中，插入过程均衡力度直到插到尽头。检查8根线芯是否已经全部充分、整齐地排列在水晶头里面。</li><li>压线：用压线钳用力压紧水晶头，抽出即可。</li></ol><h4 id="1-2-3-双绞线测试"><a href="#1-2-3-双绞线测试" class="headerlink" title="1.2.3 双绞线测试"></a>1.2.3 双绞线测试</h4><p>直连线(两端都是568B标准)：12345678</p><p>交叉线(一端是568A标准，一端是568B标准)：36145278</p><p>100Mbps以太网线实际使用的线：白绿/绿/白橙/橙</p><div STYLE="page-break-after: always;"></div><h2 id="二-DHCP"><a href="#二-DHCP" class="headerlink" title="二. DHCP"></a>二. DHCP</h2><h3 id="2-1-DHCP工作原理"><a href="#2-1-DHCP工作原理" class="headerlink" title="2.1 DHCP工作原理"></a>2.1 DHCP工作原理</h3><p>DHCP(Dynamic Host Configuration Protocol)是一种动态主机配置协议，它是一种自动分配IP地址的协议。</p><h4 id="DHCP工作原理"><a href="#DHCP工作原理" class="headerlink" title="DHCP工作原理"></a>DHCP工作原理</h4><ol><li>DHCP Discover报文<ul><li>客户端启动时，客户主机-&gt;DHCP服务器：谁能给我一个IP地址？</li><li>源地址：0.0.0.0</li><li>源端口：UDP 68</li><li>目的地址：255.255.255.255(广播)</li><li>目的端口：UDP 67</li></ul></li><li>DHCP Offer报文<ul><li>服务器接收到广播数据包后，DHCP服务器-&gt;客户主机：我能给你分配IP地址192.168.1.2</li><li>源地址：服务器IP地址</li><li>源端口：UDP 67</li><li>目的地址：255.255.255.255(广播)</li><li>目的端口：UDP 68</li></ul></li><li>DHCP Request报文<ul><li>客户端接收到DHCP服务器分配的IP地址后，客户主机-&gt;DHCP服务器：好的，我接受你分配的IP地址192.168.1.2，请求确认</li><li>源地址：0.0.0.0</li><li>源端口：UDP 68</li><li>目的地址：255.255.255.255(广播)</li><li>目的端口：UDP 67</li></ul></li><li>DHCP ACK报文<ul><li>服务器接收到广播数据包后，DHCP服务器-&gt;客户主机：好的，我已经确认分配给你IP地址</li><li>源地址：服务器IP地址</li><li>源端口：UDP 67</li><li>目的地址：255.255.255.255(广播)</li><li>目的端口：UDP 68</li></ul></li></ol><p>DHCP端口号：UDP 67(服务器)和UDP 68(客户端)</p><h3 id="2-2-DHCP配置"><a href="#2-2-DHCP配置" class="headerlink" title="2.2 DHCP配置"></a>2.2 DHCP配置</h3><ol><li>网络拓扑图<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/2_1.png" alt="网络拓扑图2"></li></ul></li><li>设置DHCP服务器和PC机的TCP/IP属性<ul><li>DHCP服务器：192.168.1.1/24</li><li>PC1/PC2：自动获取IP地址</li></ul></li><li>配置DHCP服务器<ul><li>设置IP池为192.168.1.10-20</li></ul></li></ol><h3 id="2-3-验证DHCP配置"><a href="#2-3-验证DHCP配置" class="headerlink" title="2.3 验证DHCP配置"></a>2.3 验证DHCP配置</h3><ol><li>使用ipconfig /all命令查看各机器的TCP/IP配置<ul><li>PC1: 192.168.0.10</li><li>PC2: 192.168.0.11</li></ul></li><li>使用ping命令测试连通性<ul><li>PC1&lt;-&gt;PC2：连通</li></ul></li></ol><div STYLE="page-break-after: always;"></div><h2 id="三-DNS及Web服务"><a href="#三-DNS及Web服务" class="headerlink" title="三. DNS及Web服务"></a>三. DNS及Web服务</h2><h3 id="3-1-DNS工作原理"><a href="#3-1-DNS工作原理" class="headerlink" title="3.1 DNS工作原理"></a>3.1 DNS工作原理</h3><p>DNS(Domain Name System，域名系统)是一个分布式数据库，用于域名和IP地址之间的映射。</p><ul><li>IPv4：32位二进制数，可写作点分十进制</li><li>IPv6：128位二进制数，可写作冒号分隔的8组16进制数</li></ul><p>DNS工作流程：</p><ol><li>PC机-&gt;首选DNS服务器：我要访问www.baidu.com</li><li>首选DNS服务器-&gt;根域名’.’DNS服务器：www.baidu.com对应的IP地址是多少？</li><li>根域名’.’DNS服务器-&gt;首选DNS服务器：www.baidu.com归’com’DNS服务器管</li><li>首选DNS服务器-&gt;’com’DNS服务器：www.baidu.com对应的IP地址是多少？</li><li>‘com’DNS服务器-&gt;首选DNS服务器：www.baidu.com归’baidu.com’DNS服务器管</li><li>首选DNS服务器-&gt;’baidu.com’DNS服务器：www.baidu.com对应的IP地址是多少？</li><li>‘baidu.com’DNS服务器-&gt;首选DNS服务器：www.baidu.com对应的IP地址是XXX.XXX.XXX.XXX</li><li>首选DNS服务器-&gt;PC机：www.baidu.com对应的IP地址是XXX.XXX.XXX.XXX</li><li>PC机-&gt;Web服务器：我要访问XXX.XXX.XXX.XXX</li><li>Web服务器-&gt;PC机：返回相关的数据</li></ol><ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/3_1.png" alt="DNS工作原理"></li></ul><p>递归查询：递归查询是一种DNS服务器的查询模式。在该模式下，DNS服务器接收到客户机请求，必须使用一个准确的查询结果回复客户机。如果DNS服务器本地没有存储查询DNS信息，那么该服务器会询问其他服务器，并将返回的查询结果提交给客户机。</p><p>迭代查询：DNS服务器会向客户机提供其他能够解析查询请求的DNS服务器地址。当客户机发送查询请求时，DNS服务器并不直接回复查询结果，而是告诉客户机另一台DNS服务器地址，客户机再向这台DNS服务器提交请求，依次循环直到返回查询的结果为止。</p><p>DNS端口号：UDP 53</p><h3 id="3-2-HTTP工作原理"><a href="#3-2-HTTP工作原理" class="headerlink" title="3.2 HTTP工作原理"></a>3.2 HTTP工作原理</h3><p>HTTP(HyperText Transfer Protocol，超文本传输协议)是一种用于传输超文本的协议。</p><p>HTTP工作流程：</p><pre><code>1. 客户端浏览器解析URL2. 客户端浏览器生产HTTP请求信息3. 服务器发送响应HTTP页面4. 客户端浏览器解析响应页面</code></pre><h3 id="3-3-DNS配置"><a href="#3-3-DNS配置" class="headerlink" title="3.3 DNS配置"></a>3.3 DNS配置</h3><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/3_2.png" alt="网络拓扑图3"></li></ul></li><li>配置DNS服务器<ul><li>jike.com配置Host IP：192.168.0.20(对应Web服务器)</li><li>ruanjian.com配置Host IP：192.168.0.70(没有对应的Web服务器)</li></ul></li></ol><h3 id="3-4-验证DNS配置"><a href="#3-4-验证DNS配置" class="headerlink" title="3.4 验证DNS配置"></a>3.4 验证DNS配置</h3><ol><li>使用ipconfig /all命令查看各机器的TCP/IP配置</li><li>在客户端PC上使用ping命令测试连通性<ul><li>jike.com：Reply from 192.168.0.20: bytes=32 time&lt;1ms TTL=128(来自192.168.0.20的回复：字节=32 时间&lt;1ms TTL=128)</li><li>ruanjian.com：Request timed out(来自192.168.0.10的回复：无法访问目标主机)</li><li>anquan.com：Unknown host <www.anquan.com(Ping请求找不到主机anquan.com。请检查该名称，然后重试)></li></ul></li></ol><h3 id="3-5-FTP服务配置"><a href="#3-5-FTP服务配置" class="headerlink" title="3.5 FTP服务配置"></a>3.5 FTP服务配置</h3><ol><li>在控制面板中启用IIS(Internet Information Services，Internet信息服务)</li><li>打开IIS，网站名称改为jike，选择物理路径。在路径下新建index.html文件，用ANSI编码保存，并在默认文档中添加该文档。此时访问jingke.com/index.html应该能看到index.html的内容。</li><li>新建一个FTP站点，名称为jikeFTP，选择物理路径(在该位置新建几个文件夹测试)，绑定IP地址为192.168.0.20，选择无SSL，允许所有用户访问。此时使用FTP客户端连接，应该能看到FTP站点的内容。</li></ol><div STYLE="page-break-after: always;"></div><h2 id="四-交换机工作原理及基本配置"><a href="#四-交换机工作原理及基本配置" class="headerlink" title="四. 交换机工作原理及基本配置"></a>四. 交换机工作原理及基本配置</h2><h3 id="4-1-交换机"><a href="#4-1-交换机" class="headerlink" title="4.1 交换机"></a>4.1 交换机</h3><p>交换机(Switch)是一种网络设备，用于在计算机网络中连接多个设备，实现设备之间的通信。</p><p>最常见的以太网交换机工作于OSI网络参考模型的第二层(即数据链路层)，是一种基于MAC地址识别、完成以太网数据帧转发的网络设备。</p><h4 id="4-1-1-MAC地址"><a href="#4-1-1-MAC地址" class="headerlink" title="4.1.1 MAC地址"></a>4.1.1 MAC地址</h4><p>MAC地址(Media Access Control Address，介质访问控制地址)是一个用来识别网络设备的地址，又称物理地址。</p><p>由48位二进制数组成，通常以12位十六进制数表示，如00-0C-29-3D-2E-7C，也写作000C.293D.2E7C、00:0C:29:3D:2E:7C。</p><p>前24位是厂商识别码，后24位是设备识别码(供应商对网卡的唯一编号)。</p><ul><li>单播地址：MAC地址的第一个字节的最低位为0，可作为目的地址和源地址。</li><li>组播地址：MAC地址的第一个字节的最低位为1，仅能作为目的地址。</li><li>广播地址：MAC地址全为1，即FF-FF-FF-FF-FF-FF，用于向同一网络中的所有设备发送数据。</li></ul><h4 id="4-1-2-MAC地址表"><a href="#4-1-2-MAC地址表" class="headerlink" title="4.1.2 MAC地址表"></a>4.1.2 MAC地址表</h4><p>交换机内部用于存放物理地址与交换机端口映射关系的数据库，交换机依靠MAC地址表实现数据帧的转发。</p><pre><code>- Static MAC Address：静态MAC地址，由管理员手动配置。- Dynamic MAC Address：动态MAC地址，由交换机自动学习。</code></pre><p>动态MAC地址学习过程</p><ol><li>主机A-&gt;交换机：我要发送数据帧给主机B。 交换机：记录主机A的MAC地址和端口映射。</li><li>交换机-&gt;广播：主机A的数据帧发送给所有端口。</li><li>主机B-&gt;交换机：我收到数据帧了，回复。 交换机：记录主机B的MAC地址和端口映射。</li></ol><h4 id="4-1-3-数据帧"><a href="#4-1-3-数据帧" class="headerlink" title="4.1.3 数据帧"></a>4.1.3 数据帧</h4><ul><li>数据帧(Data Frame)是数据链路层的协议数据单元，包括：帧头、数据、帧尾。</li><li>在以太网链路上的数据单元称为以太网帧，现在最常见的以太网帧是Ethernet II，包括：目的MAC地址(6B)、源MAC地址(6B)、类型/长度(2B)、数据(46-1500B)、FCS帧校验序列(4B)。</li></ul><p>帧转发/过滤：单播帧依靠MAC地址表进行转发/过滤，组播帧向指定端口转发，广播帧向所有端口转发</p><p>帧转发方式：</p><ul><li>直通转发：交换机收到帧头(通常只检查14个字节)后立刻察看目的MAC地址并进行转发。</li><li>存储转发：接收完整的帧，执行完校验后，转发正确的帧而丢弃错误的帧。</li><li>无碎片直通转发：交换机读取前64个字节后开始转发。</li></ul><h3 id="4-2-交换机本地配置-MAC地址绑定"><a href="#4-2-交换机本地配置-MAC地址绑定" class="headerlink" title="4.2 交换机本地配置-MAC地址绑定"></a>4.2 交换机本地配置-MAC地址绑定</h3><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/4_1.png" alt="网络拓扑图4"></li></ul></li><li>计算机串口或USB口与交换机的Console端口连接；打开计算机Win系统的“超级终端”或其他串口通信客户端软件；Serial Port按交换机要求设置，一般默认<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/4_2.png" alt="终端配置"></li></ul></li><li>常用命令/模式切换命令<ul><li>Switch&gt;   //(初始)用户模式</li><li>Switch&gt;enable  //进入特权模式</li><li>Switch#configure terminal  //进入全局配置模式</li><li>Switch(config)#interface fa0/1  //进入接口fa0/1配置模式</li><li>Switch(config-if)#end //退出配置模式，返回特权模式</li><li>Switch#show clock  //查看交换机时间</li><li>Switch#show version  //查看交换机版本信息</li><li>修改交换机主机名<ul><li>Switch#configure terminal</li><li>Switch(config)#hostname jike</li><li>jike(config)#</li></ul></li></ul></li><li>MAC地址表管理命令<ul><li>Switch#show mac-address-table  //查看MAC地址表</li><li>Switch#show mac-address-table aging-time  //查看MAC地址表老化时间</li><li>交换机上添加MAC地址绑定<ul><li>Switch(config)#mac-address-table static aaaa.aaaa.aaaa vlan 1 interface fa0/1</li><li>Switch(config)#mac-address-table static aaaa.aaaa.aaab vlan 1 interface fa0/2</li><li>Switch(config)#mac-address-table static aaaa.aaaa.aaac vlan 1 interface fa0/3</li></ul></li><li>交换机上删除MAC地址绑定<ul><li>Switch(config)#no mac-address-table static aaaa.aaaa.aaac vlan 1 interface fa0/3</li></ul></li></ul></li></ol><h3 id="4-3-验证交换机配置"><a href="#4-3-验证交换机配置" class="headerlink" title="4.3 验证交换机配置"></a>4.3 验证交换机配置</h3><p>以绑定了fa0/1、fa0/2的交换机为例，在以下端口连接情况下，使用ping命令测试连通性：</p><ol><li>PC1至1号端口，PC2至2号端口，PC3至3号端口<ul><li>PC1&lt;-&gt;PC2：连通</li><li>PC1&lt;-&gt;PC3：连通</li><li>PC2&lt;-&gt;PC3：连通</li><li>MAC地址表：fa0/1-PC1、fa0/2-PC2、fa0/3-PC3</li></ul></li><li>PC1至2号端口，PC2至1号端口，PC3至10号端口<ul><li>PC1&lt;-&gt;PC2：不连通</li><li>PC1&lt;-&gt;PC3：不连通</li><li>PC2&lt;-&gt;PC3：不连通</li><li>MAC地址表：fa0/1-PC2、fa0/2-PC1、fa0/10-PC3</li></ul></li><li>PC1至5号端口，PC2至2号端口，PC3至3号端口<ul><li>PC1&lt;-&gt;PC2：不连通</li><li>PC1&lt;-&gt;PC3：不连通</li><li>PC2&lt;-&gt;PC3：连通</li><li>MAC地址表：fa0/1-PC2、fa0/2-PC1、fa0/3-PC3</li></ul></li></ol><ul><li>总结：静态MAC地址绑定后，主机与端口连线必须一一对应才能传输数据；如果是动态MAC地址，主机连任意一个端口均可正常收发数据。</li></ul><div STYLE="page-break-after: always;"></div><h2 id="五-交换机VLAN"><a href="#五-交换机VLAN" class="headerlink" title="五. 交换机VLAN"></a>五. 交换机VLAN</h2><h3 id="5-1-VLAN"><a href="#5-1-VLAN" class="headerlink" title="5.1 VLAN"></a>5.1 VLAN</h3><p>交换网络是平面网络结构，必须依赖广播，广播域过大会导致网络拥塞。</p><p>VLAN(Virtual Local Area Network，虚拟局域网)是一组逻辑上的设备和用户，这些设备和用户并不受物理位置的限制，可以根据功能、部门及应用等因素将它们组织起来，相互之间的通信就好像它们在同一个网段中一样。</p><p>VLAN的特点：</p><ul><li>基于逻辑的分组，不受物理位置限制。</li><li>在同一VLAN内和真实局域网相同。</li><li>不同VLAN内用户要通信需要借助三层设备。</li></ul><p>基于端口的VLAN：交换机端口可以划分到不同的VLAN中，不同VLAN之间的通信需要通过路由器。</p><h4 id="5-1-1-IEEE-802-1Q标准"><a href="#5-1-1-IEEE-802-1Q标准" class="headerlink" title="5.1.1 IEEE 802.1Q标准"></a>5.1.1 IEEE 802.1Q标准</h4><p>定义了基于端口的VLAN模型，规定如何标识带有 VLAN 成员信息的以太帧，定义VLAN标签格式。</p><p>在以太网帧中增加了一个4字节的802.1Q帧头，用于标识VLAN信息：目的MAC地址(6B)、源MAC地址(6B)、802.1Q帧头(4B)、类型/长度(2B)、数据(46-1500B)、FCS帧校验序列(4B)。</p><p>802.1Q帧头格式：TPID(2B)+PCP(3b)+DEI(1b)+VID(12b)</p><ul><li>TPID：标识802.1Q帧头，取值0x8100</li><li>PCP：优先级，取值0-7，0最低，7最高</li><li>DEI：丢弃标志，用于QoS</li><li>VID：VLAN ID，取值1-4094</li></ul><h4 id="5-1-2-交换机端口类型"><a href="#5-1-2-交换机端口类型" class="headerlink" title="5.1.2 交换机端口类型"></a>5.1.2 交换机端口类型</h4><div class="table-container"><table><thead><tr><th></th><th>Access端口</th><th>Trunk端口</th></tr></thead><tbody><tr><td>Tag</td><td>UnTagged端口（接入端口）</td><td>Tag Aware端口（干道端口）</td></tr><tr><td>VLAN</td><td>只能属于一个VLAN</td><td>可以允许多个VLAN通过</td></tr><tr><td>数据帧</td><td>发送的数据帧不带VLAN标签</td><td>发送的数据帧带有VLAN标签</td></tr><tr><td>用途</td><td>一般用于连接终端设备（PC机）</td><td>一般用于交换机之间的连接</td></tr><tr><td>默认</td><td>交换机上的默认端口</td><td>-</td></tr></tbody></table></div><h3 id="5-2-交换机VLAN配置"><a href="#5-2-交换机VLAN配置" class="headerlink" title="5.2 交换机VLAN配置"></a>5.2 交换机VLAN配置</h3><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/5_1.png" alt="网络拓扑图5"></li></ul></li><li>创建VLAN<ul><li>Switch(config)#vlan 10</li><li>Switch(config-vlan)#exit</li><li>Switch(config)#vlan 20</li><li>Switch(config-vlan)#exit</li></ul></li><li>配置Access端口/划分VLAN<ul><li>Switch(config)#interface fa0/1-5</li><li>Switch(config-if-range)#switchport mode access</li><li>Switch(config-if-range)#switchport access vlan 10</li><li>Switch(config-if-range)#exit</li><li>Switch(config)#interface fa0/6-10</li><li>Switch(config-if-range)#switchport mode access</li><li>Switch(config-if-range)#switchport access vlan 20</li><li>Switch(config-if-range)#exit</li></ul></li><li>配置Trunk端口<ul><li>Switch(config)#interface fa0/24</li><li>Switch(config-if)#switchport mode trunk</li></ul></li></ol><h3 id="5-3-验证交换机VLAN配置"><a href="#5-3-验证交换机VLAN配置" class="headerlink" title="5.3 验证交换机VLAN配置"></a>5.3 验证交换机VLAN配置</h3><ol><li>显示交换机VLAN配置<ul><li>Switch#show vlan brief</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/5_2.png" alt="交换机VLAN配置"></li></ul></li><li>连通性测试<ul><li>PC1&lt;-&gt;PC4：连通</li><li>PC2&lt;-&gt;PC3：连通</li><li>其余PC之间不连通</li></ul></li><li>总结：同一VLAN内可以直接相互通信，不同VLAN内不能直接相互通信。</li></ol><div STYLE="page-break-after: always;"></div><h2 id="六-VLAN间通信"><a href="#六-VLAN间通信" class="headerlink" title="六. VLAN间通信"></a>六. VLAN间通信</h2><h3 id="6-1-路由"><a href="#6-1-路由" class="headerlink" title="6.1 路由"></a>6.1 路由</h3><p>VLAN是广播域（二层概念），而两个广播域之间通常由路由器连接，广播域之间来往的数据包都是由路由（三层概念）转发的。路由的主要功能由路由器和三层交换机提供。</p><p>VLAN间通信一般有以下3种实施方案：</p><ol><li>路由器多端口方式</li><li>单臂路由方式</li><li>三层交换机方式</li></ol><h3 id="6-2-路由器多端口方式"><a href="#6-2-路由器多端口方式" class="headerlink" title="6.2 路由器多端口方式"></a>6.2 路由器多端口方式</h3><p>与路由器连接不同子网通信的方法一样，为路由器端口设置一个IP地址作为对应VLAN的网关，有几个VLAN就需要在几个端口设置IP地址。</p><p>缺点：<br>一般中大型局域网，VLAN数量可以很多，而路由器端口数量较少，路由器会成为局域网性能瓶颈。<br>路由器采用软件对IP报文进行转发，占用CPU和内存资源，效率比较低，无法胜任通信数据量较大的局域网。</p><h4 id="6-2-1-配置"><a href="#6-2-1-配置" class="headerlink" title="6.2.1 配置"></a>6.2.1 配置</h4><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_1_1.png" alt="网络拓扑图6_1"></li></ul></li><li>二层交换机配置<ul><li>创建VLAN10和VLAN20</li><li>配置Access端口/划分VLAN<ul><li>VLAN10: fa0/1,fa0/10</li><li>VLAN20: fa0/2,fa0/20</li></ul></li><li>查看VLAN配置: Switch#show vlan brief</li></ul></li><li>路由器配置<ul><li>Router(config)#interface fa0/0</li><li>Router(config-if)#no shutdown //激活接口</li><li>Router(config-if)#ip address 192.168.1.1 255.255.255.0 //设置接口IP地址</li><li>Router(config-if)#exit</li><li>Router(config)#interface fa0/1</li><li>Router(config-if)#no shutdown</li><li>Router(config-if)#ip address 192.168.2.1 255.255.255.0</li><li>Router(config-if)#exit</li></ul></li></ol><h4 id="6-2-2-验证路由器配置"><a href="#6-2-2-验证路由器配置" class="headerlink" title="6.2.2 验证路由器配置"></a>6.2.2 验证路由器配置</h4><ol><li>显示路由表<ul><li>Router#show ip route</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_1_2.png" alt="路由表"></li></ul></li><li>显示IP接口信息<ul><li>Router#show ip interface brief</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_1_3.png" alt="IP接口"></li></ul></li><li>连通性测试<ul><li>PC1&lt;-&gt;PC2：连通</li></ul></li></ol><h3 id="6-3-单臂路由方式"><a href="#6-3-单臂路由方式" class="headerlink" title="6.3 单臂路由方式"></a>6.3 单臂路由方式</h3><p>在路由器的一个物理接口上配置多个子接口，作为不同VLAN的默认网关，实现原来相互隔离的不同VLAN之间的通信。</p><p>缺点：<br>路由器转发速度有限，无法满足VLAN间通信数据量大的需求，并且容易造成单点故障。<br>数据在物理链路上往返传输，会有转发延迟，很可能成为局域网性能瓶颈。</p><h4 id="6-3-1-配置"><a href="#6-3-1-配置" class="headerlink" title="6.3.1 配置"></a>6.3.1 配置</h4><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_2_1.png" alt="网络拓扑图6_2"></li></ul></li><li>二层交换机配置<ul><li>创建VLAN10和VLAN20</li><li>配置Access端口/划分VLAN<ul><li>VLAN10: fa0/1</li><li>VLAN20: fa0/2</li></ul></li><li>配置Trunk端口<ul><li>Switch(config)#interface fa0/24</li><li>Switch(config-if)#switchport mode trunk</li><li>Switch(config-if)#exit</li></ul></li><li>查看VLAN配置: Switch#show vlan brief</li></ul></li><li>路由器配置<ul><li>Router(config)#interface fa0/0</li><li>Router(config-if)#no shutdown //激活接口</li><li>Router(config-if)#exit</li><li>Router(config)#interface fa0/0.10 //创建虚拟子接口10</li><li>Router(config-subif)#encapsulation dot1Q 10 //封装VLAN协议（802.1Q）</li><li>Router(config-subif)#ip address 192.168.1.1 255.255.255.0 //设置子接口IP地址</li><li>Router(config-subif)#exit</li><li>Router(config)#interface fa0/0.20 //创建虚拟子接口20</li><li>Router(config-subif)#encapsulation dot1Q 20</li><li>Router(config-subif)#ip address 192.168.2.1 255.255.255.0</li><li>Router(config-subif)#exit</li></ul></li></ol><h4 id="6-3-2-验证路由器配置"><a href="#6-3-2-验证路由器配置" class="headerlink" title="6.3.2 验证路由器配置"></a>6.3.2 验证路由器配置</h4><ol><li>显示路由表<ul><li>Router#show ip route</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_2_2.png" alt="路由表"></li></ul></li><li>显示IP接口信息<ul><li>Router#show ip interface brief</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_2_3.png" alt="IP接口"></li></ul></li><li>显示Trunk端口<ul><li>Switch#show interfaces trunk</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_2_4.png" alt="Trunk端口"></li></ul></li><li>连通性测试<ul><li>PC1&lt;-&gt;PC2：连通</li></ul></li></ol><h3 id="6-4-三层交换机虚拟接口SVI"><a href="#6-4-三层交换机虚拟接口SVI" class="headerlink" title="6.4 三层交换机虚拟接口SVI"></a>6.4 三层交换机虚拟接口SVI</h3><p>三层交换机实现VLAN间通信的方法是在已有的VLAN上创建虚接口，它同样可以配置IP地址，借助虚接口三层交换机能够实现路由转发功能。</p><p>优点：<br>VLAN间流量不必经过路由器，网络延时和抖动都很小，同时也极大程度的减轻上层接入路由器的负载。<br>三层交换机的VLAN间路由由交换机的三层转发引擎完成，其性能取决于交换机的背板转发速率，可以在多个端口上轻松实现线速转发，可以获得很好的性能。</p><h4 id="6-4-1-配置"><a href="#6-4-1-配置" class="headerlink" title="6.4.1 配置"></a>6.4.1 配置</h4><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_3_1.png" alt="网络拓扑图6_3"></li></ul></li><li>二层交换机配置<ul><li>创建VLAN10和VLAN20</li><li>配置Access端口/划分VLAN<ul><li>VLAN10: fa0/1</li><li>VLAN20: fa0/2</li></ul></li><li>配置Trunk端口：fa0/24</li><li>查看VLAN配置: Switch#show vlan brief</li><li>查看Trunk端口: Switch#show interfaces trunk</li></ul></li><li>三层交换机配置<ul><li>创建VLAN10和VLAN20</li><li>配置SVI的ip地址<ul><li>Switch(config)#interface vlan 10</li><li>Switch(config-if)#ip address 192.168.1.1 255.255.255.0</li><li>Switch(config-if)#exit</li><li>Switch(config)#interface vlan 20</li><li>Switch(config-if)#ip address 192.168.2.1 255.255.255.0</li><li>Switch(config-if)#exit</li></ul></li><li>开启路由功能<ul><li>Switch(config)#ip routing</li></ul></li></ul></li></ol><h4 id="6-4-2-验证三层交换机配置"><a href="#6-4-2-验证三层交换机配置" class="headerlink" title="6.4.2 验证三层交换机配置"></a>6.4.2 验证三层交换机配置</h4><ol><li>显示路由表<ul><li>Switch#show ip route</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/6_3_2.png" alt="路由表"></li></ul></li><li>连通性测试：4台主机之间均可相互通信</li></ol><div STYLE="page-break-after: always;"></div><h2 id="七-静态路由"><a href="#七-静态路由" class="headerlink" title="七. 静态路由"></a>七. 静态路由</h2><h3 id="7-1-路由器"><a href="#7-1-路由器" class="headerlink" title="7.1 路由器"></a>7.1 路由器</h3><p>路由器（Router）：连接两个或多个网络的硬件设备，在网络间起网关的作用，是读取每一个IP数据包中的地址然后决定如何传送的专用智能性的网络设备。</p><ul><li>直连网络（邻村）：直接连接在某个路由器上的网络，称为该路由器的直连网络。每个路由器接口的IP作为所连直连网络的网关（村口）。</li><li>非直连网络：不直接连接在某个路由器上的网络（通常间隔多个路由器）。</li></ul><h4 id="7-1-1-路由表"><a href="#7-1-1-路由表" class="headerlink" title="7.1.1 路由表"></a>7.1.1 路由表</h4><p>路由器依靠路由表来选择转发路径。<br>路由表中包含有该路由器掌握的所有目的网络地址，以及通过此路由器到达这些网络中最佳路径，这个最佳路径指的是路由器的某个接口或下一条路由器的地址。</p><h4 id="7-1-2-静态路由"><a href="#7-1-2-静态路由" class="headerlink" title="7.1.2 静态路由"></a>7.1.2 静态路由</h4><p>静态路由：由系统管理员事先设置好固定的路由信息。</p><ul><li>优点：简单、高效、可靠。优先级最高。</li><li>缺点：不能动态地适用网络状况的变化。</li></ul><h3 id="7-2-静态路由配置"><a href="#7-2-静态路由配置" class="headerlink" title="7.2 静态路由配置"></a>7.2 静态路由配置</h3><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/7_1.png" alt="网络拓扑图7_1"></li></ul></li><li>路由器配置（Router1为例）<ul><li>Router1(config)#interface fa0/0</li><li>Router1(config-if)#no shutdown  //激活接口</li><li>Router1(config-if)#ip address 192.168.1.1 255.255.255.0 //设置接口IP地址为所在网络的网关</li><li>Router1(config-if)#exit</li><li>Router1(config)#interface fa0/1</li><li>Router1(config-if)#no shutdown</li><li>Router1(config-if)#ip address 192.168.2.1 255.255.255.0</li><li>Router1(config-if)#exit</li><li>Router1(config)#ip route 192.168.3.0 255.255.255.0 192.168.2.2 //设置静态路由：ip route &lt;目的网络&gt; &lt;子网掩码&gt; &lt;下一跳地址&gt;</li></ul></li></ol><h3 id="7-3-验证静态路由配置"><a href="#7-3-验证静态路由配置" class="headerlink" title="7.3 验证静态路由配置"></a>7.3 验证静态路由配置</h3><ol><li>显示路由表<ul><li>Router1#show ip route</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/7_2.png" alt="路由表"></li></ul></li><li>连通性测试：3台主机之间均可相互通信</li></ol><h3 id="7-4-静态路由配置补充"><a href="#7-4-静态路由配置补充" class="headerlink" title="7.4 静态路由配置补充"></a>7.4 静态路由配置补充</h3><ul><li>通过192.168.0.1的端口，访问所有24位子网掩码的目的网络<ul><li>Router1(config)#ip route 0.0.0.0 255.255.255.0 192.168.0.1</li></ul></li><li>通过192.168.0.1的端口，访问所有网络<ul><li>Router1(config)#ip route 0.0.0.0 0.0.0.0 192.168.0.1</li></ul></li></ul><div STYLE="page-break-after: always;"></div><h2 id="八-动态路由RIP"><a href="#八-动态路由RIP" class="headerlink" title="八. 动态路由RIP"></a>八. 动态路由RIP</h2><h3 id="8-1-动态路由"><a href="#8-1-动态路由" class="headerlink" title="8.1 动态路由"></a>8.1 动态路由</h3><p>动态路由：路由器根据网络系统的运行情况而自动调整的路由信息。路由器根据路由选择协议（Routing Protocol）提供的功能，自动学习和记忆网络运行情况，在需要时自动计算数据传输的最佳路径。</p><ul><li>优点：动态路由可以自动学习网络的拓朴结构，并更新路由表。</li><li>缺点：路由广播更新信息将占据大量的网络带宽。</li></ul><h4 id="8-1-1-动态路由分类"><a href="#8-1-1-动态路由分类" class="headerlink" title="8.1.1 动态路由分类"></a>8.1.1 动态路由分类</h4><ul><li>按照使用的区域不同<ul><li>内部网关协议IGP（Interior Gateway Protocol）域内协议。如RIP、EIGRP、OSPF、ISIS</li><li>外部网关协议EGP（Exterior Gateway Protocol）域间协议。如BGP</li></ul></li><li>按照算法不同<ul><li>距离矢量协议（Distance Vector）。如RIPV1、RIPV2、BGP</li><li>链路状态路由协议（Link State）。如OSPF、ISIS</li></ul></li><li>按是否携带子网掩码<ul><li>有类路由协议。如RIPV1</li><li>无类路由协议。如RIPV2、OSPF、ISIS、BGP</li></ul></li></ul><h4 id="8-1-2-RIP协议"><a href="#8-1-2-RIP协议" class="headerlink" title="8.1.2 RIP协议"></a>8.1.2 RIP协议</h4><p>RIP是一种基于距离矢量（Distance-Vector）算法的协议，它使用跳数（Hop Count）作为度量值来衡量到达目的地址的距离。</p><p>在RIP网络中，缺省情况下，设备到与它直接相连网络的跳数为0，通过一个设备可达的网络的跳数为1，其余依此类推。也就是说，度量值等于从本网络到达目的网络间的设备数量。</p><p>为限制收敛时间，RIP规定度量值取0～15之间的整数，大于或等于16的跳数被定义为无穷大，即目的网络或主机不可达。由于这个限制，使得RIP不可能在大型网络中得到应用。</p><p>RIP路由器工作流程：</p><ul><li>RIP路由器A,B初始的路由表中只有自己的直连路由。</li><li>每30秒，向相邻路由器发送自己的路由表。</li><li>收到相邻路由器的路由表后，更新自己的路由表：<ul><li>新增：如果收到的路由表中有自己没有的路由，则添加到自己的路由表中。</li><li>更新：如果收到的路由表中有自己已有的路由，且新的跳数更小，则更新自己的路由表的跳数。</li></ul></li><li>RIP计时器：<ul><li>更新计时器：每30秒左右发送一次路由表。</li><li>失效计时器：180秒（6倍更新时间）未更新，标记为不可达。</li><li>刷新计时器：无效路由240秒未更新，从路由表中删除。</li></ul></li></ul><p>路由毒化：当一个路由器发现一个网络不可达时，会向其他路由器发送一个度量值为16的路由，以传播路由失效的消息，这个过程称为路由毒化。</p><h4 id="8-1-3-RIP协议版本"><a href="#8-1-3-RIP协议版本" class="headerlink" title="8.1.3 RIP协议版本"></a>8.1.3 RIP协议版本</h4><p>RIPv1</p><ul><li>使用广播的方式发送路由更新</li><li>路由更新信息中不携带子网掩码，为有类路由协议，不支持 VLSM 和 CIDR</li><li>不支持认证</li></ul><p>RIPv2</p><ul><li>发送更新报文的方式为组播，组播地址为224.0.0.9</li><li>路由信息中加入了子网掩码，为无类路由协议，支持 VLSM，支持路由聚合与 CIDR</li><li>支持明文认证和 MD5 密文认证</li></ul><p>VLSM：可变长子网掩码Variable Length Subnet Mask</p><p>CIDR：无类域间路由聚合Classless Inter-Domain Routing</p><h3 id="8-2-RIPV1"><a href="#8-2-RIPV1" class="headerlink" title="8.2 RIPV1"></a>8.2 RIPV1</h3><h4 id="8-2-1-配置"><a href="#8-2-1-配置" class="headerlink" title="8.2.1 配置"></a>8.2.1 配置</h4><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/8_1_1.png" alt="网络拓扑图8_1"></li></ul></li><li>路由器配置（以Router1为例）<ul><li>Router1(config)#interface lo0  //创建Loopback接口，用于模拟网络</li><li>Router1(config-if)#ip address 1.1.1.1 255.255.255.0</li><li>Router1(config-if)#exit</li><li>Router1(config)#interface fa0/0</li><li>Router1(config-if)#no shutdown  //激活接口</li><li>Router1(config-if)#ip address 12.1.1.1 255.55.255.0</li><li>Router1(config-if)#exit</li><li>Router1(config)#router rip  //进入RIP协议配置模式</li><li>Router1(config-router)#version 1  //设置RIP版本为1</li><li>Router1(config-router)#network 1.1.1.0 //设置RIP协议的网络地址</li><li>Router1(config-router)#network 12.1.1.0</li></ul></li></ol><h4 id="8-2-2-验证RIPV1配置"><a href="#8-2-2-验证RIPV1配置" class="headerlink" title="8.2.2 验证RIPV1配置"></a>8.2.2 验证RIPV1配置</h4><ol><li>显示路由表<ul><li>Router1#show ip route</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/8_1_2.png" alt="路由表"></li></ul></li><li>连通性测试：Router1和Router3的lo0接口之间可以相互通信</li></ol><h3 id="8-3-RIPV2"><a href="#8-3-RIPV2" class="headerlink" title="8.3 RIPV2"></a>8.3 RIPV2</h3><h4 id="8-3-1-配置"><a href="#8-3-1-配置" class="headerlink" title="8.3.1 配置"></a>8.3.1 配置</h4><ol><li>网络拓扑图（完成TCP/IP配置）<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/8_2_1.png" alt="网络拓扑图8_2"></li></ul></li><li>路由器配置（以Router1为例）<ul><li>Router1(config)#interface lo0</li><li>Router1(config-if)#ip address 10.1.1.1 255.255.255.0</li><li>Router1(config-if)#exit</li><li>Router1(config)#interface fa0/0</li><li>Router1(config-if)#no shutdown</li><li>Router1(config-if)#ip address 12.1.1.1 255.255.255.0</li><li>Router1(config-if)#exit</li><li>Router1(config)#router rip</li><li>Router1(config-router)#version 2 //设置RIP版本为2</li><li>Router1(config-router)#network 10.1.1.0</li><li>Router1(config-router)#network 12.1.1.0</li><li>Router1(config-router)#no auto-summary //关闭自动汇总</li></ul></li></ol><h4 id="8-3-2-验证RIPV2配置"><a href="#8-3-2-验证RIPV2配置" class="headerlink" title="8.3.2 验证RIPV2配置"></a>8.3.2 验证RIPV2配置</h4><ol><li>显示路由表<ul><li>Router1#show ip route</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/8_2_2.png" alt="路由表"></li></ul></li><li>连通性测试：Router1和Router3的lo0接口之间可以相互通信</li></ol><div STYLE="page-break-after: always;"></div><h2 id="九-NAT"><a href="#九-NAT" class="headerlink" title="九. NAT"></a>九. NAT</h2><h3 id="9-1-NAT"><a href="#9-1-NAT" class="headerlink" title="9.1 NAT"></a>9.1 NAT</h3><p>NAT(Network Address Translation，网络地址转换)是一种将私有地址（RFC 1918）转换为公有地址的技术，用于解决IP地址不足的问题。</p><h4 id="9-1-1-NAT术语"><a href="#9-1-1-NAT术语" class="headerlink" title="9.1.1 NAT术语"></a>9.1.1 NAT术语</h4><ul><li>内部本地IP地址：分配给内部网络中的主机的IP地址，通常这种地址来自RFC1918指定的私有地址空间。</li><li>内部全局IP地址：内部全局IP地址，对外代表一个或多个内部本地IP地址，通常这种地址来自全局惟一的地址空间，通常由ISP提供。</li><li>外部本地IP地址：在内部网络中看到的外部主机的IP地址，通常来自RFC 1918定义的私有地址空间。</li><li>外部全局IP地址：外部网络中的主机的IP地址，通常来自全局可路由的地址空间。</li></ul><h4 id="9-1-2-NAT分类"><a href="#9-1-2-NAT分类" class="headerlink" title="9.1.2 NAT分类"></a>9.1.2 NAT分类</h4><ul><li>静态NAT：一对一映射，将一个内部IP地址映射到一个外部IP地址。</li><li>动态NAT：将内部IP地址动态转换为一组外部IP地址（IP地址池）中的一个。<ul><li>超载NAT：动态NAT的一种实现形式，利用不同端口号将多个内部IP地址转换为一个外部IP地址，也称为PAT、NAPT或端口复用NAT。</li></ul></li></ul><h3 id="9-2-NAT配置"><a href="#9-2-NAT配置" class="headerlink" title="9.2 NAT配置"></a>9.2 NAT配置</h3><h4 id="9-2-1-网路拓扑图"><a href="#9-2-1-网路拓扑图" class="headerlink" title="9.2.1 网路拓扑图"></a>9.2.1 网路拓扑图</h4><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/9_0.png" alt="网络拓扑图"></p><p>完成PC机和服务器的TCP/IP配置</p><h4 id="9-2-2-路由器基础配置"><a href="#9-2-2-路由器基础配置" class="headerlink" title="9.2.2 路由器基础配置"></a>9.2.2 路由器基础配置</h4><ol><li>Router1配置<ul><li>Router1(config)#interface fa0/0</li><li>Router1(config-if)#no shutdown</li><li>Router1(config-if)#ip address 10.1.1.1 255.255.255.0</li><li>Router1(config-if)#ip nat inside //内网接口</li><li>Router1(config-if)#exit</li><li>Router1(config)#interface fa0/1</li><li>Router1(config-if)#no shutdown</li><li>Router1(config-if)#ip address 2.2.2.1 255.255.255.0</li><li>Router1(config-if)#ip nat outside //外网接口</li><li>Router1(config-if)#exit</li><li>Router1(config)#ip route 0.0.0.0 0.0.0.0 2.2.2.2 //设置静态路由：内网主机可以通过Router1访问外网</li></ul></li><li>Router2配置<ul><li>fa0/0: 2.2.2.2/24</li><li>fa0/1: 1.1.1.1/24</li><li>外网不允许访问内网，无需配置静态路由</li></ul></li></ol><h4 id="9-2-3-静态NAT"><a href="#9-2-3-静态NAT" class="headerlink" title="9.2.3 静态NAT"></a>9.2.3 静态NAT</h4><p>在Router1上设置静态NAT，使外网PC可以使用外网地址2.2.2.10访问内部服务器Server0的Web服务（仅80端口）</p><h5 id="配置静态NAT"><a href="#配置静态NAT" class="headerlink" title="配置静态NAT"></a>配置静态NAT</h5><ul><li>Router1(config)#ip nat inside source static tcp 10.1.1.20 80 2.2.2.10 80 //设置静态NAT：内网服务器的80端口映射到外网地址的80端口</li></ul><h5 id="验证静态NAT"><a href="#验证静态NAT" class="headerlink" title="验证静态NAT"></a>验证静态NAT</h5><ol><li>外网PC使用外网地址2.2.2.10访问内部服务器Server0的Web服务<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/9_1_2.png" alt="访问Web服务"></li></ul></li><li>显示NAT转换表<ul><li>Router1#show ip nat translations</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/9_1_1.png" alt="NAT转换表"></li></ul></li></ol><h4 id="9-2-4-动态NAT"><a href="#9-2-4-动态NAT" class="headerlink" title="9.2.4 动态NAT"></a>9.2.4 动态NAT</h4><p>在Router1上设置动态NAT，使内网PC可以通过外网地址2.2.2.11-2.2.2.12访问外网服务器Server1的Web服务（仅80端口）</p><h5 id="配置动态NAT"><a href="#配置动态NAT" class="headerlink" title="配置动态NAT"></a>配置动态NAT</h5><ul><li>Router1(config)#access-list 1 permit any //创建访问控制列表 1</li><li>Router1(config)#ip nat pool pool1 2.2.2.11 2.2.2.12 netmask 255.255.255.0 //创建IP地址池：ip nat pool &lt;名称&gt; &lt;起始地址&gt; &lt;结束地址&gt; netmask &lt;子网掩码&gt;</li><li>Router1(config)#ip nat inside source list 1 pool pool1 //设置动态NAT：ip nat inside source list &lt;访问控制列表&gt; pool <IP地址池></li></ul><h5 id="验证动态NAT"><a href="#验证动态NAT" class="headerlink" title="验证动态NAT"></a>验证动态NAT</h5><ol><li>内网PC使用外网地址1.1.1.10访问外网服务器Server1的Web服务<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/9_2_1.png" alt="访问Web服务"></li><li>由于IP地址池中只有2个地址，第3个内网主机无法访问外网</li></ul></li><li>显示NAT转换表<ul><li>Router1#show ip nat translations</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/9_2_2.png" alt="NAT转换表"></li></ul></li></ol><h3 id="9-3-NAPT"><a href="#9-3-NAPT" class="headerlink" title="9.3 NAPT"></a>9.3 NAPT</h3><p>在Router1上设置动态NAPT。使内网PC可以通过外网地址2.2.2.15访问外网服务器</p><h4 id="配置NAPT"><a href="#配置NAPT" class="headerlink" title="配置NAPT"></a>配置NAPT</h4><ul><li>Router1(config)#ip nat pool pool2 2.2.2.15 2.2.2.15 netmask 255.255.255.0</li><li>Router1(config)#ip nat inside source list 1 pool pool2 overload //overload: 超载，即NAPT</li></ul><h4 id="验证NAPT"><a href="#验证NAPT" class="headerlink" title="验证NAPT"></a>验证NAPT</h4><ol><li>内网PC使用外网地址1.1.1.10访问外网服务器Server1的Web服务<ul><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/9_3_1.png" alt="访问Web服务"></li></ul></li><li>显示NAT转换表<ul><li>Router1#show ip nat translations</li><li><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/9_3_2.png" alt="NAT转换表"></li></ul></li></ol><div STYLE="page-break-after: always;"></div><h2 id="十-ARP协议分析"><a href="#十-ARP协议分析" class="headerlink" title="十. ARP协议分析"></a>十. ARP协议分析</h2><h3 id="10-1-ARP协议"><a href="#10-1-ARP协议" class="headerlink" title="10.1 ARP协议"></a>10.1 ARP协议</h3><p>ARP（Address Resolution Protocol，地址解析协议）主要用于根据 IP 地址求出主机所对应的物理地址（MAC 地址）。</p><p>在网络通信中，主机和主机之间的通信需要根据 OSI 模型进行数据包的封装和解封装，这里面不仅需要封装源目的 IP 地址，也需要源目的 MAC 地址。</p><p>一般情况下，上层应用只知道 IP 地址，而并不关心 MAC 地址，所以就需要通过一个协议来获知目的 MAC 地址，完成数据的封装，这个协议就是 ARP 协议。</p><h4 id="ARP协议工作原理"><a href="#ARP协议工作原理" class="headerlink" title="ARP协议工作原理"></a>ARP协议工作原理</h4><p>主机A已知目标主机B的IP地址，向主机B发送数据包时，需要知道目标主机B的MAC地址。</p><p>ARP协议工作原理如下：</p><ol><li>主机A在自己的ARP缓存表中查找目标主机B的MAC地址，如果找到则直接发送数据包。</li><li>如果ARP缓存表中没有目标主机B的MAC地址，则主机A向本地网络广播ARP请求报文，请求目标主机B的MAC地址。</li><li>目标主机B收到ARP请求报文后，向主机A发送ARP响应报文，包含自己的MAC地址。</li><li>主机A收到ARP响应报文后，将目标主机B的MAC地址存入ARP缓存表，并发送数据包。</li></ol><h3 id="10-2-ARP协议分析"><a href="#10-2-ARP协议分析" class="headerlink" title="10.2 ARP协议分析"></a>10.2 ARP协议分析</h3><p>以管理员身份启动cmd：</p><ul><li>查看ARP缓存表：arp -a</li><li>清空ARP缓存表：arp -d</li><li>删除指定ARP缓存：arp -d <IP地址></li></ul><ol><li>打开Wireshark，在捕获选项中设置过滤器为“arp”。</li><li>开始抓取数据。</li><li>以管理员身份启动cmd，清空ARP缓存表并等待一段时间。</li><li>结束抓取数据。</li><li>选择一对ARP请求和响应报文，进行分析。</li></ol><p>以ARP请求报文为例，具体数据包如下图：</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/10_1.png" alt="ARP请求报文"></p><p>数据链路层数据帧：</p><div class="table-container"><table><thead><tr><th>字段</th><th>长度(bit)</th><th>说明</th><th>ARP Request</th><th>ARP Reply</th></tr></thead><tbody><tr><td>Destination</td><td>48</td><td>目的MAC地址</td><td>ff:ff:ff:ff:ff:ff(广播地址)</td><td>08:xx:xx:xx:xx:ae(发送方MAC地址)</td></tr><tr><td>Source</td><td>48</td><td>源MAC地址</td><td>08:xx:xx:xx:xx:ae(发送方MAC地址)</td><td>d8:xx:xx:xx:xx:d5(目标MAC地址)</td></tr><tr><td>Type</td><td>16</td><td>类型</td><td>0x0806(ARP)</td><td>0x0806(ARP)</td></tr></tbody></table></div><p>ARP数据帧：</p><div class="table-container"><table><thead><tr><th>字段</th><th>长度(bit)</th><th>说明</th><th>ARP Request</th><th>ARP Reply</th></tr></thead><tbody><tr><td>Hardware Type</td><td>16</td><td>硬件类型，标识链路层协议</td><td>0x0001(以太网)</td><td>0x0001(以太网)</td></tr><tr><td>Protocol Type</td><td>16</td><td>协议类型，标识网络层协议</td><td>0x0800(IPv4)</td><td>0x0800(IPv4)</td></tr><tr><td>Hardware Size</td><td>8</td><td>硬件地址大小，标识MAC地址长度</td><td>0x06(6B=48bit)</td><td>0x06(6B=48bit)</td></tr><tr><td>Protocol Size</td><td>8</td><td>协议地址大小，标识IP地址长度</td><td>0x04(4B=32bit)</td><td>0x04(4B=32bit)</td></tr><tr><td>Opcode</td><td>16</td><td>操作码，标识ARP请求或响应</td><td>0x0001(ARP Request)</td><td>0x0002(ARP Reply)</td></tr><tr><td>Sender MAC Address</td><td>48</td><td>发送方MAC地址</td><td>08:xx:xx:xx:xx:ae</td><td>d8:xx:xx:xx:xx:d5(目标MAC地址)</td></tr><tr><td>Sender IP Address</td><td>32</td><td>发送方IP地址</td><td>10.234.172.214</td><td>10.234.0.1</td></tr><tr><td>Target MAC Address</td><td>48</td><td>目标MAC地址</td><td>00:00:00:00:00:00(待填充)</td><td>08:xx:xx:xx:xx:ae(发送方MAC地址)</td></tr><tr><td>Target IP Address</td><td>32</td><td>目标IP地址</td><td>10.234.0.1</td><td>10.234.172.214</td></tr></tbody></table></div><div STYLE="page-break-after: always;"></div><h2 id="十一-TCP协议分析"><a href="#十一-TCP协议分析" class="headerlink" title="十一. TCP协议分析"></a>十一. TCP协议分析</h2><h3 id="11-1-TCP协议"><a href="#11-1-TCP协议" class="headerlink" title="11.1 TCP协议"></a>11.1 TCP协议</h3><p>TCP（Transmission Control Protocol，传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。</p><p>UDP（User Datagram Protocol，用户数据报协议）是一种无连接的、不可靠的、基于数据报的传输层通信协议。</p><div class="table-container"><table><thead><tr><th>特性</th><th>TCP</th><th>UDP</th></tr></thead><tbody><tr><td>连接</td><td>面向连接</td><td>无连接</td></tr><tr><td>可靠性</td><td>可靠传输，使用流量控制和拥塞控制</td><td>不可靠传输，不使用拥塞控制</td></tr><tr><td>连接对象个数</td><td>一对一</td><td>一对一、一对多、多对多</td></tr><tr><td>传输方式</td><td>面向字节流</td><td>面向报文</td></tr><tr><td>首部开销</td><td>20字节~60字节</td><td>8字节</td></tr><tr><td>适用场景</td><td>需要可靠传输的场景，例如文件传输</td><td>传输速度、实时性要求高的场景</td></tr></tbody></table></div><p>TCP协议工作流程：</p><ol><li>建立连接：三次握手<ol><li>发起方向接收方发送SYN报文，请求建立连接</li><li>接收方收到SYN报文后，回复SYN+ACK报文，表示同意建立连接</li><li>发起方收到SYN+ACK报文后，回复ACK报文，表示连接建立成功</li><li>三次握手完成，连接建立成功</li></ol></li><li>数据传输：数据传输阶段</li><li>断开连接：四次挥手<ol><li>发起方向接收方发送FIN报文，请求断开连接</li><li>接收方收到FIN报文后，回复ACK报文，表示收到断开请求</li><li>接收方向发起方发送FIN报文，请求断开连接</li><li>发起方收到FIN报文后，回复ACK报文，表示收到断开请求</li><li>四次挥手完成，连接断开成功</li></ol></li></ol><ul><li>谁想建立或断开连接，谁就是发起方。发起方可以是客户端，也可以是服务器端，</li></ul><h3 id="11-2-TCP协议分析"><a href="#11-2-TCP协议分析" class="headerlink" title="11.2 TCP协议分析"></a>11.2 TCP协议分析</h3><p>以访问10.21.11.21为例，分析TCP协议的工作流程。</p><ol><li>打开Wireshark，在捕获选项中设置过滤器为“http and ip.addr==10.21.11.21”。</li><li>开始抓取数据。</li><li>打开浏览器，在地址栏中输入“10.21.11.21”，回车访问，等待加载完成后关闭浏览器。</li><li>结束抓取数据。</li><li>选中一条访问10.21.11.21的HTTP记录，右键，追踪TPC流。</li><li>观察TPC流是否包含完整的TPC访问过程（三次握手，四次挥手）。</li></ol><p>TCP数据包示例如下：</p><p><img src="https://source.cclmsy.cc/Posts/Course/computer_networks_and_communications_lab/11_1.png" alt="TCP数据包"></p><h4 id="TCP报文格式"><a href="#TCP报文格式" class="headerlink" title="TCP报文格式"></a>TCP报文格式</h4><div class="table-container"><table><thead><tr><th>字段</th><th>长度(bit)</th><th>说明</th></tr></thead><tbody><tr><td>Source Port</td><td>16</td><td>源端口号</td></tr><tr><td>Destination Port</td><td>16</td><td>目的端口号</td></tr><tr><td>Sequence Number</td><td>32</td><td>本报文段发送的数据组的第一个字节的序号</td></tr><tr><td>ACK Number</td><td>32</td><td>确认号，期望收到对方下一个报文段的第一个字节的序号，表明该序号之前的所有数据已经正确无误的收到</td></tr><tr><td>Header Length</td><td>4</td><td>报文头长度，指示TCP头部的长度，即数据区在报文段中的起始偏移值（字节）。偏移值=4*Header Length</td></tr><tr><td>Reserved</td><td>6</td><td>保留字段，未使用</td></tr><tr><td>Flags</td><td>6</td><td>标志位，按顺序为：URG、ACK、PSH、RST、SYN、FIN</td></tr><tr><td>Window Size</td><td>16</td><td>窗口大小，用来告知发送端接受端的缓存大小，以此控制发送端发送数据的速率，从而达到流量控制</td></tr><tr><td>TCP Checksum</td><td>16</td><td>校验和，用于检验TCP头部和数据的完整性</td></tr><tr><td>Urgent Pointer</td><td>16</td><td>紧急指针，TCP 的紧急方式是发送端向另一端发送紧急数据的一种方式</td></tr><tr><td>Options</td><td>0-320</td><td>可选字段，用于扩展TCP头部。最常见的可选字段是最长报文大小，又称为MSS（Maximum Segment Size）（在第一个TCP报文）。选项长度不一定是32位的整数倍，所以要加填充位</td></tr></tbody></table></div><h4 id="TCP三次握手"><a href="#TCP三次握手" class="headerlink" title="TCP三次握手"></a>TCP三次握手</h4><div class="table-container"><table><thead><tr><th>字段</th><th>第一次握手[SYN]</th><th>第二次握手[SYN, ACK]</th><th>第三次握手[ACK]</th></tr></thead><tbody><tr><td>Source Port</td><td>0xcf75(53109)</td><td>0x0050(80)</td><td>0xcf75(53109)</td></tr><tr><td>Destination Port</td><td>0x0050(80)</td><td>0xcf75(53109)</td><td>0x0050(80)</td></tr><tr><td>Sequence Number</td><td>0xc01d24d3(0)</td><td>0xabbe9051(0)</td><td>0xc01d24d4(1)</td></tr><tr><td>ACK Number</td><td>0x00000000(0)</td><td>0xc01d24d4(1)</td><td>0xabbe9052(1)</td></tr><tr><td>Header Length</td><td>0x8(32)</td><td>0x8(32)</td><td>0x5(20)</td></tr><tr><td>Reserved</td><td>0x00</td><td>0x00</td><td>0x00</td></tr><tr><td>Flags</td><td>0x002(SYN)</td><td>0x012(SYN, ACK)</td><td>0x010(ACK)</td></tr><tr><td>Window Size</td><td>0xfaf0(64240)</td><td>0x4000(16384)</td><td>0x0201(513)</td></tr></tbody></table></div><h4 id="TCP四次挥手"><a href="#TCP四次挥手" class="headerlink" title="TCP四次挥手"></a>TCP四次挥手</h4><div class="table-container"><table><thead><tr><th>字段</th><th>第一次挥手[FIN, ACK]</th><th>第二次挥手[ACK]</th><th>第三次挥手[FIN, ACK]</th><th>第四次挥手[ACK]</th></tr></thead><tbody><tr><td>Source Port</td><td>0x0050(80)</td><td>0xcf75(53109)</td><td>0x0050(80)</td><td>0xcf75(53109)</td></tr><tr><td>Destination Port</td><td>0xcf75(53109)</td><td>0x0050(80)</td><td>0xcf75(53109)</td><td>0x0050(80)</td></tr><tr><td>Sequence Number</td><td>0xabbe9194(323)</td><td>0xc01d26e6(531)</td><td>0xc01d26e6(531)</td><td>0xabbe9195(324)</td></tr><tr><td>ACK Number</td><td>0xc01d26e6(531)</td><td>0xabbe9195(324)</td><td>0xabbe9195(324)</td><td>0xc01d26e7(532)</td></tr><tr><td>Header Length</td><td>0x5(20)</td><td>0x5(20)</td><td>0x5(20)</td><td>0x5(20)</td></tr><tr><td>Reserved</td><td>0x00</td><td>0x00</td><td>0x00</td><td>0x00</td></tr><tr><td>Flags</td><td>0x011(FIN, ACK)</td><td>0x010(ACK)</td><td>0x011(FIN, ACK)</td><td>0x010(ACK)</td></tr><tr><td>Window Size</td><td>0xfeed(65005)</td><td>0x0200(513)</td><td>0x0200(513)</td><td>0xfeed(65005)</td></tr></tbody></table></div><div STYLE="page-break-after: always;"></div><h2 id="附录-实验涉及命令行汇总"><a href="#附录-实验涉及命令行汇总" class="headerlink" title="附录. 实验涉及命令行汇总"></a>附录. 实验涉及命令行汇总</h2><h3 id="1-PC机-终端"><a href="#1-PC机-终端" class="headerlink" title="1. PC机 终端"></a>1. PC机 终端</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ipconfig /all # 查看IP配置</span><br><span class="line">ping &lt;IP地址&gt; # 测试连通性</span><br></pre></td></tr></table></figure><h3 id="2-交换机Switch-命令行"><a href="#2-交换机Switch-命令行" class="headerlink" title="2. 交换机Switch 命令行"></a>2. 交换机Switch 命令行</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Switch&gt;enable # 进入特权模式</span><br><span class="line">Switch#configure terminal # 进入全局配置模式</span><br><span class="line">Switch(config)#hostname Switch # 设置交换机主机名</span><br><span class="line">Switch(config)#exit # 退回上一级模式</span><br><span class="line">Switch(config)#end # 退回特权模式</span><br><span class="line"></span><br><span class="line"># 显示</span><br><span class="line">Switch#show vlan brief # 显示VLAN信息</span><br><span class="line">Switch#show interfaces # 显示接口信息</span><br><span class="line">Switch#show interfaces trunk # 显示trunk接口信息</span><br><span class="line"></span><br><span class="line"># VLAN</span><br><span class="line">Switch(config)#vlan &lt;VLAN号&gt; # 创建VLAN并进入VLAN配置模式</span><br><span class="line">Switch(config)#interface vlan &lt;VLAN号&gt; # 进入VLAN配置模式</span><br><span class="line"></span><br><span class="line"># 接口</span><br><span class="line">Switch(config)#interface &lt;接口号&gt; # 进入接口配置模式</span><br><span class="line">Switch(config)#interface &lt;接口号&gt;-&lt;接口号&gt; # 进入接口范围配置模式</span><br><span class="line">Switch(config-if)#switchport mode access # 设置接口为access</span><br><span class="line">Switch(config-if)#switchport access vlan &lt;VLAN号&gt; # 设置access接口所属VLAN</span><br><span class="line">Switch(config-if)#switchport mode trunk # 设置接口为trunk</span><br><span class="line"></span><br><span class="line"># MAC地址绑定</span><br><span class="line">Switch#show mac-address-table # 查看MAC地址表</span><br><span class="line">Switch#show mac-address-table aging-time # 查看MAC地址表老化时间</span><br><span class="line">Switch(config-if)#mac-address-table static &lt;MAC地址&gt; vlan &lt;VLAN号&gt; interface &lt;接口号&gt; # 设置静态MAC地址绑定</span><br><span class="line">Switch(config-if)#no mac-address-table static &lt;MAC地址&gt; vlan &lt;VLAN号&gt; interface &lt;接口号&gt; # 删除静态MAC地址绑定</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3-路由器Router-命令行"><a href="#3-路由器Router-命令行" class="headerlink" title="3. 路由器Router 命令行"></a>3. 路由器Router 命令行</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">Router&gt;enable # 进入特权模式</span><br><span class="line">Router#configure terminal # 进入全局配置模式</span><br><span class="line">Router(config)#hostname Router # 设置路由器主机名</span><br><span class="line">Router(config)#exit # 退回上一级模式</span><br><span class="line">Router(config)#end # 退回特权模式</span><br><span class="line"></span><br><span class="line"># 显示</span><br><span class="line">Router#show ip route # 显示路由表</span><br><span class="line">Router#show ip interface brief # 显示接口IP信息</span><br><span class="line">Router#show ip interface &lt;接口号&gt; # 显示指定接口信息</span><br><span class="line">Router#show ip nat translations # 显示NAT转换表</span><br><span class="line"></span><br><span class="line"># 接口</span><br><span class="line">Router(config)#interface &lt;接口号&gt; # 进入接口配置模式</span><br><span class="line">Router(config-if)#no shutdown # 激活接口</span><br><span class="line">Router(config-if)#ip address &lt;IP地址&gt; &lt;子网掩码&gt; # 设置接口IP地址</span><br><span class="line"></span><br><span class="line"># VLAN</span><br><span class="line">Router(config)#interface &lt;接口号&gt;.&lt;子接口号&gt; # 进入子接口配置模式（接口需激活）</span><br><span class="line">Router(config-subif)#encapsulation dot1Q &lt;VLAN号&gt; # 封装VLAN协议（802.1Q），设置子接口所属VLAN</span><br><span class="line">Router(config-subif)#ip address &lt;IP地址&gt; &lt;子网掩码&gt; # 设置子接口IP地址</span><br><span class="line"></span><br><span class="line"># 路由</span><br><span class="line">Router(config)#ip route &lt;目的网络&gt; &lt;子网掩码&gt; &lt;下一跳地址&gt; # 设置静态路由</span><br><span class="line"></span><br><span class="line">Router(config)#router rip # 进入RIP协议配置模式</span><br><span class="line">Router(config-router)#version &lt;版本号&gt; # 设置RIP版本</span><br><span class="line">Router(config-router)#network &lt;网络地址&gt; # 设置RIP协议的网络地址</span><br><span class="line"></span><br><span class="line"># NAT</span><br><span class="line">Router(config)#ip nat inside source static tcp &lt;内网IP地址&gt; &lt;内网端口&gt; &lt;外网IP地址&gt; &lt;外网端口&gt; # 设置静态NAT</span><br><span class="line"></span><br><span class="line">Router(config)#access-list &lt;访问控制列表号&gt; permit any # 创建访问控制列表</span><br><span class="line">Router(config)#ip nat pool &lt;IP地址池名称&gt; &lt;起始地址&gt; &lt;结束地址&gt; netmask &lt;子网掩码&gt; # 创建IP地址池</span><br><span class="line">Router(config)#ip nat inside source list &lt;访问控制列表号&gt; pool &lt;IP地址池名称&gt; # 设置动态NAT</span><br><span class="line">Router(config)#ip nat inside source list &lt;访问控制列表号&gt; pool &lt;IP地址池名称&gt; overload # 设置NAPT</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">交换机与路由器的爱恨情仇（？</summary>
    
    
    
    <category term="课程笔记" scheme="https://www.cclmsy.cc/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="课程笔记" scheme="https://www.cclmsy.cc/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>题解|2024暑期杭电多校10</title>
    <link href="https://www.cclmsy.cc/posts/ACM_2024Summer_HDU10.html"/>
    <id>https://www.cclmsy.cc/posts/ACM_2024Summer_HDU10.html</id>
    <published>2024-08-17T16:00:00.000Z</published>
    <updated>2025-03-09T02:54:24.916Z</updated>
    
    <content type="html"><![CDATA[<p>比赛题单：<a href="https://acm.hdu.edu.cn/search.php?field=problem&amp;key=2024%A1%B0%B6%A4%B0%D2%B1%E0%B3%CC%A1%B1%D6%D0%B9%FA%B4%F3%D1%A7%C9%FA%CB%E3%B7%A8%C9%E8%BC%C6%B3%AC%BC%B6%C1%AA%C8%FC%A3%A810%A3%A9&amp;source=1&amp;searchmode=source">2024“钉耙编程”中国大学生算法设计超级联赛（10）</a></p><h1 id="1008-HDU7548-SunBian"><a href="#1008-HDU7548-SunBian" class="headerlink" title="(1008)HDU7548.SunBian"></a>(1008)HDU7548.SunBian</h1><h2 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h2><p>有排成环形的 $n$ 个横着的笋，Alice 和 Bob 轮流执行如下操作，Alice 先手：</p><ul><li>选择 [1,k] 个连续的横着的笋，把它们变成竖着的</li></ul><p>不能操作者输。</p><p>给定 $n,k$ ，求谁会赢。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><ul><li>当 $k=1$ 时，根据奇偶性判断赢家</li><li>当 $k\ge n$ 时，先手直接将笋全部竖置，必胜</li><li>其余情况下，后手每次都可以尽可能保证剩余区域数为偶数，最终必胜</li></ul><h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,k;cin &gt;&gt; n &gt;&gt; k;</span><br><span class="line">    <span class="keyword">if</span>(n&lt;=k) cout &lt;&lt; <span class="string">&#x27;A&#x27;</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(k==<span class="number">1</span>) cout &lt;&lt; (n%<span class="number">2</span>?<span class="string">&#x27;A&#x27;</span>:<span class="string">&#x27;B&#x27;</span>);</span><br><span class="line">    <span class="keyword">else</span> cout &lt;&lt; <span class="string">&#x27;B&#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="1009-HDU7549-不基本子串结构"><a href="#1009-HDU7549-不基本子串结构" class="headerlink" title="(1009)HDU7549.不基本子串结构"></a>(1009)HDU7549.不基本子串结构</h1><h2 id="题意-1"><a href="#题意-1" class="headerlink" title="题意"></a>题意</h2><p>给定2个字符串 $a,b$ ，找到一个最小长度的字符串 $c$ ，使得 $a$ 和 $b$ 在 $c$ 中出现的次数相等且不为0，输出最小长度。</p><h2 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h2><p>分类讨论，不妨假设 $len(a)\le len(b)$</p><ul><li>若 $a$ 在 $b$ 中出现的次数大于 $1$ ，则不存在满足条件的 $c$</li><li>若 $a$ 在 $b$ 中出现的次数为 $1$ ，则 $c=b$ ，输出 $len(b)$</li><li>若 $a$ 在 $b$ 中没有出现：<ul><li>记 $l_1$ 为 最大满足 $a[0:l]=b[len(b)-l:len(b)]$ 的 $l$</li><li>记 $l_2$ 为 最大满足 $a[len(a)-l:len(a)]=b[0:l]$ 的 $l$</li><li>答案为 $len(a)+len(b)-max(l_1,l_2)$</li></ul></li></ul><p>可以用字符串哈希进行检查和计数。</p><h2 id="参考代码-1"><a href="#参考代码-1" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string s1,s2;</span><br><span class="line">    cin &gt;&gt; s1 &gt;&gt; s2;</span><br><span class="line">    <span class="keyword">if</span>(s1.<span class="built_in">length</span>()&lt;s2.<span class="built_in">length</span>()) <span class="built_in">swap</span>(s1,s2);</span><br><span class="line">    <span class="function">strHash <span class="title">h1</span><span class="params">(s1)</span>,<span class="title">h2</span><span class="params">(s2)</span></span>;</span><br><span class="line">    ll n=s1.<span class="built_in">length</span>(),m=s2.<span class="built_in">length</span>();</span><br><span class="line">    ll cnt=h1.<span class="built_in">count</span>(h2);</span><br><span class="line">    <span class="keyword">if</span>(cnt&gt;<span class="number">1</span>)&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;-1&quot;</span> &lt;&lt; endl;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(cnt==<span class="number">1</span>)&#123;</span><br><span class="line">        cout &lt;&lt; n &lt;&lt; endl;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ll ans=m+n;</span><br><span class="line">    <span class="built_in">FORLL_rev</span>(len,m<span class="number">-1</span>,<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(h1.<span class="built_in">findz</span>(n-len+<span class="number">1</span>,n)==h2.<span class="built_in">findz</span>(<span class="number">1</span>,len))&#123;</span><br><span class="line">            <span class="built_in">chmin</span>(ans,n+m-len);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">FORLL_rev</span>(len,m<span class="number">-1</span>,<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(h1.<span class="built_in">findf</span>(<span class="number">1</span>,len)==h2.<span class="built_in">findf</span>(m-len+<span class="number">1</span>,m))&#123;</span><br><span class="line">            <span class="built_in">chmin</span>(ans,n+m-len);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="（1011）HDU7551-NOI2024"><a href="#（1011）HDU7551-NOI2024" class="headerlink" title="（1011）HDU7551.NOI2024"></a>（1011）HDU7551.NOI2024</h1><h2 id="题意-2"><a href="#题意-2" class="headerlink" title="题意"></a>题意</h2><p>$m$ 名选手进行 $n$ 场比赛，排名定义为分数严格大于你的人数+1。<br>第 $i$ 场比赛的分数上限为 $b_i$ ，你的排名为 $a_i$ 。<br>最终按照每场比赛的总分排名，前 $k$ 名选手将获得金牌。<br>问在给定条件下不管怎么比赛，是否一定能获得金牌。</p><h2 id="解题思路-2"><a href="#解题思路-2" class="headerlink" title="解题思路"></a>解题思路</h2><p>用最坏情况考虑：你始终为 $0$ 分，在你前面的选手都有分数。<br>最终最坏排名为 $\min(\sum_{i=1}^{n}a_i,m)$ 。</p><h2 id="参考代码-2"><a href="#参考代码-2" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,m,k;cin &gt;&gt; n &gt;&gt; m &gt;&gt; k;</span><br><span class="line">    <span class="built_in">create_vec</span>(a,n);</span><br><span class="line">    <span class="built_in">create_vec</span>(b,n);</span><br><span class="line">    ll cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">0</span>,n<span class="number">-1</span>) cnt+=<span class="built_in">max</span>(<span class="number">0ll</span>,a[i]<span class="number">-1</span>);</span><br><span class="line">    <span class="built_in">chmin</span>(cnt,m<span class="number">-1</span>);</span><br><span class="line">    <span class="keyword">if</span>(cnt&gt;=k) cout &lt;&lt; <span class="string">&quot;NO&quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">else</span> cout &lt;&lt; <span class="string">&quot;YES&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">题解|2024暑期杭电多校10</summary>
    
    
    
    <category term="竞赛笔记" scheme="https://www.cclmsy.cc/categories/%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="2024暑期杭电多校" scheme="https://www.cclmsy.cc/tags/2024%E6%9A%91%E6%9C%9F%E6%9D%AD%E7%94%B5%E5%A4%9A%E6%A0%A1/"/>
    
  </entry>
  
  <entry>
    <title>题解|2024暑期杭电多校09</title>
    <link href="https://www.cclmsy.cc/posts/ACM_2024Summer_HDU09.html"/>
    <id>https://www.cclmsy.cc/posts/ACM_2024Summer_HDU09.html</id>
    <published>2024-08-15T16:00:00.000Z</published>
    <updated>2025-03-09T02:54:24.906Z</updated>
    
    <content type="html"><![CDATA[<p>比赛题单：<a href="https://acm.hdu.edu.cn/search.php?field=problem&amp;key=2024%A1%B0%B6%A4%B0%D2%B1%E0%B3%CC%A1%B1%D6%D0%B9%FA%B4%F3%D1%A7%C9%FA%CB%E3%B7%A8%C9%E8%BC%C6%B3%AC%BC%B6%C1%AA%C8%FC%A3%A89%A3%A9&amp;source=1&amp;searchmode=source">2024“钉耙编程”中国大学生算法设计超级联赛（9）</a></p><h1 id="1005-HDU7533-怪物猎人"><a href="#1005-HDU7533-怪物猎人" class="headerlink" title="(1005)HDU7533.怪物猎人"></a>(1005)HDU7533.怪物猎人</h1><h2 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h2><p>攻击生命值为 $k$ 的怪物，每回合可选择造成 $x$ 或 $y$ 点伤害。<br>分别回答是否有一种攻击序列，可以恰好在第奇数回合/第偶数回合击杀怪物。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>最边缘的情况即为全用 $x$ 和 全用 $y$ ，分别需要 $\lceil \frac{k}{x}\rceil$ 和 $\lceil \frac{k}{y}\rceil$ 回合。</p><p>这两个回合数相等，只有一种可能，否则都可以。</p><h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,a,b;cin &gt;&gt; n &gt;&gt; a &gt;&gt; b;</span><br><span class="line">    ll ta,tb;</span><br><span class="line">    ta=(n+a<span class="number">-1</span>)/a;</span><br><span class="line">    tb=(n+b<span class="number">-1</span>)/b;</span><br><span class="line">    <span class="keyword">if</span>(ta==tb)&#123;</span><br><span class="line">        <span class="keyword">if</span>(ta%<span class="number">2</span>) cout &lt;&lt; <span class="string">&quot;Yes&quot;</span> &lt;&lt; endl &lt;&lt; <span class="string">&quot;No&quot;</span> &lt;&lt; endl;</span><br><span class="line">        <span class="keyword">else</span> cout &lt;&lt; <span class="string">&quot;No&quot;</span> &lt;&lt; endl &lt;&lt; <span class="string">&quot;Yes&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Yes&quot;</span> &lt;&lt; endl &lt;&lt; <span class="string">&quot;Yes&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="1007-HDU7535-小猫钓鱼"><a href="#1007-HDU7535-小猫钓鱼" class="headerlink" title="(1007)HDU7535.小猫钓鱼"></a>(1007)HDU7535.小猫钓鱼</h1><h2 id="题意-1"><a href="#题意-1" class="headerlink" title="题意"></a>题意</h2><p>有 $2n$ 张牌，点数 $1\sim n$ 各出现2次。<br>两人各分到 $n$ 张牌，已知每个人手中牌的点数。<br>两人轮流放牌到牌堆顶，若牌堆中此前已经有相同点数的牌，则将这两张牌以及中间的所有牌拿回手中。<br>先打完的人输。</p><h2 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h2><p>称手牌中成对的牌为双牌，单独的牌为单牌。</p><p>当自己打出单牌时，对方必定能打出一样的单牌收回，且先后手不变，这样是不优的。<br>因此，贪心的打法是先手打双牌，后手打双牌，先手打上次打的双牌。<br>这样的结果是：后手失去一个双牌，交换先后手。</p><p>观察到两人手牌中的双牌的数量相同，因此有双牌的情况下，后手先失去所有双牌，先手获胜。<br>否则，后手可以稳定收取先手打出的牌，后手获胜。</p><h2 id="参考代码-1"><a href="#参考代码-1" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n;cin &gt;&gt; n;</span><br><span class="line">    map&lt;ll,<span class="type">int</span>&gt; mp;</span><br><span class="line">    ll t;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">        cin &gt;&gt; t;</span><br><span class="line">        mp[t]++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">1</span>,n) cin &gt;&gt; t;</span><br><span class="line">    <span class="keyword">if</span>(mp.<span class="built_in">size</span>()&lt;n) cout &lt;&lt; <span class="string">&quot;shuishui&quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">else</span> cout &lt;&lt; <span class="string">&quot;sha7dow&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">题解|2024暑期杭电多校09</summary>
    
    
    
    <category term="竞赛笔记" scheme="https://www.cclmsy.cc/categories/%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="2024暑期杭电多校" scheme="https://www.cclmsy.cc/tags/2024%E6%9A%91%E6%9C%9F%E6%9D%AD%E7%94%B5%E5%A4%9A%E6%A0%A1/"/>
    
  </entry>
  
  <entry>
    <title>题解|2024暑期牛客多校10</title>
    <link href="https://www.cclmsy.cc/posts/ACM_2024Summer_NCD10.html"/>
    <id>https://www.cclmsy.cc/posts/ACM_2024Summer_NCD10.html</id>
    <published>2024-08-14T16:00:00.000Z</published>
    <updated>2025-03-09T02:54:24.906Z</updated>
    
    <content type="html"><![CDATA[<p>比赛链接：<a href="https://ac.nowcoder.com/acm/contest/81605">2024牛客暑期多校训练营10</a></p><h1 id="A-Surrender-to-My-Will"><a href="#A-Surrender-to-My-Will" class="headerlink" title="A.Surrender to My Will"></a>A.Surrender to My Will</h1><h2 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h2><p>给定一个长度为 $5$ 的字符串，<code>Y</code>代表投降，<code>N</code>代表不投降，<code>-</code>代表未投票。<br>投降人数不小于4人即可投降。<br>问字符串表示的投降结果。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>计数判断</p><h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string s;cin &gt;&gt; s;</span><br><span class="line">    map&lt;<span class="type">char</span>,<span class="type">int</span>&gt; mp;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> c:s) mp[c]++;</span><br><span class="line">    <span class="keyword">if</span>(mp[<span class="string">&#x27;Y&#x27;</span>]&gt;=<span class="number">4</span>) &#123;cout &lt;&lt; <span class="number">1</span> &lt;&lt; endl;<span class="keyword">return</span>;&#125;</span><br><span class="line">    <span class="keyword">if</span>(mp[<span class="string">&#x27;N&#x27;</span>]&gt;=<span class="number">2</span>) &#123;cout &lt;&lt; <span class="number">-1</span> &lt;&lt; endl;<span class="keyword">return</span>;&#125;</span><br><span class="line">    cout &lt;&lt; <span class="number">0</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="B-std-pair"><a href="#B-std-pair" class="headerlink" title="B.std::pair"></a>B.std::pair</h1><h2 id="题意-1"><a href="#题意-1" class="headerlink" title="题意"></a>题意</h2><p>基础合法类型<code>int</code>和<code>double</code>。<br>对于任意2个合法类型<code>A</code>和<code>B</code>，<code>pair&lt;A,B&gt;</code>是合法的，可以通过<code>.first</code>和<code>.second</code>分别访问<code>A</code>和<code>B</code>成员。<br>声明变量的格式为<code>pair&lt;A,B&gt; 变量名;</code>。</p><p>给定 $n$ 个字符串表示变量声明，变量名不重复。<br>再给定 $q$ 个字符串表示访问变量，返回被查询的成员类型。</p><h2 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h2><p>模拟题，对每个变量类型构建二叉树，查询在二叉树上进行。<br>具体实现方式可以参考代码。</p><h2 id="参考代码-1"><a href="#参考代码-1" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">pll v[N]; <span class="comment">//pair的树，x表示左子树，y表示右子树</span></span><br><span class="line">map&lt;string,ll&gt; dict; <span class="comment">//变量二叉树的根所在下标</span></span><br><span class="line">ll cnt=<span class="number">0</span>;</span><br><span class="line"><span class="function">pll <span class="title">parse_type</span><span class="params">(string s)</span></span>&#123; <span class="comment">//解析类型，构建树</span></span><br><span class="line">    <span class="keyword">if</span>(s==<span class="string">&quot;int&quot;</span>) <span class="keyword">return</span> &#123;<span class="number">-1</span>,<span class="number">-1</span>&#125;; <span class="comment">//int</span></span><br><span class="line">    <span class="keyword">if</span>(s==<span class="string">&quot;double&quot;</span>) <span class="keyword">return</span> &#123;<span class="number">-2</span>,<span class="number">-2</span>&#125;; <span class="comment">//double</span></span><br><span class="line">    s=s.<span class="built_in">substr</span>(<span class="number">5</span>,s.<span class="built_in">length</span>()<span class="number">-6</span>); </span><br><span class="line">    ll cntp=<span class="number">0</span>,cntd=<span class="number">0</span>,pos;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">0</span>,s.<span class="built_in">length</span>()<span class="number">-1</span>)&#123; <span class="comment">//找到当前pair对应的逗号位置pos</span></span><br><span class="line">        <span class="keyword">if</span>(s[i]==<span class="string">&#x27;p&#x27;</span>) cntp++;</span><br><span class="line">        <span class="keyword">if</span>(s[i]==<span class="string">&#x27;,&#x27;</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(cntp==cntd) &#123;pos=i;<span class="keyword">break</span>;&#125; </span><br><span class="line">            cntd++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    string ltype=s.<span class="built_in">substr</span>(<span class="number">0</span>,pos),rtype=s.<span class="built_in">substr</span>(pos+<span class="number">1</span>,s.<span class="built_in">length</span>()-pos<span class="number">-1</span>);</span><br><span class="line">    ll lidx=cnt++,ridx=cnt++;</span><br><span class="line">    v[lidx]=<span class="built_in">parse_type</span>(ltype);</span><br><span class="line">    v[ridx]=<span class="built_in">parse_type</span>(rtype);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">make_pair</span>(lidx,ridx);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">string <span class="title">get_type</span><span class="params">(ll idx)</span></span>&#123; <span class="comment">//重组类型字符串</span></span><br><span class="line">    <span class="keyword">if</span>(v[idx].x==<span class="number">-1</span>) <span class="keyword">return</span> <span class="string">&quot;int&quot;</span>;</span><br><span class="line">    <span class="keyword">if</span>(v[idx].x==<span class="number">-2</span>) <span class="keyword">return</span> <span class="string">&quot;double&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;pair&lt;&quot;</span>+<span class="built_in">get_type</span>(v[idx].x)+<span class="string">&quot;,&quot;</span>+<span class="built_in">get_type</span>(v[idx].y)+<span class="string">&quot;&gt;&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,q;cin &gt;&gt; n &gt;&gt; q;</span><br><span class="line">    string type,name;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">        cin &gt;&gt; type &gt;&gt; name;</span><br><span class="line">        name.<span class="built_in">pop_back</span>();</span><br><span class="line">        dict[name]=cnt++;</span><br><span class="line">        v[dict[name]]=<span class="built_in">parse_type</span>(type);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">1</span>,q)&#123;</span><br><span class="line">        cin &gt;&gt; name;</span><br><span class="line">        ll j=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(j&lt;name.<span class="built_in">length</span>()&amp;&amp;name[j]!=<span class="string">&#x27;.&#x27;</span>) j++;</span><br><span class="line">        string main_name=name.<span class="built_in">substr</span>(<span class="number">0</span>,j); j++;</span><br><span class="line">        ll idx=dict[main_name];</span><br><span class="line">        <span class="keyword">while</span>(j&lt;name.<span class="built_in">length</span>())&#123;</span><br><span class="line">            <span class="keyword">if</span>(name[j]==<span class="string">&#x27;f&#x27;</span>)&#123; <span class="comment">//first</span></span><br><span class="line">                idx=v[idx].x;</span><br><span class="line">                j+=<span class="number">6</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(name[j]==<span class="string">&#x27;s&#x27;</span>)&#123; <span class="comment">//second</span></span><br><span class="line">                idx=v[idx].y;</span><br><span class="line">                j+=<span class="number">7</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; <span class="built_in">get_type</span>(idx) &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="F-Collinear-Exception"><a href="#F-Collinear-Exception" class="headerlink" title="F.Collinear Exception"></a>F.Collinear Exception</h1><h2 id="题意-2"><a href="#题意-2" class="headerlink" title="题意"></a>题意</h2><p>有一 $n\times n$ 的点阵列，每个点的坐标为 $(i,j)$ ，$1\leq i,j\leq n$ 。<br>按给定序列对点进行标记，要求标记后不能存在三个被标记点共线，否则标记失败。<br>按给定序列顺序输出每个点是否标记成功。</p><h2 id="解题思路-2"><a href="#解题思路-2" class="headerlink" title="解题思路"></a>解题思路</h2><p>每当加入一个新的点时，和已有的点连线，标记线上的所有点。<br>被标记的点不能再被加入。</p><h2 id="参考代码-2"><a href="#参考代码-2" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n;cin &gt;&gt; n;</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">vis</span>(n+<span class="number">1</span>,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n+<span class="number">1</span>,<span class="number">0</span>));</span><br><span class="line">    vector&lt;pll&gt; points;</span><br><span class="line">    ll ub=n*n,xx,yy,ans=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">1</span>,ub)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; xx &gt;&gt; yy;</span><br><span class="line">        <span class="keyword">if</span>(vis[xx][yy])&#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span>[tx,ty]:points)&#123; <span class="comment">//标记线上的点</span></span><br><span class="line">            <span class="keyword">if</span>(xx==tx)&#123;</span><br><span class="line">                <span class="built_in">FORLL</span>(j,<span class="number">1</span>,n) vis[xx][j]=<span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                ll dx=xx-tx,dy=yy-ty;</span><br><span class="line">                ll g=__gcd(dx,dy);</span><br><span class="line">                dx/=g;dy/=g;</span><br><span class="line">                ll ttx=tx,tty=ty;</span><br><span class="line">                <span class="keyword">while</span>(ttx&lt;=n&amp;&amp;ttx&gt;=<span class="number">1</span>&amp;&amp;tty&lt;=n&amp;&amp;tty&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">                    vis[ttx][tty]=<span class="number">1</span>;</span><br><span class="line">                    ttx+=dx; tty+=dy;</span><br><span class="line">                &#125;</span><br><span class="line">                ttx=tx; tty=ty;</span><br><span class="line">                <span class="keyword">while</span>(ttx&lt;=n&amp;&amp;ttx&gt;=<span class="number">1</span>&amp;&amp;tty&lt;=n&amp;&amp;tty&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">                    vis[ttx][tty]=<span class="number">1</span>;</span><br><span class="line">                    ttx-=dx; tty-=dy;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        points.<span class="built_in">emplace_back</span>(xx,yy);</span><br><span class="line">        vis[xx][yy]=<span class="number">1</span>;</span><br><span class="line">        cout &lt;&lt; <span class="string">&#x27;1&#x27;</span>;</span><br><span class="line">    &#125;cout &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="H-All-in-at-the-Pre-flop"><a href="#H-All-in-at-the-Pre-flop" class="headerlink" title="H.All-in at the Pre-flop"></a>H.All-in at the Pre-flop</h1><h2 id="题意-3"><a href="#题意-3" class="headerlink" title="题意"></a>题意</h2><p>两名玩家初始分别有 $a,b$ 的筹码，每轮游戏各有 $\frac{1}{2}$ 的概率获胜。<br>假设当前轮两名玩家分别有 $x,y$ 的筹码，那么输家需要付给赢家 $\min(x,y)$ 的筹码。<br>问两名玩家获胜的概率。</p><h2 id="解题思路-3"><a href="#解题思路-3" class="headerlink" title="解题思路"></a>解题思路</h2><p>打个表，观察到答案是 $\frac{a}{a+b},\frac{b}{a+b}$ 。</p><h2 id="参考代码-3"><a href="#参考代码-3" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll a,b;cin &gt;&gt; a &gt;&gt; b;</span><br><span class="line">    ll c=<span class="built_in">inv</span>(<span class="built_in">add</span>(a,b));</span><br><span class="line">    cout &lt;&lt; <span class="built_in">mul</span>(a,c) &lt;&lt; <span class="string">&#x27; &#x27;</span> &lt;&lt; <span class="built_in">mul</span>(b,c) &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">题解|2024暑期牛客多校10</summary>
    
    
    
    <category term="竞赛笔记" scheme="https://www.cclmsy.cc/categories/%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="2024暑期牛客多校" scheme="https://www.cclmsy.cc/tags/2024%E6%9A%91%E6%9C%9F%E7%89%9B%E5%AE%A2%E5%A4%9A%E6%A0%A1/"/>
    
  </entry>
  
  <entry>
    <title>题解|2024暑期牛客多校09</title>
    <link href="https://www.cclmsy.cc/posts/ACM_2024Summer_NCD09.html"/>
    <id>https://www.cclmsy.cc/posts/ACM_2024Summer_NCD09.html</id>
    <published>2024-08-12T16:00:00.000Z</published>
    <updated>2025-03-09T02:54:24.892Z</updated>
    
    <content type="html"><![CDATA[<p>比赛链接：<a href="https://ac.nowcoder.com/acm/contest/81604">2024牛客暑期多校训练营9</a></p><h1 id="A-Image-Scaling"><a href="#A-Image-Scaling" class="headerlink" title="A.Image Scaling"></a>A.Image Scaling</h1><h2 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h2><p>给定由 <code>.</code> 和 <code>x</code> 组成的 $n\times m$ 的 $n\times m$ 矩阵，$x$ 部分是一个子矩阵。<br>提取并在长宽比不变的情况下，将子矩阵尽可能缩小并输出。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>模拟，缩小到 $1/gcd$</p><h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,m;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    ll fl=<span class="number">0</span>,st=<span class="number">-1</span>;</span><br><span class="line">    ll nn=<span class="number">-1</span>,mm,idx;</span><br><span class="line">    string s;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">0</span>,n<span class="number">-1</span>)&#123;</span><br><span class="line">        cin &gt;&gt; s;</span><br><span class="line">        <span class="keyword">if</span>(fl) <span class="keyword">if</span>(s[idx]!=<span class="string">&#x27;x&#x27;</span>) &#123;nn=i-st;<span class="keyword">break</span>;&#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">FORLL</span>(j,<span class="number">0</span>,m<span class="number">-1</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(s[j]==<span class="string">&#x27;x&#x27;</span>)&#123;</span><br><span class="line">                    fl=<span class="number">1</span>; st=i; idx=j;</span><br><span class="line">                    ll t=j;</span><br><span class="line">                    <span class="keyword">while</span>(t&lt;m&amp;&amp;s[t]==<span class="string">&#x27;x&#x27;</span>) t++;</span><br><span class="line">                    mm=t-j;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(nn==<span class="number">-1</span>) nn=n-st;</span><br><span class="line">    ll g=__gcd(nn,mm);</span><br><span class="line">    nn/=g; mm/=g;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">0</span>,nn<span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="built_in">FORLL</span>(j,<span class="number">0</span>,mm<span class="number">-1</span>)&#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&#x27;x&#x27;</span>;</span><br><span class="line">        &#125;cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="K-Kill-The-Monsters"><a href="#K-Kill-The-Monsters" class="headerlink" title="K.Kill The Monsters"></a>K.Kill The Monsters</h1><h2 id="题意-1"><a href="#题意-1" class="headerlink" title="题意"></a>题意</h2><p>$n$ 个怪兽，第 $i$ 个怪兽的体力为 $a_i$ 。</p><p>每次可以进行一种操作：</p><ol><li>所有怪兽体力 $-1$</li><li>选择一个怪兽 $i$ 使得 $a_i\leftarrow \lfloor \dfrac{a_i}{k} \rfloor$</li></ol><p>问最少多少次操作可以使所有怪兽的体力都为 $0$ 。</p><h2 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h2><p>贪心的，先连续对最大体力的怪兽进行第二种操作，再进行第一种操作。<br>用优先队列维护最大体力。<br>记当前已经操作了 $cnt$ 次，用 $a_{max}+cnt$ 维护最小操作次数。</p><h2 id="参考代码-1"><a href="#参考代码-1" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,k,t;cin &gt;&gt; n &gt;&gt; k;</span><br><span class="line">    priority_queue&lt;ll&gt; pq;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">1</span>,n) &#123;cin &gt;&gt; t;pq.<span class="built_in">push</span>(t);&#125;</span><br><span class="line">    ll cur=<span class="number">0</span>,ans=pq.<span class="built_in">top</span>();</span><br><span class="line">    <span class="keyword">if</span>(k==<span class="number">1</span>) &#123;cout &lt;&lt; ans &lt;&lt; endl;<span class="keyword">return</span>;&#125;</span><br><span class="line">    <span class="keyword">while</span>(pq.<span class="built_in">top</span>()&gt;<span class="number">1</span>)&#123;</span><br><span class="line">        t=pq.<span class="built_in">top</span>();pq.<span class="built_in">pop</span>();</span><br><span class="line">        t/=k; cur++;</span><br><span class="line">        pq.<span class="built_in">push</span>(t);</span><br><span class="line">        <span class="built_in">chmin</span>(ans,pq.<span class="built_in">top</span>()+cur);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans  &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">题解|2024暑期牛客多校09</summary>
    
    
    
    <category term="竞赛笔记" scheme="https://www.cclmsy.cc/categories/%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="2024暑期牛客多校" scheme="https://www.cclmsy.cc/tags/2024%E6%9A%91%E6%9C%9F%E7%89%9B%E5%AE%A2%E5%A4%9A%E6%A0%A1/"/>
    
  </entry>
  
  <entry>
    <title>题解|2024暑期杭电多校08</title>
    <link href="https://www.cclmsy.cc/posts/ACM_2024Summer_HDU08.html"/>
    <id>https://www.cclmsy.cc/posts/ACM_2024Summer_HDU08.html</id>
    <published>2024-08-11T16:00:00.000Z</published>
    <updated>2025-03-09T02:54:24.916Z</updated>
    
    <content type="html"><![CDATA[<p>比赛题单：<a href="https://acm.hdu.edu.cn/search.php?field=problem&amp;key=2024%A1%B0%B6%A4%B0%D2%B1%E0%B3%CC%A1%B1%D6%D0%B9%FA%B4%F3%D1%A7%C9%FA%CB%E3%B7%A8%C9%E8%BC%C6%B3%AC%BC%B6%C1%AA%C8%FC%A3%A88%A3%A9&amp;source=1&amp;searchmode=source">2024“钉耙编程”中国大学生算法设计超级联赛（8）</a></p><h1 id="1004-HDU7520-cats-的重力拼图"><a href="#1004-HDU7520-cats-的重力拼图" class="headerlink" title="(1004)HDU7520.cats 的重力拼图"></a>(1004)HDU7520.cats 的重力拼图</h1><h2 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h2><p>有一个 $n\times m$ 的方格阵列，物块初始位于 $(x,y),1\le x\le n,1\le y\le m$。<br>每次操作可以改变重力方向：向上、向下、向左、向右，物块会沿重力方向移动，直到碰到边界。<br>求任意操作下物块最多经过的格子数。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>有2种最贪心的操作序列：</p><ol><li>向左、向右、再沿边缘一周</li><li>向上、向下、再沿边缘一周</li></ol><p>特判 $n=1$ 或 $m=1$ 或初始在边缘的情况。</p><h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,m,a,b;cin &gt;&gt; n &gt;&gt; m &gt;&gt; a &gt;&gt; b;</span><br><span class="line">    <span class="keyword">if</span>(n&lt;=<span class="number">2</span>||m&lt;=<span class="number">2</span>)&#123; cout &lt;&lt; n*m &lt;&lt; endl; <span class="keyword">return</span> ; &#125;</span><br><span class="line">    ll ans=<span class="number">2</span>*(n+m<span class="number">-2</span>);</span><br><span class="line">    <span class="keyword">if</span>(a==<span class="number">1</span>||a==n)&#123;</span><br><span class="line">        <span class="keyword">if</span>(b==<span class="number">1</span>||b==m) cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">        <span class="keyword">else</span> cout &lt;&lt; ans+n<span class="number">-2</span> &lt;&lt; endl;</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(b==<span class="number">1</span>||b==m) cout &lt;&lt; ans+m<span class="number">-2</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">else</span> cout &lt;&lt; ans+<span class="built_in">max</span>(&#123;<span class="number">0ll</span>,n<span class="number">-2</span>,m<span class="number">-2</span>&#125;) &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="（1006）HDU7522-cats-的最小生成树"><a href="#（1006）HDU7522-cats-的最小生成树" class="headerlink" title="（1006）HDU7522.cats 的最小生成树"></a>（1006）HDU7522.cats 的最小生成树</h1><h2 id="题意-1"><a href="#题意-1" class="headerlink" title="题意"></a>题意</h2><p>给定一个有 $n$ 个节点，可能含重边的带权无向图， $m$ 条边按顺序给出，第 $i$ 条边的权值为 $i$。<br>每次删去当前图的最小生成树的所有边，直到图不连通。</p><p>求每条边是在第几次被删除去的。</p><h2 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h2><p>根据Kruskal算法思想，最小生成树加边是从小到大加入的。<br>开若干个并查集，遍历边，每次二分查找当前边最早可以加入第几个并查集。</p><h2 id="参考代码-1"><a href="#参考代码-1" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,m;cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    ll ub=m/(n<span class="number">-1</span>);</span><br><span class="line">    <span class="function">vector&lt;DSU&gt; <span class="title">dsu</span><span class="params">(ub+<span class="number">2</span>,DSU(n))</span></span>;</span><br><span class="line">    <span class="function">vector&lt;ll&gt; <span class="title">cnt</span><span class="params">(ub+<span class="number">2</span>,<span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="function">vector&lt;ll&gt; <span class="title">ans</span><span class="params">(m+<span class="number">1</span>,<span class="number">0</span>)</span></span>;</span><br><span class="line">    ll u,v;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">1</span>,m)&#123;</span><br><span class="line">        cin &gt;&gt; u &gt;&gt; v;</span><br><span class="line">        ll l=<span class="number">1</span>,r=ub+<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)&#123;</span><br><span class="line">            ll mid=(l+r+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">            <span class="type">bool</span> fl=(dsu[mid].<span class="built_in">find</span>(u)==dsu[mid].<span class="built_in">find</span>(v));</span><br><span class="line">            <span class="keyword">if</span>(fl) l=mid;</span><br><span class="line">            <span class="keyword">else</span> r=mid<span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ll tar=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">FORLL</span>(i,<span class="built_in">max</span>(<span class="number">1ll</span>,l<span class="number">-3</span>),<span class="built_in">min</span>(ub+<span class="number">1</span>,l+<span class="number">3</span>))&#123;</span><br><span class="line">            <span class="keyword">if</span>(dsu[i].<span class="built_in">find</span>(u)!=dsu[i].<span class="built_in">find</span>(v))&#123;</span><br><span class="line">                tar=i;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(tar==ub+<span class="number">1</span>) &#123; ans[i]=<span class="number">-1</span>; <span class="keyword">continue</span>; &#125;</span><br><span class="line">        dsu[tar].<span class="built_in">merge</span>(u,v);</span><br><span class="line">        cnt[tar]++; ans[i]=tar;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">1</span>,m)&#123;</span><br><span class="line">        <span class="keyword">if</span>(ans[i]!=<span class="number">-1</span>&amp;&amp;cnt[ans[i]]==n<span class="number">-1</span>) cout &lt;&lt; ans[i];</span><br><span class="line">        <span class="keyword">else</span> cout &lt;&lt; <span class="string">&quot;-1&quot;</span>;</span><br><span class="line">        cout &lt;&lt; <span class="built_in">Presentation</span>(i,m);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="1007-HDU7523-cats-的-k-xor"><a href="#1007-HDU7523-cats-的-k-xor" class="headerlink" title="(1007)HDU7523.cats 的 k-xor"></a>(1007)HDU7523.cats 的 k-xor</h1><h2 id="题意-2"><a href="#题意-2" class="headerlink" title="题意"></a>题意</h2><p>给定2个十进制整数 $a,b,c$ ， $a,b$ 进行 $k(k\ge 2)$ 进制不进位加法后的结果是 $c$。<br>求 $k$ 有多少种可能。</p><h2 id="解题思路-2"><a href="#解题思路-2" class="headerlink" title="解题思路"></a>解题思路</h2><p>不进位加法下，丢失的进位信息 $a+b-c$ 是 $k$ 的倍数。<br>枚举 $a+b-c$ 的因子作为 $k$ ，check是否满足条件。</p><h2 id="参考代码-2"><a href="#参考代码-2" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(ll a,ll b,ll c,ll k)</span></span>&#123;</span><br><span class="line">    ll dif=a+b-c;</span><br><span class="line">    ll cur=<span class="number">1</span>,nxt=k,s=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(a/cur||b/cur)&#123;</span><br><span class="line">        ll ta=a/cur%k,tb=b/cur%k;</span><br><span class="line">        s+=(ta+tb)/k*nxt;</span><br><span class="line">        cur*=k; nxt*=k;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> s==dif;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll a,b,c;cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;</span><br><span class="line">    ll dif=a+b-c;</span><br><span class="line">    <span class="keyword">if</span>(dif==<span class="number">0</span>)&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;-1&quot;</span> &lt;&lt; endl;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    ll cnt=<span class="number">0</span>,ub=<span class="built_in">sqrt</span>(dif)+<span class="number">1</span>;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">2</span>,ub) <span class="keyword">if</span>(dif%i==<span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(i*i&gt;dif) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">check</span>(a,b,c,i)) cnt++;</span><br><span class="line">        <span class="keyword">if</span>(i*i!=dif&amp;&amp;<span class="built_in">check</span>(a,b,c,dif/i)) cnt++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">check</span>(a,b,c,dif)) cnt++;</span><br><span class="line">    cout &lt;&lt; cnt &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="（1012）HDU7528-cats-的电脑中毒"><a href="#（1012）HDU7528-cats-的电脑中毒" class="headerlink" title="（1012）HDU7528.cats 的电脑中毒"></a>（1012）HDU7528.cats 的电脑中毒</h1><h2 id="题意-3"><a href="#题意-3" class="headerlink" title="题意"></a>题意</h2><p>给定 $3$ 个长度为 $n$ 的二进制串 $a,b,c$ ， 表示病毒的初始位置。<br>每过一秒，病毒会感染相邻的所有二进制编码。（当且仅当两个二进制编码仅有一个位置不同时，这两个编码为相邻）<br>问所有的 $2^n$ 个二进制编码都被感染需要多少时间。</p><h2 id="解题思路-3"><a href="#解题思路-3" class="headerlink" title="解题思路"></a>解题思路</h2><p>考虑最后一秒被感染的二进制串，它的 距离三个初始位置的最小汉明距离 最大，找到这个串。<br>设这个串为 $s$ ，枚举每一位。若三个初始位置的这一位上，1的数量较多，则设为0；否则设为1。<br>然后进行微调，使得 $s$ 到三个初始位置的最小汉明距离 最大。</p><h2 id="参考代码-3"><a href="#参考代码-3" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">ll n;</span><br><span class="line"><span class="function">ll <span class="title">dis</span><span class="params">(string s1,string s2)</span></span>&#123;</span><br><span class="line">    ll cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">0</span>,n<span class="number">-1</span>) cnt+=(s1[i]!=s2[i]);</span><br><span class="line">    <span class="keyword">return</span> cnt;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    string s[<span class="number">3</span>],<span class="built_in">ns</span>(n,<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">0</span>,<span class="number">2</span>) cin &gt;&gt; s[i];</span><br><span class="line">    ll cnta=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">0</span>,n<span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="type">int</span> cnt1=<span class="number">0</span>,t=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">FORLL</span>(j,<span class="number">0</span>,<span class="number">2</span>) <span class="keyword">if</span>(s[j][i]==<span class="string">&#x27;1&#x27;</span>) cnt1++;</span><br><span class="line">        ns[i]=(cnt1&gt;=<span class="number">2</span>)?<span class="string">&#x27;0&#x27;</span>:<span class="string">&#x27;1&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ll dis0=<span class="built_in">dis</span>(s[<span class="number">0</span>],ns),dis1=<span class="built_in">dis</span>(s[<span class="number">1</span>],ns),dis2=<span class="built_in">dis</span>(s[<span class="number">2</span>],ns);</span><br><span class="line">    <span class="keyword">while</span>(dis0&lt;dis1<span class="number">-1</span>&amp;&amp;dis0&lt;dis2<span class="number">-1</span>)&#123; dis0++; dis1--; dis2--; &#125;</span><br><span class="line">    <span class="keyword">while</span>(dis1&lt;dis0<span class="number">-1</span>&amp;&amp;dis1&lt;dis2<span class="number">-1</span>)&#123; dis1++; dis0--; dis2--; &#125;</span><br><span class="line">    <span class="keyword">while</span>(dis2&lt;dis0<span class="number">-1</span>&amp;&amp;dis2&lt;dis1<span class="number">-1</span>)&#123; dis2++; dis0--; dis1--; &#125;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">min</span>(&#123;dis0,dis1,dis2&#125;) &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">题解|2024暑期杭电多校08</summary>
    
    
    
    <category term="竞赛笔记" scheme="https://www.cclmsy.cc/categories/%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="2024暑期杭电多校" scheme="https://www.cclmsy.cc/tags/2024%E6%9A%91%E6%9C%9F%E6%9D%AD%E7%94%B5%E5%A4%9A%E6%A0%A1/"/>
    
  </entry>
  
  <entry>
    <title>题解|2024暑期杭电多校07</title>
    <link href="https://www.cclmsy.cc/posts/ACM_2024Summer_HDU07.html"/>
    <id>https://www.cclmsy.cc/posts/ACM_2024Summer_HDU07.html</id>
    <published>2024-08-08T16:00:00.000Z</published>
    <updated>2025-03-09T02:54:24.916Z</updated>
    
    <content type="html"><![CDATA[<p>比赛题单：<a href="https://acm.hdu.edu.cn/search.php?field=problem&amp;key=2024%A1%B0%B6%A4%B0%D2%B1%E0%B3%CC%A1%B1%D6%D0%B9%FA%B4%F3%D1%A7%C9%FA%CB%E3%B7%A8%C9%E8%BC%C6%B3%AC%BC%B6%C1%AA%C8%FC%A3%A87%A3%A9&amp;source=1&amp;searchmode=source">2024“钉耙编程”中国大学生算法设计超级联赛（7）</a></p><h1 id="1010-HDU7514-故障机器人想活下去"><a href="#1010-HDU7514-故障机器人想活下去" class="headerlink" title="(1010)HDU7514.故障机器人想活下去"></a>(1010)HDU7514.故障机器人想活下去</h1><h2 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h2><p>故障机器人有 $x$ 点血量。<br>他将按顺序进行 $n$ 场战斗，第 $i$ 场战斗会受到 $a_i$ 点伤害。<br>故障机器人还有 $k$ 个烟雾弹，每个烟雾弹可以让他跳过一场战斗而不受伤害。</p><p>问故障机器人最多能活到第几场战斗结束。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>维护当前最高的 $k$ 个伤害，作为被跳过的伤害。<br>统计受伤量，直到受伤量超过 $x$ 为止。</p><h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll n,x,k;cin &gt;&gt; n &gt;&gt; x &gt;&gt; k;</span><br><span class="line">    <span class="built_in">create_vec</span>(v,n);</span><br><span class="line">    priority_queue&lt;ll,vector&lt;ll&gt;,less&lt;ll&gt;&gt; pq;</span><br><span class="line">    ll S=<span class="number">0</span>,cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">FORLL</span>(i,<span class="number">0</span>,n<span class="number">-1</span>)&#123;</span><br><span class="line">        S+=v[i];</span><br><span class="line">        pq.<span class="built_in">push</span>(v[i]);</span><br><span class="line">        <span class="keyword">while</span>(S&gt;=x)&#123;</span><br><span class="line">            <span class="keyword">if</span>(cnt&gt;=k)&#123;</span><br><span class="line">                cout &lt;&lt; pq.<span class="built_in">size</span>()+cnt<span class="number">-1</span> &lt;&lt; endl;</span><br><span class="line">                <span class="keyword">return</span> ;</span><br><span class="line">            &#125;</span><br><span class="line">            S-=pq.<span class="built_in">top</span>();</span><br><span class="line">            pq.<span class="built_in">pop</span>();</span><br><span class="line">            cnt++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;cout &lt;&lt; n &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="1011-HDU7515-蛋糕上的草莓是蛋糕的灵魂"><a href="#1011-HDU7515-蛋糕上的草莓是蛋糕的灵魂" class="headerlink" title="(1011)HDU7515.蛋糕上的草莓是蛋糕的灵魂"></a>(1011)HDU7515.蛋糕上的草莓是蛋糕的灵魂</h1><h2 id="题意-1"><a href="#题意-1" class="headerlink" title="题意"></a>题意</h2><p>有 $x$ 个相同的草莓和 $y$ 个相同的蛋糕。<br>每次可以将当前所有草莓或蛋糕放在一起切 $m$ 刀，即份数变为此前的 $2m$ 倍。<br>在满足条件的情况下切草莓的次数尽可能少。<br>求出最终草莓和蛋糕的数量。</p><h2 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h2><p>贪心的，观察到蛋糕是不用切的，将草莓切到 $\gcd(x,y)$ 份即可。<br>若 $\gcd$ 是 $x$ 的奇数倍，需要多来一刀。</p><h2 id="参考代码-1"><a href="#参考代码-1" class="headerlink" title="参考代码"></a>参考代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll x,y;cin &gt;&gt; x &gt;&gt; y;</span><br><span class="line">    ll g=__gcd(x,y);</span><br><span class="line">    <span class="keyword">if</span>(y/g&gt;<span class="number">1</span>&amp;&amp;(y/g)%<span class="number">2</span>) cout &lt;&lt; y &lt;&lt; <span class="string">&#x27; &#x27;</span> &lt;&lt; x/g*<span class="number">2</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">else</span> cout &lt;&lt; y &lt;&lt; <span class="string">&#x27; &#x27;</span> &lt;&lt; x/g &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">题解|2024暑期杭电多校07</summary>
    
    
    
    <category term="竞赛笔记" scheme="https://www.cclmsy.cc/categories/%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="2024暑期杭电多校" scheme="https://www.cclmsy.cc/tags/2024%E6%9A%91%E6%9C%9F%E6%9D%AD%E7%94%B5%E5%A4%9A%E6%A0%A1/"/>
    
  </entry>
  
</feed>
